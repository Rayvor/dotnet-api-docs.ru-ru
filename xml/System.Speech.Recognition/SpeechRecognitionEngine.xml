<Type Name="SpeechRecognitionEngine" FullName="System.Speech.Recognition.SpeechRecognitionEngine">
  <Metadata><Meta Name="ms.openlocfilehash" Value="eed89ae88fdd6e3eb5d993c63104666d18daaa47" /><Meta Name="ms.sourcegitcommit" Value="055a4a82a0b08bfbdc21bd1347fb71f7fe2c099e" /><Meta Name="ms.translationtype" Value="MT" /><Meta Name="ms.contentlocale" Value="ru-RU" /><Meta Name="ms.lasthandoff" Value="08/15/2019" /><Meta Name="ms.locfileid" Value="69231055" /></Metadata><TypeSignature Language="C#" Value="public class SpeechRecognitionEngine : IDisposable" />
  <TypeSignature Language="ILAsm" Value=".class public auto ansi beforefieldinit SpeechRecognitionEngine extends System.Object implements class System.IDisposable" />
  <TypeSignature Language="DocId" Value="T:System.Speech.Recognition.SpeechRecognitionEngine" />
  <TypeSignature Language="VB.NET" Value="Public Class SpeechRecognitionEngine&#xA;Implements IDisposable" />
  <TypeSignature Language="C++ CLI" Value="public ref class SpeechRecognitionEngine : IDisposable" />
  <TypeSignature Language="F#" Value="type SpeechRecognitionEngine = class&#xA;    interface IDisposable" />
  <AssemblyInfo>
    <AssemblyName>System.Speech</AssemblyName>
    <AssemblyVersion>3.0.0.0</AssemblyVersion>
    <AssemblyVersion>4.0.0.0</AssemblyVersion>
  </AssemblyInfo>
  <Base>
    <BaseTypeName>System.Object</BaseTypeName>
  </Base>
  <Interfaces>
    <Interface>
      <InterfaceName>System.IDisposable</InterfaceName>
    </Interface>
  </Interfaces>
  <Docs>
    <summary>Предоставляет средства доступа и управления механизмом распознавания речи внутри процесса.</summary>
    <remarks>
      <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Экземпляр этого класса можно создать для любого из установленных распознавателей речи. Чтобы получить сведения об установленных распознавателях, используйте статический <xref:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers%2A> метод.  
  
 Этот класс предназначен для запуска модулей распознавания речи в процессе и предоставляет контроль над различными аспектами распознавания речи следующим образом.  
  
-   Чтобы создать внутрипроцессный распознаватель речи, используйте один из <xref:System.Speech.Recognition.SpeechRecognitionEngine.%23ctor%2A> конструкторов.  
  
-   Для управления <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A>грамматиками распознавания речи используйте методы, <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.UnloadGrammar%2A>и <xref:System.Speech.Recognition.SpeechRecognitionEngine.UnloadAllGrammars%2A> , а также <xref:System.Speech.Recognition.SpeechRecognitionEngine.Grammars%2A> свойство.  
  
-   Чтобы настроить <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>входные данные для распознавателя, используйте метод <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A> <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>,, или <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A> .  
  
-   Чтобы выполнить распознавание речи, используйте <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A> метод <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> или.  
  
-   Чтобы изменить способ распознавания методом бездействия или непредвиденного ввода, <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>используйте <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>свойства <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A>,, <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> и.  
  
-   Чтобы изменить число альтернативных вариантов, возвращаемых распознавателем, используйте <xref:System.Speech.Recognition.SpeechRecognitionEngine.MaxAlternates%2A> свойство. Распознаватель возвращает результаты распознавания в <xref:System.Speech.Recognition.RecognitionResult> объект.  
  
-   Чтобы синхронизировать изменения распознавателя, используйте <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> метод. Распознаватель использует более одного потока для выполнения задач.  
  
-   Чтобы эмулировать входные данные распознавателя, используйте <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize%2A> методы и. <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A>  
  
 <xref:System.Speech.Recognition.SpeechRecognitionEngine> Объект предназначен для единственного использования процесса, создающего экземпляр объекта. И наоборот, <xref:System.Speech.Recognition.SpeechRecognizer> использует один распознаватель с любым приложением, которое хочет его использовать.  
  
> [!NOTE]
>  Всегда вызывайте <xref:System.Speech.Recognition.SpeechRecognitionEngine.Dispose%2A> , прежде чем вы освободите последнюю ссылку на распознаватель речи. В противном случае ресурсы, которые он использует, не будут освобождены до тех пор, пока сборщик мусора `Finalize` не вызовет метод объекта распознавателя.  
  
   
  
## Examples  
 В следующем примере показана часть консольного приложения, демонстрирующее базовое распознавание речи. Поскольку в этом примере используется `Multiple` режим <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> метода, он выполняет распознавание до закрытия окна консоли или завершения отладки.  
  
```csharp  
using System;  
using System.Speech.Recognition;  
  
namespace SpeechRecognitionApp  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Create an in-process speech recognizer for the en-US locale.  
      using (  
      SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(  
          new System.Globalization.CultureInfo("en-US")))  
      {  
  
        // Create and load a dictation grammar.  
        recognizer.LoadGrammar(new DictationGrammar());  
  
        // Add a handler for the speech recognized event.  
        recognizer.SpeechRecognized +=   
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start asynchronous, continuous speech recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Keep the console window open.  
        while (true)  
        {  
          Console.ReadLine();  
        }  
      }  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Recognized text: " + e.Result.Text);  
    }  
  }  
}  
```  
  
 ]]></format>
    </remarks>
    <altmember cref="T:System.Speech.Recognition.Grammar" />
    <altmember cref="T:System.Speech.Recognition.SpeechRecognizer" />
  </Docs>
  <Members>
    <MemberGroup MemberName=".ctor">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>Инициализирует новый экземпляр класса <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 <xref:System.Speech.Recognition.SpeechRecognitionEngine> Экземпляр можно создать одним из следующих:  
  
-   Модуль распознавания речи по умолчанию для системы  
  
-   Конкретный модуль распознавания речи, указанный по имени  
  
-   Модуль распознавания речи по умолчанию для указанного языкового стандарта  
  
-   Специальный механизм распознавания, который соответствует критериям, заданным в <xref:System.Speech.Recognition.RecognizerInfo> объекте.  
  
 Прежде чем распознаватель речи сможет начать распознавание, необходимо загрузить по крайней мере одну грамматику распознавания речи и настроить входные данные для распознавателя.  
  
 Чтобы загрузить грамматику, вызовите <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A> метод <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> или.  
  
 Чтобы настроить входные аудио, используйте один из следующих методов.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A>  
  
 ]]></format>
        </remarks>
      </Docs>
    </MemberGroup>
    <Member MemberName=".ctor">
      <MemberSignature Language="C#" Value="public SpeechRecognitionEngine ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig specialname rtspecialname instance void .ctor() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.#ctor" />
      <MemberSignature Language="VB.NET" Value="Public Sub New ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; SpeechRecognitionEngine();" />
      <MemberType>Constructor</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Parameters />
      <Docs>
        <summary>Инициализирует новый экземпляр класса <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />, используя распознаватель речи по умолчанию для системы.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Прежде чем распознаватель речи сможет начать распознавание речи, необходимо загрузить по крайней мере одну грамматику распознавания и настроить входные данные для распознавателя.  
  
 Чтобы загрузить грамматику, вызовите <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A> метод <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> или.  
  
 Чтобы настроить входные аудио, используйте один из следующих методов.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A>  
  
 ]]></format>
        </remarks>
      </Docs>
    </Member>
    <Member MemberName=".ctor">
      <MemberSignature Language="C#" Value="public SpeechRecognitionEngine (System.Globalization.CultureInfo culture);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig specialname rtspecialname instance void .ctor(class System.Globalization.CultureInfo culture) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.#ctor(System.Globalization.CultureInfo)" />
      <MemberSignature Language="VB.NET" Value="Public Sub New (culture As CultureInfo)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; SpeechRecognitionEngine(System::Globalization::CultureInfo ^ culture);" />
      <MemberSignature Language="F#" Value="new System.Speech.Recognition.SpeechRecognitionEngine : System.Globalization.CultureInfo -&gt; System.Speech.Recognition.SpeechRecognitionEngine" Usage="new System.Speech.Recognition.SpeechRecognitionEngine culture" />
      <MemberType>Constructor</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Parameters>
        <Parameter Name="culture" Type="System.Globalization.CultureInfo" />
      </Parameters>
      <Docs>
        <param name="culture">Языковой стандарт, который должен поддерживаться распознавателем речи.</param>
        <summary>Инициализирует новый экземпляр класса <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />, используя распознаватель речи по умолчанию для указанного языкового стандарта.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Microsoft Windows и API System. Speech принимают все допустимые коды языковых стран. Чтобы выполнить распознавание речи с использованием языка, указанного `CultureInfo` в аргументе, необходимо установить обработчик распознавания речи, поддерживающий этот код языка и страны. Модули распознавания речи, поставляемые вместе с Microsoft Windows 7, работают со следующими кодами языковых стран.  
  
-   EN-GB. Английский (Великобритания)  
  
-   EN-US. Английский (США)  
  
-   de-DE. Немецкий (Германия)  
  
-   ES-ES. Испанский (Испания)  
  
-   fr-FR. Французский (Франция)  
  
-   ja-JP. Японский (Япония)  
  
-   zh-CN. Китайский (Китай)  
  
-   zh-TW. Китайский (Тайвань)  
  
 Также разрешены двухбуквенный код языка, например "en", "fr" или "ES".  
  
 Прежде чем распознаватель речи сможет начать распознавание, необходимо загрузить по крайней мере одну грамматику распознавания речи и настроить входные данные для распознавателя.  
  
 Чтобы загрузить грамматику, вызовите <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A> метод <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> или.  
  
 Чтобы настроить входные аудио, используйте один из следующих методов.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A>  
  
   
  
## Examples  
 В следующем примере показана часть консольного приложения, которое демонстрирует базовое распознавание речи, и инициализирует распознаватель речи для локали en-US.  
  
```csharp  
using System;  
using System.Speech.Recognition;  
  
namespace SpeechRecognitionApp  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Create an in-process speech recognizer for the en-US locale.  
      using (  
      SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(  
          new System.Globalization.CultureInfo("en-US")))  
      {  
  
        // Create and load a dictation grammar.  
        recognizer.LoadGrammar(new DictationGrammar());  
  
        // Add a handler for the speech recognized event.  
        recognizer.SpeechRecognized +=   
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start asynchronous, continuous speech recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Keep the console window open.  
        while (true)  
        {  
          Console.ReadLine();  
        }  
      }  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Recognized text: " + e.Result.Text);  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentException">Ни одно из установленных средств распознавания речи не поддерживает указанное местоположение, либо <paramref name="culture" /> является языковым стандартом по умолчанию.</exception>
        <exception cref="T:System.ArgumentNullException"><paramref name="Culture" /> — <see langword="null" />.</exception>
      </Docs>
    </Member>
    <Member MemberName=".ctor">
      <MemberSignature Language="C#" Value="public SpeechRecognitionEngine (System.Speech.Recognition.RecognizerInfo recognizerInfo);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig specialname rtspecialname instance void .ctor(class System.Speech.Recognition.RecognizerInfo recognizerInfo) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.#ctor(System.Speech.Recognition.RecognizerInfo)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; SpeechRecognitionEngine(System::Speech::Recognition::RecognizerInfo ^ recognizerInfo);" />
      <MemberSignature Language="F#" Value="new System.Speech.Recognition.SpeechRecognitionEngine : System.Speech.Recognition.RecognizerInfo -&gt; System.Speech.Recognition.SpeechRecognitionEngine" Usage="new System.Speech.Recognition.SpeechRecognitionEngine recognizerInfo" />
      <MemberType>Constructor</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Parameters>
        <Parameter Name="recognizerInfo" Type="System.Speech.Recognition.RecognizerInfo" />
      </Parameters>
      <Docs>
        <param name="recognizerInfo">Сведения для конкретного распознавателя речи.</param>
        <summary>Инициализирует новый экземпляр класса <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />, используя информацию в объекте <see cref="T:System.Speech.Recognition.RecognizerInfo" /> для указания средства распознавания, которое необходимо использовать.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Экземпляр этого класса можно создать для любого из установленных распознавателей речи. Чтобы получить сведения об установленных распознавателях, используйте <xref:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers%2A> метод.  
  
 Прежде чем распознаватель речи сможет начать распознавание, необходимо загрузить по крайней мере одну грамматику распознавания речи и настроить входные данные для распознавателя.  
  
 Чтобы загрузить грамматику, вызовите <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A> метод <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> или.  
  
 Чтобы настроить входные аудио, используйте один из следующих методов.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A>  
  
   
  
## Examples  
 В следующем примере показана часть консольного приложения, которое демонстрирует базовое распознавание речи, и инициализируется распознаватель речи, поддерживающий английский язык.  
  
```csharp  
 using System;  
using System.Speech.Recognition;  
  
namespace SpeechRecognitionApp  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Select a speech recognizer that supports English.  
      RecognizerInfo info = null;  
      foreach (RecognizerInfo ri in SpeechRecognitionEngine.InstalledRecognizers())  
      {  
        if (ri.Culture.TwoLetterISOLanguageName.Equals("en"))  
        {  
          info = ri;  
          break;  
        }  
      }  
      if (info == null) return;  
  
      // Create the selected recognizer.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(info))  
      {  
  
        // Create and load a dictation grammar.  
        recognizer.LoadGrammar(new DictationGrammar());  
  
        // Add a handler for the speech recognized event.  
        recognizer.SpeechRecognized +=   
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start asynchronous, continuous speech recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Keep the console window open.  
        while (true)  
        {  
          Console.ReadLine();  
        }  
      }  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Recognized text: " + e.Result.Text);  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
      </Docs>
    </Member>
    <Member MemberName=".ctor">
      <MemberSignature Language="C#" Value="public SpeechRecognitionEngine (string recognizerId);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig specialname rtspecialname instance void .ctor(string recognizerId) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.#ctor(System.String)" />
      <MemberSignature Language="VB.NET" Value="Public Sub New (recognizerId As String)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; SpeechRecognitionEngine(System::String ^ recognizerId);" />
      <MemberSignature Language="F#" Value="new System.Speech.Recognition.SpeechRecognitionEngine : string -&gt; System.Speech.Recognition.SpeechRecognitionEngine" Usage="new System.Speech.Recognition.SpeechRecognitionEngine recognizerId" />
      <MemberType>Constructor</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Parameters>
        <Parameter Name="recognizerId" Type="System.String" />
      </Parameters>
      <Docs>
        <param name="recognizerId">Имя токена распознавателя речи.</param>
        <summary>Инициализирует новый экземпляр класса <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> с заданным строковым параметром, задающим имя средства распознавания, которое необходимо использовать.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Имя токена распознавателя является значением <xref:System.Speech.Recognition.RecognizerInfo.Id%2A> свойства <xref:System.Speech.Recognition.RecognizerInfo> объекта, возвращаемого <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerInfo%2A> свойством распознавателя. Чтобы получить коллекцию всех установленных распознавателей, используйте статический <xref:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers%2A> метод.  
  
 Прежде чем распознаватель речи сможет начать распознавание, необходимо загрузить по крайней мере одну грамматику распознавания речи и настроить входные данные для распознавателя.  
  
 Чтобы загрузить грамматику, вызовите <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A> метод <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> или.  
  
 Чтобы настроить входные аудио, используйте один из следующих методов.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A>  
  
   
  
## Examples  
 В следующем примере показана часть консольного приложения, демонстрирующее базовое распознавание речи, и создается экземпляр распознавателя речи 8,0 для Windows (Английский (США)).  
  
```csharp  
using System;  
using System.Speech.Recognition;  
  
namespace SpeechRecognitionApp  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Create an instance of the Microsoft Speech Recognizer 8.0 for  
      // Windows (English - US).  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine("MS-1033-80-DESK"))  
      {  
  
        // Create and load a dictation grammar.  
        recognizer.LoadGrammar(new DictationGrammar());  
  
        // Add a handler for the speech recognized event.  
        recognizer.SpeechRecognized += new EventHandler(recognizer_SpeechRecognized);  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start asynchronous, continuous speech recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Keep the console window open.  
        while (true)  
        {  
          Console.ReadLine();  
        }  
      }  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Recognized text: " + e.Result.Text);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentException">Отсутствует установленное средство распознавания речи с указанным именем токена или <paramref name="recognizerId" /> является пустой строкой ("").</exception>
        <exception cref="T:System.ArgumentNullException"><paramref name="recognizerId" /> — <see langword="null" />.</exception>
      </Docs>
    </Member>
    <Member MemberName="AudioFormat">
      <MemberSignature Language="C#" Value="public System.Speech.AudioFormat.SpeechAudioFormatInfo AudioFormat { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance class System.Speech.AudioFormat.SpeechAudioFormatInfo AudioFormat" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioFormat" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property AudioFormat As SpeechAudioFormatInfo" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property System::Speech::AudioFormat::SpeechAudioFormatInfo ^ AudioFormat { System::Speech::AudioFormat::SpeechAudioFormatInfo ^ get(); };" />
      <MemberSignature Language="F#" Value="member this.AudioFormat : System.Speech.AudioFormat.SpeechAudioFormatInfo" Usage="System.Speech.Recognition.SpeechRecognitionEngine.AudioFormat" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.AudioFormat.SpeechAudioFormatInfo</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Получает формат аудио, принимаемого объектом <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />.</summary>
        <value>Входной звуковой формат для экземпляра <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> или значение <see langword="null" />, если входные данные не настроены или они пустые.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Чтобы настроить входные аудио, используйте один из следующих методов.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A>  
  
   
  
## Examples  
 В приведенном ниже <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioFormat%2A> примере используется для получения и вывода данных в формате звука.  
  
```  
static void DisplayAudioDeviceFormat(Label label, SpeechRecognitionEngine recognitionEngine)   
{  
  
  if (recognitionEngine != null && label != null)   
  {  
    label.Text = String.Format("Encoding Format:         {0}\n" +  
          "AverageBytesPerSecond    {1}\n" +  
          "BitsPerSample            {2}\n" +  
          "BlockAlign               {3}\n" +  
          "ChannelCount             {4}\n" +  
          "SamplesPerSecond         {5}",  
          recognitionEngine.AudioFormat.EncodingFormat.ToString(),  
          recognitionEngine.AudioFormat.AverageBytesPerSecond,  
          recognitionEngine.AudioFormat.BitsPerSample,  
          recognitionEngine.AudioFormat.BlockAlign,  
          recognitionEngine.AudioFormat.ChannelCount,  
          recognitionEngine.AudioFormat.SamplesPerSecond);  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.AudioFormat.SpeechAudioFormatInfo" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.AudioFormat" />
      </Docs>
    </Member>
    <Member MemberName="AudioLevel">
      <MemberSignature Language="C#" Value="public int AudioLevel { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance int32 AudioLevel" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevel" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property AudioLevel As Integer" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property int AudioLevel { int get(); };" />
      <MemberSignature Language="F#" Value="member this.AudioLevel : int" Usage="System.Speech.Recognition.SpeechRecognitionEngine.AudioLevel" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Int32</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Получает уровень звука, принимаемого объектом <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />.</summary>
        <value>Уровень звука ввода в распознаватель речи, от 0 до 100.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Значение 0 означает тишина, а 100 — максимальное значение входного тома.  
  
 ]]></format>
        </remarks>
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevelUpdated" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.AudioLevel" />
      </Docs>
    </Member>
    <Member MemberName="AudioLevelUpdated">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.AudioLevelUpdatedEventArgs&gt; AudioLevelUpdated;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.AudioLevelUpdatedEventArgs&gt; AudioLevelUpdated" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevelUpdated" />
      <MemberSignature Language="VB.NET" Value="Public Custom Event AudioLevelUpdated As EventHandler(Of AudioLevelUpdatedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::AudioLevelUpdatedEventArgs ^&gt; ^ AudioLevelUpdated;" />
      <MemberSignature Language="F#" Value="member this.AudioLevelUpdated : EventHandler&lt;System.Speech.Recognition.AudioLevelUpdatedEventArgs&gt; " Usage="member this.AudioLevelUpdated : System.EventHandler&lt;System.Speech.Recognition.AudioLevelUpdatedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.AudioLevelUpdatedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Возникает, когда <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> сообщает об уровне аудиовхода.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 <xref:System.Speech.Recognition.SpeechRecognitionEngine> Создает это событие несколько раз в секунду. Частота, с которой возникает событие, зависит от компьютера, на котором работает приложение.  
  
 Чтобы получить уровень звука во время события, используйте <xref:System.Speech.Recognition.AudioLevelUpdatedEventArgs.AudioLevel%2A> свойство связанного <xref:System.Speech.Recognition.AudioLevelUpdatedEventArgs>объекта. Чтобы получить текущий уровень звука входных данных распознавателя, используйте <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevel%2A> свойство распознавателя.  
  
 При создании делегата <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevelUpdated> определяется метод обработки события. Чтобы связать событие с обработчиком событий, нужно добавить в событие экземпляр делегата. Обработчик событий вызывается всякий раз, когда происходит событие, если делегат не удален. Дополнительные сведения о делегатах обработчиков событий см. в разделе [события и делегаты](https://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 В следующем примере обработчик <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevelUpdated> события добавляется <xref:System.Speech.Recognition.SpeechRecognitionEngine> в объект. Обработчик выводит новый звуковой уровень в консоль.  
  
```  
private SpeechRecognitionEngine recognizer;  
  
// Initialize the SpeechRecognitionEngine object.   
private void Initialize()  
{  
  recognizer = new SpeechRecognitionEngine();  
  
  // Add an event handler for the AudioLevelUpdated event.  
  recognizer.AudioLevelUpdated +=   
   new EventHandler<AudioLevelUpdatedEventArgs>(recognizer_AudioLevelUpdated);  
  
  // Add other initialization code here.  
  
}  
  
// Write the audio level to the console when the AudioLevelUpdated event is raised.  
void recognizer_AudioLevelUpdated(object sender, AudioLevelUpdatedEventArgs e)  
{  
  Console.WriteLine("The audio level is now: {0}.", e.AudioLevel);  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.AudioLevelUpdatedEventArgs" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevel" />
      </Docs>
    </Member>
    <Member MemberName="AudioPosition">
      <MemberSignature Language="C#" Value="public TimeSpan AudioPosition { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.TimeSpan AudioPosition" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property AudioPosition As TimeSpan" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property TimeSpan AudioPosition { TimeSpan get(); };" />
      <MemberSignature Language="F#" Value="member this.AudioPosition : TimeSpan" Usage="System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.TimeSpan</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Получает текущую позицию в аудиопотоке, создаваемом устройством, которое предоставляет входные данные для <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />.</summary>
        <value>Текущее расположение в потоке звука, созданном устройством ввода.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A> Свойство ссылается на расположение устройства ввода в созданном звуковом потоке. Напротив, <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A> свойство ссылается на расположение распознавателя в своем звуковом входе. Эти позиции могут отличаться. Например, если распознаватель получил входные данные, для которых он еще не создал результат распознавания, значение <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A> свойства меньше значения <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A> свойства.  
  
   
  
## Examples  
 В следующем примере внутрипроцессный распознаватель речи использует грамматику диктовки для сопоставления речевого ввода. Обработчик <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected> события записывает данные в <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A>консоль, <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A>и <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevel%2A> , когда распознаватель речи обнаруживает речь во входных данных.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    private static SpeechRecognitionEngine recognizer;  
    public static void Main(string[] args)  
    {  
  
      // Initialize an in-process speech recognition engine for US English.  
      using (recognizer = new SpeechRecognitionEngine(  
        new System.Globalization.CultureInfo("en-US")))  
      {  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Create a grammar for finding services in different cities.  
        Choices services = new Choices(new string[] { "restaurants", "hotels", "gas stations" });  
        Choices cities = new Choices(new string[] { "Seattle", "Boston", "Dallas" });  
  
        GrammarBuilder findServices = new GrammarBuilder("Find");  
        findServices.Append(services);  
        findServices.Append("near");  
        findServices.Append(cities);  
  
        // Create a Grammar object from the GrammarBuilder and load it to the recognizer.  
        Grammar servicesGrammar = new Grammar(findServices);  
        recognizer.LoadGrammarAsync(servicesGrammar);  
  
        // Add handlers for events.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
        recognizer.SpeechDetected +=  
          new EventHandler<SpeechDetectedEventArgs>(recognizer_SpeechDetected);  
  
        // Start asynchronous recognition.  
        recognizer.RecognizeAsync();  
        Console.WriteLine("Starting asynchronous recognition...");  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Gather information about detected speech and write it to the console.  
    static void recognizer_SpeechDetected(object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine();  
      Console.WriteLine("Speech detected:");  
      Console.WriteLine("  Audio level: " + recognizer.AudioLevel);  
      Console.WriteLine("  Audio position at the event: " + e.AudioPosition);  
      Console.WriteLine("  Current audio position: " + recognizer.AudioPosition);  
      Console.WriteLine("  Current recognizer audio position: " +   
        recognizer.RecognizerAudioPosition);  
    }  
  
    // Write the text of the recognition result to the console.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("\nSpeech recognized: " + e.Result.Text);  
  
      // Add event handler code here.  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.AudioPosition" />
      </Docs>
    </Member>
    <Member MemberName="AudioSignalProblemOccurred">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.AudioSignalProblemOccurredEventArgs&gt; AudioSignalProblemOccurred;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.AudioSignalProblemOccurredEventArgs&gt; AudioSignalProblemOccurred" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.AudioSignalProblemOccurred" />
      <MemberSignature Language="VB.NET" Value="Public Custom Event AudioSignalProblemOccurred As EventHandler(Of AudioSignalProblemOccurredEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::AudioSignalProblemOccurredEventArgs ^&gt; ^ AudioSignalProblemOccurred;" />
      <MemberSignature Language="F#" Value="member this.AudioSignalProblemOccurred : EventHandler&lt;System.Speech.Recognition.AudioSignalProblemOccurredEventArgs&gt; " Usage="member this.AudioSignalProblemOccurred : System.EventHandler&lt;System.Speech.Recognition.AudioSignalProblemOccurredEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.AudioSignalProblemOccurredEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Возникает, когда <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> обнаруживает проблему в аудиосигнале.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Чтобы узнать, какая проблема возникла, <xref:System.Speech.Recognition.AudioSignalProblemOccurredEventArgs.AudioSignalProblem%2A> используйте свойство связанного <xref:System.Speech.Recognition.AudioSignalProblemOccurredEventArgs>объекта.  
  
 При создании делегата <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioSignalProblemOccurred> определяется метод обработки события. Чтобы связать событие с обработчиком событий, нужно добавить в событие экземпляр делегата. Обработчик событий вызывается всякий раз, когда происходит событие, если делегат не удален. Дополнительные сведения о делегатах обработчиков событий см. в разделе [события и делегаты](https://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 В следующем примере определяется обработчик событий, собирающий сведения о <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioSignalProblemOccurred> событии.  
  
```  
private SpeechRecognitionEngine recognizer;  
  
// Initialize the speech recognition engine.  
private void Initialize()  
{  
  recognizer = new SpeechRecognitionEngine();  
  
  // Add a handler for the AudioSignalProblemOccurred event.  
  recognizer.AudioSignalProblemOccurred +=   
    new EventHandler<AudioSignalProblemOccurredEventArgs>(  
      recognizer_AudioSignalProblemOccurred);  
}  
  
// Gather information when the AudioSignalProblemOccurred event is raised.  
void recognizer_AudioSignalProblemOccurred(object sender, AudioSignalProblemOccurredEventArgs e)  
{  
  StringBuilder details = new StringBuilder();  
  
  details.AppendLine("Audio signal problem information:");  
  details.AppendFormat(  
    " Audio level:               {0}" + Environment.NewLine +  
    " Audio position:            {1}" + Environment.NewLine +  
    " Audio signal problem:      {2}" + Environment.NewLine +  
    " Recognition engine audio position: {3}" + Environment.NewLine,  
    e.AudioLevel, e.AudioPosition,  e.AudioSignalProblem,  
    e.recoEngineAudioPosition);  
  
  // Insert additional event handler code here.  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.AudioSignalProblem" />
        <altmember cref="T:System.Speech.Recognition.AudioSignalProblemOccurredEventArgs" />
      </Docs>
    </Member>
    <Member MemberName="AudioState">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.AudioState AudioState { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.Speech.Recognition.AudioState AudioState" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioState" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property AudioState As AudioState" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property System::Speech::Recognition::AudioState AudioState { System::Speech::Recognition::AudioState get(); };" />
      <MemberSignature Language="F#" Value="member this.AudioState : System.Speech.Recognition.AudioState" Usage="System.Speech.Recognition.SpeechRecognitionEngine.AudioState" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.AudioState</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Получает состояние звука, принимаемого объектом <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />.</summary>
        <value>Состояние звукового ввода в распознаватель речи.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Свойство представляет звуковое состояние с элементом <xref:System.Speech.Recognition.AudioState> перечисления. <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioState%2A>  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.AudioState" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.AudioStateChanged" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.AudioState" />
      </Docs>
    </Member>
    <Member MemberName="AudioStateChanged">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.AudioStateChangedEventArgs&gt; AudioStateChanged;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.AudioStateChangedEventArgs&gt; AudioStateChanged" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.AudioStateChanged" />
      <MemberSignature Language="VB.NET" Value="Public Custom Event AudioStateChanged As EventHandler(Of AudioStateChangedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::AudioStateChangedEventArgs ^&gt; ^ AudioStateChanged;" />
      <MemberSignature Language="F#" Value="member this.AudioStateChanged : EventHandler&lt;System.Speech.Recognition.AudioStateChangedEventArgs&gt; " Usage="member this.AudioStateChanged : System.EventHandler&lt;System.Speech.Recognition.AudioStateChangedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.AudioStateChangedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Возникает, когда меняется состояние получаемого объектом <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> аудио.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Чтобы получить состояние звука во время события, используйте <xref:System.Speech.Recognition.AudioStateChangedEventArgs.AudioState%2A> свойство связанного <xref:System.Speech.Recognition.AudioStateChangedEventArgs>объекта. Чтобы получить текущее звуковое состояние входных данных распознавателя, используйте <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioState%2A> свойство распознавателя. Дополнительные сведения о звуковом состоянии см. в <xref:System.Speech.Recognition.AudioState> описании перечисления.  
  
 При создании делегата <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioStateChanged> определяется метод обработки события. Чтобы связать событие с обработчиком событий, нужно добавить в событие экземпляр делегата. Обработчик событий вызывается всякий раз, когда происходит событие, если делегат не удален. Дополнительные сведения о делегатах обработчиков событий см. в разделе [события и делегаты](https://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 В следующем примере обработчик <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioStateChanged> события используется для записи нового <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioState%2A> распознавателя в консоль при каждом его изменении <xref:System.Speech.Recognition.AudioState> с помощью члена перечисления.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
  
    // Initialize an in-process speech recognition engine.  
    {  
      using (SpeechRecognitionEngine recognizer =  
         new SpeechRecognitionEngine(new System.Globalization.CultureInfo("en-US")))  
      {  
  
        // Create and load a grammar.  
        Choices animals = new Choices(new string[] { "cow", "pig", "goat" });  
        GrammarBuilder farm = new GrammarBuilder("On this farm he had a");  
        farm.Append(animals);  
        Grammar farmAnimals = new Grammar(farm);  
        farmAnimals.Name = "Farm";  
        recognizer.LoadGrammar(farmAnimals);  
  
        // Attach event handlers.  
        recognizer.AudioStateChanged +=  
          new EventHandler<AudioStateChangedEventArgs>(recognizer_AudioStateChanged);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
        recognizer.LoadGrammarCompleted +=  
          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
  
        // Set the input to the recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start recognition.  
        recognizer.RecognizeAsync();  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Handle the LoadGrammarCompleted event.  
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      Console.WriteLine("Grammar loaded: " + e.Grammar.Name);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      if (e.Result != null && e.Result.Text != null)  
      {  
        Console.WriteLine();  
        Console.WriteLine("  Recognized text =  {0}", e.Result.Text);  
        Console.WriteLine();  
      }  
      else  
      {  
        Console.WriteLine("  Recognized text not available.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Done.");  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the AudioStateChanged event.  
    static void recognizer_AudioStateChanged(object sender, AudioStateChangedEventArgs e)  
    {  
      Console.WriteLine("The new audio state is: " + e.AudioState);  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.AudioState" />
        <altmember cref="T:System.Speech.Recognition.AudioStateChangedEventArgs" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioState" />
      </Docs>
    </Member>
    <Member MemberName="BabbleTimeout">
      <MemberSignature Language="C#" Value="public TimeSpan BabbleTimeout { get; set; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.TimeSpan BabbleTimeout" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout" />
      <MemberSignature Language="VB.NET" Value="Public Property BabbleTimeout As TimeSpan" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property TimeSpan BabbleTimeout { TimeSpan get(); void set(TimeSpan value); };" />
      <MemberSignature Language="F#" Value="member this.BabbleTimeout : TimeSpan with get, set" Usage="System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Attributes>
        <Attribute>
          <AttributeName>System.ComponentModel.EditorBrowsable(System.ComponentModel.EditorBrowsableState.Advanced)</AttributeName>
        </Attribute>
      </Attributes>
      <ReturnValue>
        <ReturnType>System.TimeSpan</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Получает или задает интервал времени, в течение которого <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> принимает входные данные, содержащие только фоновый шум, прежде чем финализировать распознавание.</summary>
        <value>Длительность интервала времени.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Каждый распознаватель речи имеет алгоритм для различения тишины и речи. Распознаватель классифицируется как фоновый шум любых входных данных, не относящихся к бездействиям, которые не соответствуют начальному правилу всех загруженных и включенных грамматик распознавания речи распознавателя. Если распознаватель получает только фоновый шум и бездействие в течение интервала времени ожидания баббле, то распознаватель завершает эту операцию распознавания.  
  
-   Для асинхронных <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> операций распознавания распознаватель создает событие, <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.BabbleTimeout%2A?displayProperty=nameWithType> где <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.Result%2A?displayProperty=nameWithType> свойство имеет `true`значение, а свойство — `null`.  
  
-   Для синхронных операций распознавания и эмуляции распознаватель возвращает `null`, а не является допустимым. <xref:System.Speech.Recognition.RecognitionResult>  
  
 Если значение времени ожидания баббле равно 0, то распознаватель не выполняет проверку времени ожидания баббле. Интервал времени ожидания может быть любым неотрицательным значением. Значение по умолчанию — 0 секунд.  
  
   
  
## Examples  
 В следующем примере показана часть консольного приложения, демонстрирующее базовое распознавание речи, <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A> которое <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> задает свойства <xref:System.Speech.Recognition.SpeechRecognitionEngine> и перед инициацией распознавания речи. Обработчики для событий распознавания <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioStateChanged> речи и <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> выводят сведения о событиях на консоль, чтобы продемонстрировать, <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> <xref:System.Speech.Recognition.SpeechRecognitionEngine> как свойства влияют на операции распознавания.  
  
```csharp  
  
using System;  
using System.Speech.Recognition;  
  
namespace SpeechRecognitionApp  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Initialize an in-process speech recognizer.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(  
          new System.Globalization.CultureInfo("en-US")))  
      {  
        // Load a Grammar object.  
        recognizer.LoadGrammar(CreateServicesGrammar("FindServices"));  
  
        // Add event handlers.  
        recognizer.AudioStateChanged +=  
          new EventHandler<AudioStateChangedEventArgs>(  
            AudioStateChangedHandler);  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(  
            RecognizeCompletedHandler);  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        recognizer.InitialSilenceTimeout = TimeSpan.FromSeconds(3);  
        recognizer.BabbleTimeout = TimeSpan.FromSeconds(2);  
        recognizer.EndSilenceTimeout = TimeSpan.FromSeconds(1);  
        recognizer.EndSilenceTimeoutAmbiguous = TimeSpan.FromSeconds(1.5);  
  
        Console.WriteLine("BabbleTimeout: {0}", recognizer.BabbleTimeout);  
        Console.WriteLine("InitialSilenceTimeout: {0}", recognizer.InitialSilenceTimeout);  
        Console.WriteLine("EndSilenceTimeout: {0}", recognizer.EndSilenceTimeout);  
        Console.WriteLine("EndSilenceTimeoutAmbiguous: {0}", recognizer.EndSilenceTimeoutAmbiguous);  
        Console.WriteLine();  
  
        // Start asynchronous speech recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Single);  
  
        // Keep the console window open.  
        while (true)  
        {  
          Console.ReadLine();  
        }  
      }  
    }  
  
    // Create a grammar and build it into a Grammar object.   
    static Grammar CreateServicesGrammar(string grammarName)  
    {  
  
      // Create a grammar for finding services in different cities.  
      Choices services = new Choices(new string[] { "restaurants", "hotels", "gas stations" });  
      Choices cities = new Choices(new string[] { "Seattle", "Boston", "Dallas" });  
  
      GrammarBuilder findServices = new GrammarBuilder("Find");  
      findServices.Append(services);  
      findServices.Append("near");  
      findServices.Append(cities);  
  
      // Create a Grammar object from the GrammarBuilder. 
      Grammar servicesGrammar = new Grammar(findServices);  
      servicesGrammar.Name = ("FindServices");  
      return servicesGrammar;  
    }  
  
    // Handle the AudioStateChanged event.  
    static void AudioStateChangedHandler(  
      object sender, AudioStateChangedEventArgs e)  
    {  
      Console.WriteLine("AudioStateChanged ({0}): {1}",  
        DateTime.Now.ToString("mm:ss.f"), e.AudioState);  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void RecognizeCompletedHandler(  
      object sender, RecognizeCompletedEventArgs e)  
    {  
      Console.WriteLine("RecognizeCompleted ({0}):",  
        DateTime.Now.ToString("mm:ss.f"));  
  
      string resultText;  
      if (e.Result != null) { resultText = e.Result.Text; }  
      else { resultText = "<null>"; }  
  
      Console.WriteLine(  
        " BabbleTimeout: {0}; InitialSilenceTimeout: {1}; Result text: {2}",  
        e.BabbleTimeout, e.InitialSilenceTimeout, resultText);  
      if (e.Error != null)  
      {  
        Console.WriteLine(" Exception message: ", e.Error.Message);  
      }  
  
      // Start the next asynchronous recognition operation.  
      ((SpeechRecognitionEngine)sender).RecognizeAsync(RecognizeMode.Single);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentOutOfRangeException">Этому свойству задано значение меньше 0 секунд.</exception>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
      </Docs>
    </Member>
    <MemberGroup MemberName="Dispose">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>Удаляет объект <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />.</summary>
      </Docs>
    </MemberGroup>
    <Member MemberName="Dispose">
      <MemberSignature Language="C#" Value="public void Dispose ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig newslot virtual instance void Dispose() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.Dispose" />
      <MemberSignature Language="VB.NET" Value="Public Sub Dispose ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; virtual void Dispose();" />
      <MemberSignature Language="F#" Value="abstract member Dispose : unit -&gt; unit&#xA;override this.Dispose : unit -&gt; unit" Usage="speechRecognitionEngine.Dispose " />
      <MemberType>Method</MemberType>
      <Implements>
        <InterfaceMember>M:System.IDisposable.Dispose</InterfaceMember>
      </Implements>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Удаляет объект <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />.</summary>
        <remarks>To be added.</remarks>
      </Docs>
    </Member>
    <Member MemberName="Dispose">
      <MemberSignature Language="C#" Value="protected virtual void Dispose (bool disposing);" />
      <MemberSignature Language="ILAsm" Value=".method familyhidebysig newslot virtual instance void Dispose(bool disposing) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.Dispose(System.Boolean)" />
      <MemberSignature Language="VB.NET" Value="Protected Overridable Sub Dispose (disposing As Boolean)" />
      <MemberSignature Language="C++ CLI" Value="protected:&#xA; virtual void Dispose(bool disposing);" />
      <MemberSignature Language="F#" Value="abstract member Dispose : bool -&gt; unit&#xA;override this.Dispose : bool -&gt; unit" Usage="speechRecognitionEngine.Dispose disposing" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="disposing" Type="System.Boolean" />
      </Parameters>
      <Docs>
        <param name="disposing">Значение <see langword="true" /> позволяет освободить управляемые и неуправляемые ресурсы; значение <see langword="false" /> позволяет освободить только неуправляемые ресурсы.</param>
        <summary>Удаляет объект <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> и освобождает ресурсы, используемые во время сеанса.</summary>
        <remarks>To be added.</remarks>
      </Docs>
    </Member>
    <MemberGroup MemberName="EmulateRecognize">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>Эмулирует ввод в распознаватель речи, используя текст вместо аудио для синхронного распознавания речи.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Эти методы обходят входные звуковые данные системы и предоставляют распознаватель в <xref:System.String> виде объектов или <xref:System.Speech.Recognition.RecognizedWordUnit> массива объектов. Это может быть полезно при тестировании или отладке приложения или грамматики. Например, можно использовать эмуляцию, чтобы определить, находится ли слово в грамматике и какая семантика возвращается при распознавании слова. <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A> Используйте метод, чтобы отключить входные аудио в подсистему распознавания речи во время операций эмуляции.  
  
 Распознаватель речи создает <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>события, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>и <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> , как если бы операция распознавания не была эмуляциа. Распознаватель игнорирует новые строки и лишние пробелы и рассматривает знаки препинания как литеральные входные данные.  
  
> [!NOTE]
>  Объект, созданный распознавателем речи в ответ на эмулированные входные данные, имеет `null` значение для его <xref:System.Speech.Recognition.RecognitionResult.Audio%2A> свойства. <xref:System.Speech.Recognition.RecognitionResult>  
  
 Чтобы эмулировать асинхронное распознавание, используйте <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> метод.  
  
 ]]></format>
        </remarks>
      </Docs>
    </MemberGroup>
    <Member MemberName="EmulateRecognize">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.RecognitionResult EmulateRecognize (string inputText);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance class System.Speech.Recognition.RecognitionResult EmulateRecognize(string inputText) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String)" />
      <MemberSignature Language="VB.NET" Value="Public Function EmulateRecognize (inputText As String) As RecognitionResult" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; System::Speech::Recognition::RecognitionResult ^ EmulateRecognize(System::String ^ inputText);" />
      <MemberSignature Language="F#" Value="member this.EmulateRecognize : string -&gt; System.Speech.Recognition.RecognitionResult" Usage="speechRecognitionEngine.EmulateRecognize inputText" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Attributes>
        <Attribute FrameworkAlternate="netframework-4.0">
          <AttributeName>System.Runtime.TargetedPatchingOptOut("Performance critical to inline this type of method across NGen image boundaries")</AttributeName>
        </Attribute>
      </Attributes>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.RecognitionResult</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="inputText" Type="System.String" />
      </Parameters>
      <Docs>
        <param name="inputText">Входные данные для операции распознавания.</param>
        <summary>Эмулирует ввод фразы в распознаватель речи, используя текст вместо аудио для синхронного распознавания речи.</summary>
        <returns>Результат операции распознавания или <see langword="null" />, если операция завершилась с ошибкой или распознаватель не включен.</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Распознаватель речи создает <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>события, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>и <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> , как если бы операция распознавания не была эмуляциа.  
  
 Распознаватели, поставляемые с Vista и Windows 7, не учитывают регистр и ширину символов при применении грамматических правил к входной фразе. Дополнительные сведения об этом типе сравнения см. в <xref:System.Globalization.CompareOptions> разделе значения <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> перечисления и <xref:System.Globalization.CompareOptions.IgnoreWidth>. Распознаватели также пропускают новые строки и дополнительные пробелы и обрабатывают знаки препинания в качестве литеральных входных данных.  
  
   
  
## Examples  
 Приведенный ниже пример кода является частью консольного приложения, которое демонстрирует эмулированные входные данные, связанные результаты распознавания и связанные события, вызванные распознавателем речи. В примере создаются следующие выходные данные.  
  
```  
TestRecognize("Smith")...  
 SpeechDetected event raised.  
 SpeechRecognized event raised.  
  Grammar = Smith; Text = Smith  
...Recognition result text = Smith  
  
TestRecognize("Jones")...  
 SpeechDetected event raised.  
 SpeechRecognized event raised.  
  Grammar = Jones; Text = Jones  
...Recognition result text = Jones  
  
TestRecognize("Mister")...  
 SpeechDetected event raised.  
 SpeechHypothesized event raised.  
  Grammar = Smith; Text = mister  
 SpeechRecognitionRejected event raised.  
  Grammar = <not available>; Text =  
...No recognition result.  
  
TestRecognize("Mister Smith")...  
 SpeechDetected event raised.  
 SpeechRecognized event raised.  
  Grammar = Smith; Text = mister Smith  
...Recognition result text = mister Smith  
  
press any key to exit...  
```  
  
```csharp  
  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
  
namespace Sre_EmulateRecognize  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Create an in-process speech recognizer for the en-US locale.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
  
        // Load grammars.  
        recognizer.LoadGrammar(CreateNameGrammar("Smith"));  
        recognizer.LoadGrammar(CreateNameGrammar("Jones"));  
  
        // Disable audio input to the recognizer.  
        recognizer.SetInputToNull();  
  
        // Add handlers for events raised by the EmulateRecognize method.  
        recognizer.SpeechDetected +=  
          new EventHandler<SpeechDetectedEventArgs>(  
            SpeechDetectedHandler);  
        recognizer.SpeechHypothesized +=  
          new EventHandler<SpeechHypothesizedEventArgs>(  
            SpeechHypothesizedHandler);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(  
            SpeechRecognitionRejectedHandler);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
  
        // Start four synchronous emulated recognition operations.  
        TestRecognize(recognizer, "Smith");  
        TestRecognize(recognizer, "Jones");  
        TestRecognize(recognizer, "Mister");  
        TestRecognize(recognizer, "Mister Smith");  
      }  
  
      Console.WriteLine("press any key to exit...");  
      Console.ReadKey(true);  
    }  
  
    // Create a simple name grammar.  
    // Set the grammar name to the surname.  
    private static Grammar CreateNameGrammar(string surname)  
    {  
      GrammarBuilder builder = new GrammarBuilder("mister", 0, 1);  
      builder.Append(surname);  
  
      Grammar nameGrammar = new Grammar(builder);  
      nameGrammar.Name = surname;  
  
      return nameGrammar;  
    }  
  
    // Send emulated input to the recognizer for synchronous recognition.  
    private static void TestRecognize(  
      SpeechRecognitionEngine recognizer, string input)  
    {  
      Console.WriteLine("TestRecognize(\"{0}\")...", input);  
      RecognitionResult result =  
        recognizer.EmulateRecognize(input,CompareOptions.IgnoreCase);  
      if (result != null)  
      {  
        Console.WriteLine("...Recognition result text = {0}",  
          result.Text ?? "<null>");  
      }  
      else  
      {  
        Console.WriteLine("...No recognition result.");  
      }  
      Console.WriteLine();  
    }  
  
    static void SpeechDetectedHandler(  
      object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine(" SpeechDetected event raised.");  
    }  
  
    // Handle events.  
    static void SpeechHypothesizedHandler(  
      object sender, SpeechHypothesizedEventArgs e)  
    {  
      Console.WriteLine(" SpeechHypothesized event raised.");  
      if (e.Result != null)  
      {  
        Console.WriteLine("  Grammar = {0}; Text = {1}",  
          e.Result.Grammar.Name ?? "<none>", e.Result.Text);  
      }  
      else  
      {  
        Console.WriteLine("  No recognition result available.");  
      }  
    }  
  
    static void SpeechRecognitionRejectedHandler(  
      object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine(" SpeechRecognitionRejected event raised.");  
      if (e.Result != null)  
      {  
        string grammarName;  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name ?? "<none>";  
        }  
        else  
        {  
          grammarName = "<not available>";  
        }  
        Console.WriteLine("  Grammar = {0}; Text = {1}",  
          grammarName, e.Result.Text);  
      }  
      else  
      {  
        Console.WriteLine("  No recognition result available.");  
      }  
    }  
  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine(" SpeechRecognized event raised.");  
      if (e.Result != null)  
      {  
        Console.WriteLine("  Grammar = {0}; Text = {1}",  
          e.Result.Grammar.Name ?? "<none>", e.Result.Text);  
      }  
      else  
      {  
        Console.WriteLine("  No recognition result available.");  
      }  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.InvalidOperationException">Для распознавателя нет загруженных грамматик распознавания речи.</exception>
        <exception cref="T:System.ArgumentNullException"><paramref name="inputText" /> — <see langword="null" />.</exception>
        <exception cref="T:System.ArgumentException"><paramref name="inputText" /> является пустой строкой ("").</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
      </Docs>
    </Member>
    <Member MemberName="EmulateRecognize">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.RecognitionResult EmulateRecognize (System.Speech.Recognition.RecognizedWordUnit[] wordUnits, System.Globalization.CompareOptions compareOptions);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance class System.Speech.Recognition.RecognitionResult EmulateRecognize(class System.Speech.Recognition.RecognizedWordUnit[] wordUnits, valuetype System.Globalization.CompareOptions compareOptions) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; System::Speech::Recognition::RecognitionResult ^ EmulateRecognize(cli::array &lt;System::Speech::Recognition::RecognizedWordUnit ^&gt; ^ wordUnits, System::Globalization::CompareOptions compareOptions);" />
      <MemberSignature Language="F#" Value="member this.EmulateRecognize : System.Speech.Recognition.RecognizedWordUnit[] * System.Globalization.CompareOptions -&gt; System.Speech.Recognition.RecognitionResult" Usage="speechRecognitionEngine.EmulateRecognize (wordUnits, compareOptions)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.RecognitionResult</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="wordUnits" Type="System.Speech.Recognition.RecognizedWordUnit[]" />
        <Parameter Name="compareOptions" Type="System.Globalization.CompareOptions" />
      </Parameters>
      <Docs>
        <param name="wordUnits">Массив единиц слов, содержащий входные данные для операции распознавания.</param>
        <param name="compareOptions">Поразрядное сочетание значений перечисления, описывающих тип сравнения, который требуется использовать для эмулируемой операции распознавания.</param>
        <summary>Эмулирует ввод конкретных слов в распознаватель речи, используя текст вместо аудио для синхронного распознавания речи, и указывает способ обработки распознавателем сравнения Юникода между словами и загруженными грамматиками распознавания речи.</summary>
        <returns>Результат операции распознавания или <see langword="null" />, если операция завершилась с ошибкой или распознаватель не включен.</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Распознаватель речи создает <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>события, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>и <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> , как если бы операция распознавания не была эмуляциа.  
  
 Распознаватель использует `compareOptions` при применении грамматических правил к входной фразе. Распознаватели, поставляемые с Vista и Windows 7, не учитывают <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> регистр <xref:System.Globalization.CompareOptions.IgnoreCase> , если имеется значение или. Распознаватель всегда игнорирует ширину символов и никогда не игнорирует тип японской азбуки. Распознаватель также игнорирует новые строки и дополнительные пробелы и рассматривает знаки препинания как литеральные входные данные. Дополнительные сведения о ширине символов и типе японской азбуки см <xref:System.Globalization.CompareOptions> . в разделе перечисление.  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.InvalidOperationException">Для распознавателя нет загруженных грамматик распознавания речи.</exception>
        <exception cref="T:System.ArgumentNullException"><paramref name="wordUnits" /> — <see langword="null" />.</exception>
        <exception cref="T:System.ArgumentException"><paramref name="wordUnits" /> содержит один или несколько элементов <see langword="null" />.</exception>
        <exception cref="T:System.NotSupportedException"><paramref name="compareOptions" /> содержит флаг <see cref="F:System.Globalization.CompareOptions.IgnoreNonSpace" />, <see cref="F:System.Globalization.CompareOptions.IgnoreSymbols" /> или <see cref="F:System.Globalization.CompareOptions.StringSort" />.</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
      </Docs>
    </Member>
    <Member MemberName="EmulateRecognize">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.RecognitionResult EmulateRecognize (string inputText, System.Globalization.CompareOptions compareOptions);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance class System.Speech.Recognition.RecognitionResult EmulateRecognize(string inputText, valuetype System.Globalization.CompareOptions compareOptions) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String,System.Globalization.CompareOptions)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; System::Speech::Recognition::RecognitionResult ^ EmulateRecognize(System::String ^ inputText, System::Globalization::CompareOptions compareOptions);" />
      <MemberSignature Language="F#" Value="member this.EmulateRecognize : string * System.Globalization.CompareOptions -&gt; System.Speech.Recognition.RecognitionResult" Usage="speechRecognitionEngine.EmulateRecognize (inputText, compareOptions)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.RecognitionResult</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="inputText" Type="System.String" />
        <Parameter Name="compareOptions" Type="System.Globalization.CompareOptions" />
      </Parameters>
      <Docs>
        <param name="inputText">Входная фраза для операции распознавания.</param>
        <param name="compareOptions">Поразрядное сочетание значений перечисления, описывающих тип сравнения, который требуется использовать для эмулируемой операции распознавания.</param>
        <summary>Эмулирует ввод фразы в распознаватель речи, используя текст вместо аудио для синхронного распознавания речи, и указывает способ обработки распознавателем сравнения Юникода между фразой и загруженными грамматиками распознавания речи.</summary>
        <returns>Результат операции распознавания или <see langword="null" />, если операция завершилась с ошибкой или распознаватель не включен.</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Распознаватель речи создает <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>события, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>и <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> , как если бы операция распознавания не была эмуляциа.  
  
 Распознаватель использует `compareOptions` при применении грамматических правил к входной фразе. Распознаватели, поставляемые с Vista и Windows 7, не учитывают <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> регистр <xref:System.Globalization.CompareOptions.IgnoreCase> , если имеется значение или. Распознаватель всегда игнорирует ширину символов и никогда не игнорирует тип японской азбуки. Распознаватель также игнорирует новые строки и дополнительные пробелы и рассматривает знаки препинания как литеральные входные данные. Дополнительные сведения о ширине символов и типе японской азбуки см <xref:System.Globalization.CompareOptions> . в разделе перечисление.  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.InvalidOperationException">Для распознавателя нет загруженных грамматик распознавания речи.</exception>
        <exception cref="T:System.ArgumentNullException"><paramref name="inputText" /> — <see langword="null" />.</exception>
        <exception cref="T:System.ArgumentException"><paramref name="inputText" /> является пустой строкой ("").</exception>
        <exception cref="T:System.NotSupportedException"><paramref name="compareOptions" /> содержит флаг <see cref="F:System.Globalization.CompareOptions.IgnoreNonSpace" />, <see cref="F:System.Globalization.CompareOptions.IgnoreSymbols" /> или <see cref="F:System.Globalization.CompareOptions.StringSort" />.</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
      </Docs>
    </Member>
    <MemberGroup MemberName="EmulateRecognizeAsync">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>Эмулирует ввод в распознаватель речи, используя текст вместо аудио для асинхронного распознавания речи.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Эти методы обходят входные звуковые данные системы и предоставляют распознаватель в <xref:System.String> виде объектов или <xref:System.Speech.Recognition.RecognizedWordUnit> массива объектов. Это может быть полезно при тестировании или отладке приложения или грамматики. Например, можно использовать эмуляцию, чтобы определить, находится ли слово в грамматике и какая семантика возвращается при распознавании слова. <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A> Используйте метод, чтобы отключить входные аудио в подсистему распознавания речи во время операций эмуляции.  
  
 Распознаватель речи создает <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>события, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>и <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> , как если бы операция распознавания не была эмуляциа. Когда распознаватель завершает операцию асинхронного распознавания, он вызывает <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> событие. Распознаватель игнорирует новые строки и лишние пробелы и рассматривает знаки препинания как литеральные входные данные.  
  
> [!NOTE]
>  Объект, созданный распознавателем речи в ответ на эмулированные входные данные, имеет `null` значение для его <xref:System.Speech.Recognition.RecognitionResult.Audio%2A> свойства. <xref:System.Speech.Recognition.RecognitionResult>  
  
 Чтобы эмулировать синхронное распознавание, используйте <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize%2A> метод.  
  
 ]]></format>
        </remarks>
      </Docs>
    </MemberGroup>
    <Member MemberName="EmulateRecognizeAsync">
      <MemberSignature Language="C#" Value="public void EmulateRecognizeAsync (string inputText);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void EmulateRecognizeAsync(string inputText) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
      <MemberSignature Language="VB.NET" Value="Public Sub EmulateRecognizeAsync (inputText As String)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void EmulateRecognizeAsync(System::String ^ inputText);" />
      <MemberSignature Language="F#" Value="member this.EmulateRecognizeAsync : string -&gt; unit" Usage="speechRecognitionEngine.EmulateRecognizeAsync inputText" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Attributes>
        <Attribute FrameworkAlternate="netframework-4.0">
          <AttributeName>System.Runtime.TargetedPatchingOptOut("Performance critical to inline this type of method across NGen image boundaries")</AttributeName>
        </Attribute>
      </Attributes>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="inputText" Type="System.String" />
      </Parameters>
      <Docs>
        <param name="inputText">Входные данные для операции распознавания.</param>
        <summary>Эмулирует ввод фразы в распознаватель речи, используя текст вместо аудио для асинхронного распознавания речи.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Распознаватель речи создает <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>события, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>и <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> , как если бы операция распознавания не была эмуляциа. Когда распознаватель завершает операцию асинхронного распознавания, он вызывает <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> событие.  
  
 Распознаватели, поставляемые с Vista и Windows 7, не учитывают регистр и ширину символов при применении грамматических правил к входной фразе. Дополнительные сведения об этом типе сравнения см. в <xref:System.Globalization.CompareOptions> разделе значения <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> перечисления и <xref:System.Globalization.CompareOptions.IgnoreWidth>. Распознаватели также пропускают новые строки и дополнительные пробелы и обрабатывают знаки препинания в качестве литеральных входных данных.  
  
   
  
## Examples  
 Приведенный ниже пример кода является частью консольного приложения, которое демонстрирует асинхронную эмуляцию ввода, связанные результаты распознавания и связанные события, вызванные распознавателем речи. В примере создаются следующие выходные данные.  
  
```  
  
TestRecognizeAsync("Smith")...  
 SpeechDetected event raised.  
 SpeechRecognized event raised.  
  Grammar = Smith; Text = Smith  
 EmulateRecognizeCompleted event raised.  
  Grammar = Smith; Text = Smith  
 Done.  
  
TestRecognizeAsync("Jones")...  
 SpeechDetected event raised.  
 SpeechRecognized event raised.  
  Grammar = Jones; Text = Jones  
 EmulateRecognizeCompleted event raised.  
  Grammar = Jones; Text = Jones  
 Done.  
  
TestRecognizeAsync("Mister")...  
 SpeechDetected event raised.  
 SpeechHypothesized event raised.  
  Grammar = Smith; Text = mister  
 SpeechRecognitionRejected event raised.  
  Grammar = <not available>; Text =  
 EmulateRecognizeCompleted event raised.  
  No recognition result available.  
 Done.  
  
TestRecognizeAsync("Mister Smith")...  
 SpeechDetected event raised.  
 SpeechRecognized event raised.  
  Grammar = Smith; Text = mister Smith  
 EmulateRecognizeCompleted event raised.  
  Grammar = Smith; Text = mister Smith  
 Done.  
  
press any key to exit...  
```  
  
```csharp  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace SreEmulateRecognizeAsync  
{  
  class Program  
  {  
    // Indicate when an asynchronous operation is finished.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
        // Load grammars.  
        recognizer.LoadGrammar(CreateNameGrammar("Smith"));  
        recognizer.LoadGrammar(CreateNameGrammar("Jones"));  
  
        // Configure the audio input.  
        recognizer.SetInputToNull();  
  
        // Add event handlers for the events raised by the  
        // EmulateRecognizeAsync method.  
        recognizer.SpeechDetected +=  
          new EventHandler<SpeechDetectedEventArgs>(  
            SpeechDetectedHandler);  
        recognizer.SpeechHypothesized +=  
          new EventHandler<SpeechHypothesizedEventArgs>(  
            SpeechHypothesizedHandler);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(  
            SpeechRecognitionRejectedHandler);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.EmulateRecognizeCompleted +=  
          new EventHandler<EmulateRecognizeCompletedEventArgs>(  
            EmulateRecognizeCompletedHandler);  
  
        // Start four asynchronous emulated recognition operations.  
        TestRecognizeAsync(recognizer, "Smith");  
        TestRecognizeAsync(recognizer, "Jones");  
        TestRecognizeAsync(recognizer, "Mister");  
        TestRecognizeAsync(recognizer, "Mister Smith");  
      }  
  
      Console.WriteLine("press any key to exit...");  
      Console.ReadKey(true);  
    }  
  
    // Create a simple name grammar.  
    // Set the grammar name to the surname.  
    private static Grammar CreateNameGrammar(string surname)  
    {  
      GrammarBuilder builder = new GrammarBuilder("mister", 0, 1);  
      builder.Append(surname);  
  
      Grammar nameGrammar = new Grammar(builder);  
      nameGrammar.Name = surname;  
  
      return nameGrammar;  
    }  
  
    // Send emulated input to the recognizer for asynchronous  
    // recognition.  
    private static void TestRecognizeAsync(  
      SpeechRecognitionEngine recognizer, string input)  
    {  
      completed = false;  
  
      Console.WriteLine("TestRecognizeAsync(\"{0}\")...", input);  
      recognizer.EmulateRecognizeAsync(input);  
  
      // Wait for the operation to complete.  
      while (!completed)  
      {  
        Thread.Sleep(333);  
      }  
  
      Console.WriteLine(" Done.");  
      Console.WriteLine();  
    }  
  
    static void SpeechDetectedHandler(  
      object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine(" SpeechDetected event raised.");  
    }  
  
    static void SpeechHypothesizedHandler(  
      object sender, SpeechHypothesizedEventArgs e)  
    {  
      Console.WriteLine(" SpeechHypothesized event raised.");  
      if (e.Result != null)  
      {  
        Console.WriteLine("  Grammar = {0}; Text = {1}",  
          e.Result.Grammar.Name ?? "<none>", e.Result.Text);  
      }  
      else  
      {  
        Console.WriteLine("  No recognition result available.");  
      }  
    }  
  
    // Handle events.  
    static void SpeechRecognitionRejectedHandler(  
      object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine(" SpeechRecognitionRejected event raised.");  
      if (e.Result != null)  
      {  
        string grammarName;  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name ?? "<none>";  
        }  
        else  
        {  
          grammarName = "<not available>";  
        }  
        Console.WriteLine("  Grammar = {0}; Text = {1}",  
          grammarName, e.Result.Text);  
      }  
      else  
      {  
        Console.WriteLine("  No recognition result available.");  
      }  
    }  
  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine(" SpeechRecognized event raised.");  
      if (e.Result != null)  
      {  
        Console.WriteLine("  Grammar = {0}; Text = {1}",  
          e.Result.Grammar.Name ?? "<none>", e.Result.Text );  
      }  
      else  
      {  
        Console.WriteLine("  No recognition result available.");  
      }  
    }  
  
    static void EmulateRecognizeCompletedHandler(  
      object sender, EmulateRecognizeCompletedEventArgs e)  
    {  
      Console.WriteLine(" EmulateRecognizeCompleted event raised.");  
  
      if (e.Error != null)  
      {  
        Console.WriteLine("  {0} exception encountered: {1}:",  
          e.Error.GetType().Name, e.Error.Message);  
      }  
      else if (e.Cancelled)  
      {  
        Console.WriteLine("  Operation cancelled.");  
      }  
      else if (e.Result != null)  
      {  
        Console.WriteLine("  Grammar = {0}; Text = {1}",  
          e.Result.Grammar.Name ?? "<none>", e.Result.Text);  
      }  
      else  
      {  
        Console.WriteLine("  No recognition result available.");  
      }  
  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.InvalidOperationException">Для распознавателя нет загруженных грамматик распознавания речи или распознаватель имеет асинхронную операцию распознавания, которая еще не завершена.</exception>
        <exception cref="T:System.ArgumentNullException"><paramref name="inputText" /> — <see langword="null" />.</exception>
        <exception cref="T:System.ArgumentException"><paramref name="inputText" /> является пустой строкой ("").</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="EmulateRecognizeAsync">
      <MemberSignature Language="C#" Value="public void EmulateRecognizeAsync (System.Speech.Recognition.RecognizedWordUnit[] wordUnits, System.Globalization.CompareOptions compareOptions);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void EmulateRecognizeAsync(class System.Speech.Recognition.RecognizedWordUnit[] wordUnits, valuetype System.Globalization.CompareOptions compareOptions) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void EmulateRecognizeAsync(cli::array &lt;System::Speech::Recognition::RecognizedWordUnit ^&gt; ^ wordUnits, System::Globalization::CompareOptions compareOptions);" />
      <MemberSignature Language="F#" Value="member this.EmulateRecognizeAsync : System.Speech.Recognition.RecognizedWordUnit[] * System.Globalization.CompareOptions -&gt; unit" Usage="speechRecognitionEngine.EmulateRecognizeAsync (wordUnits, compareOptions)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="wordUnits" Type="System.Speech.Recognition.RecognizedWordUnit[]" />
        <Parameter Name="compareOptions" Type="System.Globalization.CompareOptions" />
      </Parameters>
      <Docs>
        <param name="wordUnits">Массив единиц слов, содержащий входные данные для операции распознавания.</param>
        <param name="compareOptions">Поразрядное сочетание значений перечисления, описывающих тип сравнения, который требуется использовать для эмулируемой операции распознавания.</param>
        <summary>Эмулирует ввод конкретных слов в общий распознаватель речи, используя массив объектов <see cref="T:System.Speech.Recognition.RecognizedWordUnit" /> вместо аудио для асинхронного распознавания речи, и указывает способ обработки распознавателем сравнения Юникода между словами и загруженными грамматиками распознавания речи.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Распознаватель речи создает <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>события, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>и <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> , как если бы операция распознавания не была эмуляциа. Когда распознаватель завершает операцию асинхронного распознавания, он вызывает <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> событие.  
  
 Распознаватель использует `compareOptions` при применении грамматических правил к входной фразе. Распознаватели, поставляемые с Vista и Windows 7, не учитывают <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> регистр <xref:System.Globalization.CompareOptions.IgnoreCase> , если имеется значение или. Распознаватели всегда пропускают ширину символов и никогда не пропускают тип японской азбуки. Распознаватели также пропускают новые строки и дополнительные пробелы и обрабатывают знаки препинания в качестве литеральных входных данных. Дополнительные сведения о ширине символов и типе японской азбуки см <xref:System.Globalization.CompareOptions> . в разделе перечисление.  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.InvalidOperationException">Для распознавателя нет загруженных грамматик распознавания речи или распознаватель имеет асинхронную операцию распознавания, которая еще не завершена.</exception>
        <exception cref="T:System.ArgumentNullException">Свойство <paramref name="wordUnits" /> имеет значение <see langword="null" />.</exception>
        <exception cref="T:System.ArgumentException"><paramref name="wordUnits" /> содержит один или несколько элементов <see langword="null" />.</exception>
        <exception cref="T:System.NotSupportedException"><paramref name="compareOptions" /> содержит флаг <see cref="F:System.Globalization.CompareOptions.IgnoreNonSpace" />, <see cref="F:System.Globalization.CompareOptions.IgnoreSymbols" /> или <see cref="F:System.Globalization.CompareOptions.StringSort" />.</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="EmulateRecognizeAsync">
      <MemberSignature Language="C#" Value="public void EmulateRecognizeAsync (string inputText, System.Globalization.CompareOptions compareOptions);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void EmulateRecognizeAsync(string inputText, valuetype System.Globalization.CompareOptions compareOptions) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String,System.Globalization.CompareOptions)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void EmulateRecognizeAsync(System::String ^ inputText, System::Globalization::CompareOptions compareOptions);" />
      <MemberSignature Language="F#" Value="member this.EmulateRecognizeAsync : string * System.Globalization.CompareOptions -&gt; unit" Usage="speechRecognitionEngine.EmulateRecognizeAsync (inputText, compareOptions)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="inputText" Type="System.String" />
        <Parameter Name="compareOptions" Type="System.Globalization.CompareOptions" />
      </Parameters>
      <Docs>
        <param name="inputText">Входная фраза для операции распознавания.</param>
        <param name="compareOptions">Поразрядное сочетание значений перечисления, описывающих тип сравнения, который требуется использовать для эмулируемой операции распознавания.</param>
        <summary>Эмулирует ввод фразы в распознаватель речи, используя текст вместо аудио для асинхронного распознавания речи, и указывает способ обработки распознавателем сравнения Юникода между фразой и загруженными грамматиками распознавания речи.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Распознаватель речи создает <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>события, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>и <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> , как если бы операция распознавания не была эмуляциа. Когда распознаватель завершает операцию асинхронного распознавания, он вызывает <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> событие.  
  
 Распознаватель использует `compareOptions` при применении грамматических правил к входной фразе. Распознаватели, поставляемые с Vista и Windows 7, не учитывают <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> регистр <xref:System.Globalization.CompareOptions.IgnoreCase> , если имеется значение или. Распознаватели всегда пропускают ширину символов и никогда не пропускают тип японской азбуки. Распознаватели также пропускают новые строки и дополнительные пробелы и обрабатывают знаки препинания в качестве литеральных входных данных. Дополнительные сведения о ширине символов и типе японской азбуки см <xref:System.Globalization.CompareOptions> . в разделе перечисление.  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.InvalidOperationException">Для распознавателя нет загруженных грамматик распознавания речи или распознаватель имеет асинхронную операцию распознавания, которая еще не завершена.</exception>
        <exception cref="T:System.ArgumentNullException">Свойство <paramref name="inputText" /> имеет значение <see langword="null" />.</exception>
        <exception cref="T:System.ArgumentException"><paramref name="inputText" /> является пустой строкой ("").</exception>
        <exception cref="T:System.NotSupportedException"><paramref name="compareOptions" /> содержит флаг <see cref="F:System.Globalization.CompareOptions.IgnoreNonSpace" />, <see cref="F:System.Globalization.CompareOptions.IgnoreSymbols" /> или <see cref="F:System.Globalization.CompareOptions.StringSort" />.</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="EmulateRecognizeCompleted">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.EmulateRecognizeCompletedEventArgs&gt; EmulateRecognizeCompleted;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.EmulateRecognizeCompletedEventArgs&gt; EmulateRecognizeCompleted" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted" />
      <MemberSignature Language="VB.NET" Value="Public Event EmulateRecognizeCompleted As EventHandler(Of EmulateRecognizeCompletedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::EmulateRecognizeCompletedEventArgs ^&gt; ^ EmulateRecognizeCompleted;" />
      <MemberSignature Language="F#" Value="member this.EmulateRecognizeCompleted : EventHandler&lt;System.Speech.Recognition.EmulateRecognizeCompletedEventArgs&gt; " Usage="member this.EmulateRecognizeCompleted : System.EventHandler&lt;System.Speech.Recognition.EmulateRecognizeCompletedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.EmulateRecognizeCompletedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Возникает, если <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> оформляет асинхронную операцию распознавания эмулированного ввода.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Каждый <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> метод начинает асинхронную операцию распознавания. Вызывает событие, <xref:System.Speech.Recognition.SpeechRecognitionEngine>когда завершаетсяасинхроннаяоперация.<xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted>  
  
 <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> Операция <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>может <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>вызывать события, ,<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>и .<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> Событие — это последнее событие, которое распознаватель создает для данной операции.  
  
 Если эмуляция была успешной, можно получить доступ к результатам распознавания одним из следующих методов:  
  
-   Свойство в объекте в обработчике для события. <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> <xref:System.Speech.Recognition.EmulateRecognizeCompletedEventArgs.Result%2A> <xref:System.Speech.Recognition.EmulateRecognizeCompletedEventArgs>  
  
-   <xref:System.Speech.Recognition.RecognitionEventArgs.Result%2A>свойство в <xref:System.Speech.Recognition.SpeechRecognizedEventArgs> объекте обработчика <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> события.  
  
 Если эмуляция не прошла успешно, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> событие не порождается <xref:System.Speech.Recognition.EmulateRecognizeCompletedEventArgs.Result%2A> и будет иметь значение null.  
  
 Интерфейс <xref:System.Speech.Recognition.EmulateRecognizeCompletedEventArgs> является производным от интерфейса <xref:System.ComponentModel.AsyncCompletedEventArgs>.  
  
 Интерфейс <xref:System.Speech.Recognition.SpeechRecognizedEventArgs> является производным от интерфейса <xref:System.Speech.Recognition.RecognitionEventArgs>.  
  
 При создании делегата <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> определяется метод обработки события. Чтобы связать событие с обработчиком событий, нужно добавить в событие экземпляр делегата. Обработчик событий вызывается всякий раз, когда происходит событие, если делегат не удален. Дополнительные сведения о делегатах обработчиков событий см. в разделе [события и делегаты](https://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 Следующий пример является частью консольного приложения, которое загружает грамматику распознавания речи и демонстрирует асинхронную эмуляцию ввода, связанные результаты распознавания и связанные события, вызванные распознавателем речи.  
  
```  
using System;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace InProcessRecognizer  
{  
  class Program  
  {  
    // Indicate whether the asynchronous emulate recognition  
    // operation has completed.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
  
      // Initialize an instance of an in-process recognizer.  
      using (SpeechRecognitionEngine recognizer =   
        new SpeechRecognitionEngine(new System.Globalization.CultureInfo("en-US")))  
      {  
        // Create and load a sample grammar.  
        Grammar testGrammar =  
          new Grammar(new GrammarBuilder("testing testing"));  
        testGrammar.Name = "Test Grammar";  
        recognizer.LoadGrammar(testGrammar);  
  
        // Attach event handlers for recognition events.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(SpeechRecognizedHandler);  
        recognizer.EmulateRecognizeCompleted +=  
          new EventHandler<EmulateRecognizeCompletedEventArgs>(  
            EmulateRecognizeCompletedHandler);  
  
        completed = false;  
  
        // This EmulateRecognizeAsync call matches the grammar  
        // and generates a SpeechRecognized event.  
        recognizer.EmulateRecognizeAsync("testing testing");  
  
        // Wait for the asynchronous operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
  
        completed = false;  
  
        // This EmulateRecognizeAsync call does not match the grammar  
        // or generate a SpeechRecognized event.  
        recognizer.EmulateRecognizeAsync("testing one two three");  
  
        // Wait for the asynchronous operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechRecognized event.  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      if (e.Result != null)  
      {  
        Console.WriteLine("Result of 1st call to EmulateRecognizeAsync = {0}",  
          e.Result.Text ?? "<no text>");  
        Console.WriteLine();  
      }  
      else  
      {  
        Console.WriteLine("No recognition result");  
      }  
    }  
  
    // Handle the EmulateRecognizeCompleted event.  
    static void EmulateRecognizeCompletedHandler(  
      object sender, EmulateRecognizeCompletedEventArgs e)  
    {  
      if (e.Result == null)  
      {  
        Console.WriteLine("Result of 2nd call to EmulateRecognizeAsync = No result generated.");  
      }  
  
      // Indicate the asynchronous operation is complete.  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.EmulateRecognizeCompletedEventArgs" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="EndSilenceTimeout">
      <MemberSignature Language="C#" Value="public TimeSpan EndSilenceTimeout { get; set; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.TimeSpan EndSilenceTimeout" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
      <MemberSignature Language="VB.NET" Value="Public Property EndSilenceTimeout As TimeSpan" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property TimeSpan EndSilenceTimeout { TimeSpan get(); void set(TimeSpan value); };" />
      <MemberSignature Language="F#" Value="member this.EndSilenceTimeout : TimeSpan with get, set" Usage="System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Attributes>
        <Attribute>
          <AttributeName>System.ComponentModel.EditorBrowsable(System.ComponentModel.EditorBrowsableState.Advanced)</AttributeName>
        </Attribute>
      </Attributes>
      <ReturnValue>
        <ReturnType>System.TimeSpan</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Получает или задает интервал молчания, который <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> будет принимать в конце однозначных вводных данных, прежде чем финализировать операцию распознавания.</summary>
        <value>Длительность периода молчания.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Распознаватель речи использует этот интервал времени ожидания, когда входные данные распознавания являются неоднозначными. Например, для грамматики распознавания речи, которая поддерживает распознавание "новой игры" или "Новая игра", "Новая игра", является однозначным входом, а "Новая игра" является неоднозначным входом.  
  
 Это свойство определяет, как долго обработчик распознавания речи будет ожидать дополнительных входных данных перед завершением операции распознавания. Интервал времени ожидания может составлять от 0 секунд до 10 секунд включительно. Значение по умолчанию — 150 миллисекунд.  
  
 Чтобы задать интервал времени ожидания для неоднозначных входных данных <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> , используйте свойство.  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentOutOfRangeException">Этому свойству задано значение меньше 0 или больше 10 секунд.</exception>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
      </Docs>
    </Member>
    <Member MemberName="EndSilenceTimeoutAmbiguous">
      <MemberSignature Language="C#" Value="public TimeSpan EndSilenceTimeoutAmbiguous { get; set; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.TimeSpan EndSilenceTimeoutAmbiguous" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
      <MemberSignature Language="VB.NET" Value="Public Property EndSilenceTimeoutAmbiguous As TimeSpan" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property TimeSpan EndSilenceTimeoutAmbiguous { TimeSpan get(); void set(TimeSpan value); };" />
      <MemberSignature Language="F#" Value="member this.EndSilenceTimeoutAmbiguous : TimeSpan with get, set" Usage="System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Attributes>
        <Attribute>
          <AttributeName>System.ComponentModel.EditorBrowsable(System.ComponentModel.EditorBrowsableState.Advanced)</AttributeName>
        </Attribute>
      </Attributes>
      <ReturnValue>
        <ReturnType>System.TimeSpan</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Получает или задает интервал молчания, который <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> будет принимать в конце неоднозначных входных данных, прежде чем финализировать операцию распознавания.</summary>
        <value>Длительность периода молчания.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Распознаватель речи использует этот интервал времени ожидания, если входные данные распознавания неоднозначны. Например, для грамматики распознавания речи, которая поддерживает распознавание "новой игры" или "Новая игра", "Новая игра", является однозначным входом, а "Новая игра" является неоднозначным входом.  
  
 Это свойство определяет, как долго обработчик распознавания речи будет ожидать дополнительных входных данных перед завершением операции распознавания. Интервал времени ожидания может составлять от 0 секунд до 10 секунд включительно. Значение по умолчанию — 500 миллисекунд.  
  
 Чтобы задать интервал времени ожидания для однозначных входных данных <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A> , используйте свойство.  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentOutOfRangeException">Этому свойству задано значение меньше 0 или больше 10 секунд.</exception>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
      </Docs>
    </Member>
    <Member MemberName="Grammars">
      <MemberSignature Language="C#" Value="public System.Collections.ObjectModel.ReadOnlyCollection&lt;System.Speech.Recognition.Grammar&gt; Grammars { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance class System.Collections.ObjectModel.ReadOnlyCollection`1&lt;class System.Speech.Recognition.Grammar&gt; Grammars" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.Grammars" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property Grammars As ReadOnlyCollection(Of Grammar)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property System::Collections::ObjectModel::ReadOnlyCollection&lt;System::Speech::Recognition::Grammar ^&gt; ^ Grammars { System::Collections::ObjectModel::ReadOnlyCollection&lt;System::Speech::Recognition::Grammar ^&gt; ^ get(); };" />
      <MemberSignature Language="F#" Value="member this.Grammars : System.Collections.ObjectModel.ReadOnlyCollection&lt;System.Speech.Recognition.Grammar&gt;" Usage="System.Speech.Recognition.SpeechRecognitionEngine.Grammars" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Collections.ObjectModel.ReadOnlyCollection&lt;System.Speech.Recognition.Grammar&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Получает коллекцию объектов <see cref="T:System.Speech.Recognition.Grammar" />, загруженных в данных экземпляр <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />.</summary>
        <value>Коллекция объектов <see cref="T:System.Speech.Recognition.Grammar" />.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Examples  
 В следующем примере информация выводится на консоль для каждой грамматики распознавания речи, которая в настоящее время загружена распознавателем речи.  
  
> [!IMPORTANT]
>  Скопируйте коллекцию грамматики, чтобы избежать ошибок при изменении коллекции, когда этот метод перечисляет элементы коллекции.  
  
```csharp  
  
private static void ListGrammars(SpeechRecognitionEngine recognizer)  
{  
  string qualifier;  
  List<Grammar> grammars = new List<Grammar>(recognizer.Grammars);  
  foreach (Grammar g in grammars)  
  {  
    qualifier = (g.Enabled) ? "enabled" : "disabled";  
  
    Console.WriteLine("Grammar {0} is loaded and is {1}.",  
      g.Name, qualifier);  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.Grammar" />
      </Docs>
    </Member>
    <Member MemberName="InitialSilenceTimeout">
      <MemberSignature Language="C#" Value="public TimeSpan InitialSilenceTimeout { get; set; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.TimeSpan InitialSilenceTimeout" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout" />
      <MemberSignature Language="VB.NET" Value="Public Property InitialSilenceTimeout As TimeSpan" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property TimeSpan InitialSilenceTimeout { TimeSpan get(); void set(TimeSpan value); };" />
      <MemberSignature Language="F#" Value="member this.InitialSilenceTimeout : TimeSpan with get, set" Usage="System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Attributes>
        <Attribute>
          <AttributeName>System.ComponentModel.EditorBrowsable(System.ComponentModel.EditorBrowsableState.Advanced)</AttributeName>
        </Attribute>
      </Attributes>
      <ReturnValue>
        <ReturnType>System.TimeSpan</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Получает или задает интервал времени, в течение которого <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> принимает входные данные, содержащие только молчание, прежде чем финализировать распознавание.</summary>
        <value>Длительность периода молчания.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Каждый распознаватель речи имеет алгоритм для различения тишины и речи. Если входные данные распознавателя заменяются в течение начального времени ожидания тишины, распознаватель завершает эту операцию распознавания.  
  
-   Для асинхронных <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> операций и эмуляции распознавания распознаватель создает событие, <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.InitialSilenceTimeout%2A?displayProperty=nameWithType> где <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.Result%2A?displayProperty=nameWithType> свойство имеет `true`значение, а свойство — `null`.  
  
-   Для синхронных операций распознавания и эмуляции распознаватель возвращает `null`, а не является допустимым. <xref:System.Speech.Recognition.RecognitionResult>  
  
 Если для начального интервала бездействия бездействия установлено значение 0, то распознаватель не выполняет начальную проверку времени ожидания тишины. Интервал времени ожидания может быть любым неотрицательным значением. Значение по умолчанию — 0 секунд.  
  
   
  
## Examples  
 В следующем примере показана часть консольного приложения, демонстрирующее базовое распознавание речи. В примере <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A> задаются <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> свойства <xref:System.Speech.Recognition.SpeechRecognitionEngine> и перед инициацией распознавания речи. Обработчики для событий распознавания <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioStateChanged> речи и <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> выводят сведения о событиях на консоль, чтобы продемонстрировать, <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> <xref:System.Speech.Recognition.SpeechRecognitionEngine> как свойства свойств влияют на операции распознавания.  
  
```csharp  
  
using System;  
using System.Speech.Recognition;  
  
namespace SpeechRecognitionApp  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Initialize an in-process speech recognizer.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(  
          new System.Globalization.CultureInfo("en-US")))  
      {  
        // Load a Grammar object.  
        recognizer.LoadGrammar(CreateServicesGrammar("FindServices"));  
  
        // Add event handlers.  
        recognizer.AudioStateChanged +=  
          new EventHandler<AudioStateChangedEventArgs>(  
            AudioStateChangedHandler);  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(  
            RecognizeCompletedHandler);  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        recognizer.InitialSilenceTimeout = TimeSpan.FromSeconds(3);  
        recognizer.BabbleTimeout = TimeSpan.FromSeconds(2);  
        recognizer.EndSilenceTimeout = TimeSpan.FromSeconds(1);  
        recognizer.EndSilenceTimeoutAmbiguous = TimeSpan.FromSeconds(1.5);  
  
        Console.WriteLine("BabbleTimeout: {0}", recognizer.BabbleTimeout);  
        Console.WriteLine("InitialSilenceTimeout: {0}", recognizer.InitialSilenceTimeout);  
        Console.WriteLine("EndSilenceTimeout: {0}", recognizer.EndSilenceTimeout);  
        Console.WriteLine("EndSilenceTimeoutAmbiguous: {0}", recognizer.EndSilenceTimeoutAmbiguous);  
        Console.WriteLine();  
  
        // Start asynchronous speech recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Single);  
  
        // Keep the console window open.  
        while (true)  
        {  
          Console.ReadLine();  
        }  
      }  
    }  
  
    // Create a grammar and build it into a Grammar object.   
    static Grammar CreateServicesGrammar(string grammarName)  
    {  
  
      // Create a grammar for finding services in different cities.  
      Choices services = new Choices(new string[] { "restaurants", "hotels", "gas stations" });  
      Choices cities = new Choices(new string[] { "Seattle", "Boston", "Dallas" });  
  
      GrammarBuilder findServices = new GrammarBuilder("Find");  
      findServices.Append(services);  
      findServices.Append("near");  
      findServices.Append(cities);  
  
      // Create a Grammar object from the GrammarBuilder. 
      Grammar servicesGrammar = new Grammar(findServices);  
      servicesGrammar.Name = ("FindServices");  
      return servicesGrammar;  
    }  
  
    // Handle the AudioStateChanged event.  
    static void AudioStateChangedHandler(  
      object sender, AudioStateChangedEventArgs e)  
    {  
      Console.WriteLine("AudioStateChanged ({0}): {1}",  
        DateTime.Now.ToString("mm:ss.f"), e.AudioState);  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void RecognizeCompletedHandler(  
      object sender, RecognizeCompletedEventArgs e)  
    {  
      Console.WriteLine("RecognizeCompleted ({0}):",  
        DateTime.Now.ToString("mm:ss.f"));  
  
      string resultText;  
      if (e.Result != null) { resultText = e.Result.Text; }  
      else { resultText = "<null>"; }  
  
      Console.WriteLine(  
        " BabbleTimeout: {0}; InitialSilenceTimeout: {1}; Result text: {2}",  
        e.BabbleTimeout, e.InitialSilenceTimeout, resultText);  
      if (e.Error != null)  
      {  
        Console.WriteLine(" Exception message: ", e.Error.Message);  
      }  
  
      // Start the next asynchronous recognition operation.  
      ((SpeechRecognitionEngine)sender).RecognizeAsync(RecognizeMode.Single);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentOutOfRangeException">Этому свойству задано значение меньше 0 секунд.</exception>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
      </Docs>
    </Member>
    <Member MemberName="InstalledRecognizers">
      <MemberSignature Language="C#" Value="public static System.Collections.ObjectModel.ReadOnlyCollection&lt;System.Speech.Recognition.RecognizerInfo&gt; InstalledRecognizers ();" />
      <MemberSignature Language="ILAsm" Value=".method public static hidebysig class System.Collections.ObjectModel.ReadOnlyCollection`1&lt;class System.Speech.Recognition.RecognizerInfo&gt; InstalledRecognizers() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers" />
      <MemberSignature Language="VB.NET" Value="Public Shared Function InstalledRecognizers () As ReadOnlyCollection(Of RecognizerInfo)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; static System::Collections::ObjectModel::ReadOnlyCollection&lt;System::Speech::Recognition::RecognizerInfo ^&gt; ^ InstalledRecognizers();" />
      <MemberSignature Language="F#" Value="static member InstalledRecognizers : unit -&gt; System.Collections.ObjectModel.ReadOnlyCollection&lt;System.Speech.Recognition.RecognizerInfo&gt;" Usage="System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers " />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Collections.ObjectModel.ReadOnlyCollection&lt;System.Speech.Recognition.RecognizerInfo&gt;</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Возвращает сведения для всех установленных распознавателей речи в текущей системе.</summary>
        <returns>Доступная только для чтения коллекция объектов <see cref="T:System.Speech.Recognition.RecognizerInfo" />, описывающих установленные распознаватели.</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Чтобы получить сведения о текущем распознавателе, используйте <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerInfo%2A> свойство.  
  
   
  
## Examples  
 В следующем примере показана часть консольного приложения, демонстрирующее базовое распознавание речи. В примере коллекция, возвращаемая <xref:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers%2A> методом, используется для поиска распознавателя речи, поддерживающего английский язык.  
  
```csharp  
using System;  
using System.Speech.Recognition;  
  
namespace SpeechRecognitionApp  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Select a speech recognizer that supports English.  
      RecognizerInfo info = null;  
      foreach (RecognizerInfo ri in SpeechRecognitionEngine.InstalledRecognizers())  
      {  
        if (ri.Culture.TwoLetterISOLanguageName.Equals("en"))  
        {  
          info = ri;  
          break;  
        }  
      }  
      if (info == null) return;  
  
      // Create the selected recognizer.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(info))  
      {  
  
        // Create and load a dictation grammar.  
        recognizer.LoadGrammar(new DictationGrammar());  
  
        // Add a handler for the speech recognized event.  
        recognizer.SpeechRecognized +=   
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start asynchronous, continuous speech recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Keep the console window open.  
        while (true)  
        {  
          Console.ReadLine();  
        }  
      }  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Recognized text: " + e.Result.Text);  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.#ctor(System.Speech.Recognition.RecognizerInfo)" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerInfo" />
      </Docs>
    </Member>
    <Member MemberName="LoadGrammar">
      <MemberSignature Language="C#" Value="public void LoadGrammar (System.Speech.Recognition.Grammar grammar);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void LoadGrammar(class System.Speech.Recognition.Grammar grammar) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar(System.Speech.Recognition.Grammar)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void LoadGrammar(System::Speech::Recognition::Grammar ^ grammar);" />
      <MemberSignature Language="F#" Value="member this.LoadGrammar : System.Speech.Recognition.Grammar -&gt; unit" Usage="speechRecognitionEngine.LoadGrammar grammar" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="grammar" Type="System.Speech.Recognition.Grammar" />
      </Parameters>
      <Docs>
        <param name="grammar">Объект грамматики для загрузки.</param>
        <summary>Выполняет синхронную загрузку объекта <see cref="T:System.Speech.Recognition.Grammar" />.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Распознаватель создает исключение, если <xref:System.Speech.Recognition.Grammar> объект уже загружен, асинхронно загружен или не удалось загрузить в любой распознаватель. Один и тот же <xref:System.Speech.Recognition.Grammar> объект нельзя загрузить в несколько <xref:System.Speech.Recognition.SpeechRecognitionEngine>экземпляров. Вместо этого создайте новый <xref:System.Speech.Recognition.Grammar> объект для каждого <xref:System.Speech.Recognition.SpeechRecognitionEngine> экземпляра.  
  
 Если распознаватель запущен, приложения должны использовать <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> для приостановки модуля распознавания речи перед загрузкой, выгрузкой, включением или отключением грамматики.  
  
 При загрузке грамматики она включена по умолчанию. Чтобы отключить загруженную грамматику, используйте <xref:System.Speech.Recognition.Grammar.Enabled%2A> свойство.  
  
 Для асинхронной <xref:System.Speech.Recognition.Grammar> загрузки объекта <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> используйте метод.  
  
   
  
## Examples  
 В следующем примере показана часть консольного приложения, демонстрирующее базовое распознавание речи. В примере создается <xref:System.Speech.Recognition.DictationGrammar> и загружается в распознаватель речи.  
  
```csharp  
using System;  
using System.Speech.Recognition;  
  
namespace SpeechRecognitionApp  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Create an in-process speech recognizer for the en-US locale.  
      using (  
      SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(  
          new System.Globalization.CultureInfo("en-US")))  
      {  
  
        // Create and load a dictation grammar.  
        recognizer.LoadGrammar(new DictationGrammar());  
  
        // Add a handler for the speech recognized event.  
        recognizer.SpeechRecognized +=   
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start asynchronous, continuous speech recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Keep the console window open.  
        while (true)  
        {  
          Console.ReadLine();  
        }  
      }  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Recognized text: " + e.Result.Text);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentNullException">Свойство <paramref name="Grammar" /> имеет значение <see langword="null" />.</exception>
        <exception cref="T:System.InvalidOperationException">Состояние <paramref name="Grammar" /> недопустимо.</exception>
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.UnloadAllGrammars" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.UnloadGrammar(System.Speech.Recognition.Grammar)" />
      </Docs>
    </Member>
    <Member MemberName="LoadGrammarAsync">
      <MemberSignature Language="C#" Value="public void LoadGrammarAsync (System.Speech.Recognition.Grammar grammar);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void LoadGrammarAsync(class System.Speech.Recognition.Grammar grammar) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync(System.Speech.Recognition.Grammar)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void LoadGrammarAsync(System::Speech::Recognition::Grammar ^ grammar);" />
      <MemberSignature Language="F#" Value="member this.LoadGrammarAsync : System.Speech.Recognition.Grammar -&gt; unit" Usage="speechRecognitionEngine.LoadGrammarAsync grammar" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="grammar" Type="System.Speech.Recognition.Grammar" />
      </Parameters>
      <Docs>
        <param name="grammar">Грамматика распознавания речи для загрузки.</param>
        <summary>Выполняет асинхронную загрузку грамматики распознавания речи.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Когда распознаватель завершает загрузку <xref:System.Speech.Recognition.Grammar> объекта, он <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarCompleted> вызывает событие. Распознаватель создает исключение, если <xref:System.Speech.Recognition.Grammar> объект уже загружен, асинхронно загружен или не удалось загрузить в любой распознаватель. Один и тот же <xref:System.Speech.Recognition.Grammar> объект нельзя загрузить в несколько <xref:System.Speech.Recognition.SpeechRecognitionEngine>экземпляров. Вместо этого создайте новый <xref:System.Speech.Recognition.Grammar> объект для каждого <xref:System.Speech.Recognition.SpeechRecognitionEngine> экземпляра.  
  
 Если распознаватель запущен, приложения должны использовать <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> для приостановки модуля распознавания речи перед загрузкой, выгрузкой, включением или отключением грамматики.  
  
 При загрузке грамматики она включена по умолчанию. Чтобы отключить загруженную грамматику, используйте <xref:System.Speech.Recognition.Grammar.Enabled%2A> свойство.  
  
 Для синхронной загрузки грамматики распознавания речи используйте <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A> метод.  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentNullException"><paramref name="Grammar" /> — <see langword="null" />.</exception>
        <exception cref="T:System.InvalidOperationException">Состояние <paramref name="Grammar" /> недопустимо.</exception>
        <exception cref="T:System.OperationCanceledException">Асинхронная операция была отменена.</exception>
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarCompleted" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.UnloadAllGrammars" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.UnloadGrammar(System.Speech.Recognition.Grammar)" />
      </Docs>
    </Member>
    <Member MemberName="LoadGrammarCompleted">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.LoadGrammarCompletedEventArgs&gt; LoadGrammarCompleted;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.LoadGrammarCompletedEventArgs&gt; LoadGrammarCompleted" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarCompleted" />
      <MemberSignature Language="VB.NET" Value="Public Event LoadGrammarCompleted As EventHandler(Of LoadGrammarCompletedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::LoadGrammarCompletedEventArgs ^&gt; ^ LoadGrammarCompleted;" />
      <MemberSignature Language="F#" Value="member this.LoadGrammarCompleted : EventHandler&lt;System.Speech.Recognition.LoadGrammarCompletedEventArgs&gt; " Usage="member this.LoadGrammarCompleted : System.EventHandler&lt;System.Speech.Recognition.LoadGrammarCompletedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.LoadGrammarCompletedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Возникает при завершении <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> асинхронной загрузки объекта <see cref="T:System.Speech.Recognition.Grammar" />.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> Метод распознавателя инициирует асинхронную операцию. <xref:System.Speech.Recognition.SpeechRecognitionEngine> Вызывает это событие при завершении операции. Чтобы получить <xref:System.Speech.Recognition.Grammar> объект, загруженный распознавателем, <xref:System.Speech.Recognition.LoadGrammarCompletedEventArgs.Grammar%2A> используйте свойство связанного <xref:System.Speech.Recognition.LoadGrammarCompletedEventArgs>объекта. Чтобы получить текущие <xref:System.Speech.Recognition.Grammar> объекты, загруженные распознавателем, используйте <xref:System.Speech.Recognition.SpeechRecognitionEngine.Grammars%2A> свойство распознавателя.  
  
 Если распознаватель запущен, приложения должны использовать <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> для приостановки модуля распознавания речи перед загрузкой, выгрузкой, включением или отключением грамматики.  
  
 При создании делегата <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarCompleted> необходимо указать метод, обрабатывающий событие. Чтобы связать событие с обработчиком событий, нужно добавить в событие экземпляр делегата. Обработчик событий вызывается всякий раз, когда происходит событие, если делегат не удален. Дополнительные сведения о делегатах обработчиков событий см. в разделе [события и делегаты](https://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 В следующем примере создается внутрипроцессный распознаватель речи, а затем создаются два типа грамматики для распознавания конкретных слов и принятия бесплатной диктовки. В примере создается <xref:System.Speech.Recognition.Grammar> объект из каждой завершенной грамматики распознавания речи, а затем происходит асинхронная <xref:System.Speech.Recognition.Grammar> загрузка объектов в <xref:System.Speech.Recognition.SpeechRecognitionEngine> экземпляр. Обработчики для событий распознавателя <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarCompleted> и <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> записывают в <xref:System.Speech.Recognition.Grammar> консоль имя объекта, который использовался для выполнения распознавания, и текста результата распознавания соответственно.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    private static SpeechRecognitionEngine recognizer;  
    public static void Main(string[] args)  
    {  
  
      // Initialize an in-process speech recognition engine and set its input.  
      recognizer = new SpeechRecognitionEngine();  
      recognizer.SetInputToDefaultAudioDevice();  
  
      // Add a handler for the LoadGrammarCompleted event.  
      recognizer.LoadGrammarCompleted +=  
        new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
  
      // Add a handler for the SpeechRecognized event.  
      recognizer.SpeechRecognized +=  
        new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
      // Create the "yesno" grammar.  
      Choices yesChoices = new Choices(new string[] { "yes", "yup", "yeah" });  
      SemanticResultValue yesValue =  
          new SemanticResultValue(yesChoices, (bool)true);  
      Choices noChoices = new Choices(new string[] { "no", "nope", "neah" });  
      SemanticResultValue noValue =  
          new SemanticResultValue(noChoices, (bool)false);  
      SemanticResultKey yesNoKey =  
          new SemanticResultKey("yesno", new Choices(new GrammarBuilder[] { yesValue, noValue }));  
      Grammar yesnoGrammar = new Grammar(yesNoKey);  
      yesnoGrammar.Name = "yesNo";  
  
      // Create the "done" grammar.  
      Grammar doneGrammar =  
        new Grammar(new Choices(new string[] { "done", "exit", "quit", "stop" }));  
      doneGrammar.Name = "Done";  
  
      // Create a dictation grammar.  
      Grammar dictation = new DictationGrammar();  
      dictation.Name = "Dictation";  
  
      // Load grammars to the recognizer.  
      recognizer.LoadGrammarAsync(yesnoGrammar);  
      recognizer.LoadGrammarAsync(doneGrammar);  
      recognizer.LoadGrammarAsync(dictation);  
  
      // Start asynchronous, continuous recognition.  
      recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
      // Keep the console window open.  
      Console.ReadLine();  
    }  
  
    // Handle the LoadGrammarCompleted event.   
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      string grammarName = e.Grammar.Name;  
      bool grammarLoaded = e.Grammar.Loaded;  
  
      if (e.Error != null)  
      {  
        Console.WriteLine("LoadGrammar for {0} failed with a {1}.",  
        grammarName, e.Error.GetType().Name);  
  
        // Add exception handling code here.  
      }  
  
      Console.WriteLine("Grammar {0} {1} loaded.",  
      grammarName, (grammarLoaded) ? "is" : "is not");  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Grammar({0}): {1}", e.Result.Grammar.Name, e.Result.Text);  
  
      // Add event handler code here.  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached" />
        <altmember cref="T:System.Speech.Recognition.Grammar" />
        <altmember cref="T:System.Speech.Recognition.LoadGrammarCompletedEventArgs" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync(System.Speech.Recognition.Grammar)" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.Grammars" />
      </Docs>
    </Member>
    <Member MemberName="MaxAlternates">
      <MemberSignature Language="C#" Value="public int MaxAlternates { get; set; }" />
      <MemberSignature Language="ILAsm" Value=".property instance int32 MaxAlternates" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.MaxAlternates" />
      <MemberSignature Language="VB.NET" Value="Public Property MaxAlternates As Integer" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property int MaxAlternates { int get(); void set(int value); };" />
      <MemberSignature Language="F#" Value="member this.MaxAlternates : int with get, set" Usage="System.Speech.Recognition.SpeechRecognitionEngine.MaxAlternates" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Int32</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Получает или задает максимальное количество альтернативных результатов распознавания, которые <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> возвращает для каждой операции распознавания.</summary>
        <value>Число альтернативных возвращаемых результатов.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 <xref:System.Speech.Recognition.RecognitionResult.Alternates%2A> Свойство<xref:System.Speech.Recognition.RecognitionResult> класса содержит коллекцию объектов,представляющихвозможныеинтерпретациивходныхданных.<xref:System.Speech.Recognition.RecognizedPhrase>  
  
 Значение по умолчанию <xref:System.Speech.Recognition.SpeechRecognitionEngine.MaxAlternates%2A> для равно 10.  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentOutOfRangeException">Для свойства <see cref="P:System.Speech.Recognition.SpeechRecognitionEngine.MaxAlternates" /> задано значение менее 0.</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
      </Docs>
    </Member>
    <Member MemberName="QueryRecognizerSetting">
      <MemberSignature Language="C#" Value="public object QueryRecognizerSetting (string settingName);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance object QueryRecognizerSetting(string settingName) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.QueryRecognizerSetting(System.String)" />
      <MemberSignature Language="VB.NET" Value="Public Function QueryRecognizerSetting (settingName As String) As Object" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; System::Object ^ QueryRecognizerSetting(System::String ^ settingName);" />
      <MemberSignature Language="F#" Value="member this.QueryRecognizerSetting : string -&gt; obj" Usage="speechRecognitionEngine.QueryRecognizerSetting settingName" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Object</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="settingName" Type="System.String" />
      </Parameters>
      <Docs>
        <param name="settingName">Имя возвращаемого параметра.</param>
        <summary>Возвращает значения параметров для распознавателя.</summary>
        <returns>Значение параметра.</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Параметры распознавателя могут содержать строку, 64-разрядное целое число или данные адреса памяти. В следующей таблице описаны параметры, определенные для распознавателя, совместимого с Microsoft Speech API (SAPI). Следующие параметры должны иметь один и тот же диапазон для каждого распознавателя, который поддерживает этот параметр. Совместимый с SAPI распознаватель не требуется для поддержки этих параметров и может поддерживать другие параметры.  
  
|name|Описание|  
|----------|-----------------|  
|`ResourceUsage`|Указывает использование ЦП распознавателем. Диапазон — от 0 до 100. Значение по умолчанию — 50.|  
|`ResponseSpeed`|Указывает длину тишины в конце однозначных входных данных до того, как распознаватель речи Завершает операцию распознавания. Значение в диапазоне от 0 до 10 000 миллисекунд (МС). Этот параметр соответствует <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A> свойству распознавателя.  Значение по умолчанию — 150ms.|  
|`ComplexResponseSpeed`|Указывает длину тишины в конце неоднозначных входных данных до того, как распознаватель речи Завершает операцию распознавания. Диапазон — от 0 до 10, 000 мс.. Этот параметр соответствует <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> свойству распознавателя. Значение по умолчанию — 500 мс.|  
|`AdaptationOn`|Указывает, включена ли адаптация акустической модели (value = `1`) или OFF (значение = `0`). Значение по умолчанию `1` — (on).|  
|`PersistedBackgroundAdaptation`|Указывает, включена ли фоновая адаптация (value `1`=) или OFF (значение `0`=), и сохраняет параметр в реестре. Значение по умолчанию `1` — (on).|  
  
 Чтобы обновить параметр для распознавателя, используйте один из <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> методов.  
  
   
  
## Examples  
 Следующий пример является частью консольного приложения, которое выводит значения для ряда параметров, определенных для распознавателя, поддерживающего языковой стандарт EN-US. В примере создаются следующие выходные данные.  
  
```  
Settings for recognizer MS-1033-80-DESK:  
  
  ResourceUsage                  is not supported by this recognizer.  
  ResponseSpeed                  = 150  
  ComplexResponseSpeed           = 500  
  AdaptationOn                   = 1  
  PersistedBackgroundAdaptation  = 1  
  
Press any key to exit...  
```  
  
```csharp  
  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
  
namespace RecognizerSettings  
{  
  class Program  
  {  
    static readonly string[] settings = new string[] {  
      "ResourceUsage",  
      "ResponseSpeed",  
      "ComplexResponseSpeed",  
      "AdaptationOn",  
      "PersistedBackgroundAdaptation"  
    };  
  
    static void Main(string[] args)  
    {  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new System.Globalization.CultureInfo("en-US")))  
      {  
        Console.WriteLine("Settings for recognizer {0}:",  
          recognizer.RecognizerInfo.Name);  
        Console.WriteLine();  
  
        foreach (string setting in settings)  
        {  
          try  
          {  
            object value = recognizer.QueryRecognizerSetting(setting);  
            Console.WriteLine("  {0,-30} = {1}", setting, value);  
          }  
          catch  
          {  
            Console.WriteLine("  {0,-30} is not supported by this recognizer.",  
              setting);  
          }  
        }  
      }  
      Console.WriteLine();  
  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentNullException"><paramref name="settingName" /> — <see langword="null" />.</exception>
        <exception cref="T:System.ArgumentException"><paramref name="settingName" /> является пустой строкой ("").</exception>
        <exception cref="T:System.Collections.Generic.KeyNotFoundException">Распознаватель не имеет параметра с указанным именем.</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting(System.String,System.Int32)" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
      </Docs>
    </Member>
    <MemberGroup MemberName="Recognize">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>Запускает синхронную операцию распознавания речи.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Эти методы выполняют одну синхронную операцию распознавания. Распознаватель выполняет эту операцию по загруженным и включенным грамматикам распознавания речи.  
  
 При вызове этого метода распознаватель может создавать следующие события:  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>.  Возникает, когда распознаватель обнаруживает ввод, который может быть идентифицирован как речь.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>.  Вызывается, когда входные данные создают неоднозначное соответствие с одной из активных грамматик.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> или <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>. Вызывается, когда распознаватель завершает операцию распознавания.  
  
 Распознаватель не вызывает <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> событие при использовании одного <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A> из методов.  
  
 Методы возвращают объект или `null` , если операция не выполнена, или распознаватель не включен. <xref:System.Speech.Recognition.RecognitionResult> <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A>  
  
 Операция синхронного распознавания может завершиться сбоем по следующим причинам:  
  
-   Распознавание речи не определяется до истечения времени ожидания для <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A> `initialSilenceTimeout` свойств <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> или или <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A> для параметра метода.  
  
-   Механизм распознавания обнаруживает распознавание речи, но не находит совпадений ни в одном из <xref:System.Speech.Recognition.Grammar> загруженных и включенных объектов.  
  
 Чтобы изменить, как распознаватель обрабатывает время речи или тишины в отношении <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>распознавания, используйте свойства, <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A>и <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> .  
  
 Прежде <xref:System.Speech.Recognition.SpeechRecognitionEngine> чем выполнять распознавание, <xref:System.Speech.Recognition.Grammar> необходимо загрузить по крайней мере один объект. Чтобы загрузить грамматику распознавания речи, используйте <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A> метод или. <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A>  
  
 Чтобы выполнить асинхронное распознавание, используйте один <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> из методов.  
  
 ]]></format>
        </remarks>
      </Docs>
    </MemberGroup>
    <Member MemberName="Recognize">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.RecognitionResult Recognize ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance class System.Speech.Recognition.RecognitionResult Recognize() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
      <MemberSignature Language="VB.NET" Value="Public Function Recognize () As RecognitionResult" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; System::Speech::Recognition::RecognitionResult ^ Recognize();" />
      <MemberSignature Language="F#" Value="member this.Recognize : unit -&gt; System.Speech.Recognition.RecognitionResult" Usage="speechRecognitionEngine.Recognize " />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.RecognitionResult</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Выполняет синхронную операцию распознавания речи.</summary>
        <returns>Результат распознавания для ввода или <see langword="null" />, если операция завершилась с ошибкой или распознаватель не включен.</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Этот метод выполняет одну операцию распознавания. Распознаватель выполняет эту операцию по загруженным и включенным грамматикам распознавания речи.  
  
 При вызове этого метода распознаватель может создавать следующие события:  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>.  Возникает, когда распознаватель обнаруживает ввод, который может быть идентифицирован как речь.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>.  Вызывается, когда входные данные создают неоднозначное соответствие с одной из активных грамматик.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> или <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>. Вызывается, когда распознаватель завершает операцию распознавания.  
  
 Распознаватель не вызывает <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> событие при использовании этого метода.  
  
 Метод возвращает объект или`null` значение, если операция не выполнена. <xref:System.Speech.Recognition.RecognitionResult> <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize>  
  
 Операция синхронного распознавания может завершиться сбоем по следующим причинам:  
  
-   Распознавание речи не определяется до истечения времени ожидания для <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A> свойств <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> или.  
  
-   Механизм распознавания обнаруживает распознавание речи, но не находит совпадений ни в одном из <xref:System.Speech.Recognition.Grammar> загруженных и включенных объектов.  
  
 Чтобы выполнить асинхронное распознавание, используйте один <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> из методов.  
  
   
  
## Examples  
 В следующем примере показана часть консольного приложения, демонстрирующее базовое распознавание речи. В примере создается <xref:System.Speech.Recognition.DictationGrammar>, загружается в внутрипроцессный распознаватель речи и выполняется одна операция распознавания.  
  
```  
  
using System;  
using System.Speech.Recognition;  
  
namespace SynchronousRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
      // Create an in-process speech recognizer for the en-US locale.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(  
          new System.Globalization.CultureInfo("en-US")))  
      {  
  
        // Create and load a dictation grammar.  
        recognizer.LoadGrammar(new DictationGrammar());  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Modify the initial silence time-out value.  
        recognizer.InitialSilenceTimeout = TimeSpan.FromSeconds(5);  
  
        // Start synchronous speech recognition.  
        RecognitionResult result = recognizer.Recognize();  
  
        if (result != null)  
        {  
          Console.WriteLine("Recognized text = {0}", result.Text);  
        }  
        else  
        {  
          Console.WriteLine("No recognition result available.");  
        }  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to continue...");  
      Console.ReadKey();  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String)" />
      </Docs>
    </Member>
    <Member MemberName="Recognize">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.RecognitionResult Recognize (TimeSpan initialSilenceTimeout);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance class System.Speech.Recognition.RecognitionResult Recognize(valuetype System.TimeSpan initialSilenceTimeout) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize(System.TimeSpan)" />
      <MemberSignature Language="VB.NET" Value="Public Function Recognize (initialSilenceTimeout As TimeSpan) As RecognitionResult" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; System::Speech::Recognition::RecognitionResult ^ Recognize(TimeSpan initialSilenceTimeout);" />
      <MemberSignature Language="F#" Value="member this.Recognize : TimeSpan -&gt; System.Speech.Recognition.RecognitionResult" Usage="speechRecognitionEngine.Recognize initialSilenceTimeout" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.RecognitionResult</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="initialSilenceTimeout" Type="System.TimeSpan" />
      </Parameters>
      <Docs>
        <param name="initialSilenceTimeout">Интервал времени, в течение которого распознаватель речи принимает входные данные, содержащие только тишину, перед завершением распознавания.</param>
        <summary>Выполняет синхронную операцию распознавания речи с указанным начальным временем ожидания бездействия.</summary>
        <returns>Результат распознавания для ввода или <see langword="null" />, если операция завершилась с ошибкой или распознаватель не включен.</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Если модуль распознавания речи обнаруживает распознавание речи в течение интервала времени `initialSilenceTimeout` , указанного <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%28System.TimeSpan%29> аргументом, выполняет одну операцию распознавания и завершается.  Параметр заменяет <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> свойство распознавателя. `initialSilenceTimeout`  
  
 При вызове этого метода распознаватель может создавать следующие события:  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>.  Возникает, когда распознаватель обнаруживает ввод, который может быть идентифицирован как речь.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>.  Вызывается, когда входные данные создают неоднозначное соответствие с одной из активных грамматик.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> или <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>. Вызывается, когда распознаватель завершает операцию распознавания.  
  
 Распознаватель не вызывает <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> событие при использовании этого метода.  
  
 Метод возвращает объект или`null` значение, если операция не выполнена. <xref:System.Speech.Recognition.RecognitionResult> <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize>  
  
 Операция синхронного распознавания может завершиться сбоем по следующим причинам:  
  
-   Распознавание речи не определяется до истечения времени ожидания для <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A> `initialSilenceTimeout` параметра или.  
  
-   Механизм распознавания обнаруживает распознавание речи, но не находит совпадений ни в одном из <xref:System.Speech.Recognition.Grammar> загруженных и включенных объектов.  
  
 Чтобы выполнить асинхронное распознавание, используйте один <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> из методов.  
  
   
  
## Examples  
 В следующем примере показана часть консольного приложения, демонстрирующее базовое распознавание речи. В примере создается <xref:System.Speech.Recognition.DictationGrammar>, загружается в внутрипроцессный распознаватель речи и выполняется одна операция распознавания.  
  
```csharp  
  
using System;  
using System.Speech.Recognition;  
  
namespace SynchronousRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
      // Create an in-process speech recognizer for the en-US locale.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(  
          new System.Globalization.CultureInfo("en-US")))  
      {  
        // Create and load a dictation grammar.  
        recognizer.LoadGrammar(new DictationGrammar());  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start synchronous speech recognition.  
        RecognitionResult result = recognizer.Recognize(TimeSpan.FromSeconds(5));  
  
        if (result != null)  
        {  
          Console.WriteLine("Recognized text = {0}", result.Text);  
        }  
        else  
        {  
          Console.WriteLine("No recognition result available.");  
        }  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to continue...");  
      Console.ReadKey();  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String)" />
      </Docs>
    </Member>
    <MemberGroup MemberName="RecognizeAsync">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>Запускает асинхронную операцию распознавания речи.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Эти методы выполняют одну или несколько асинхронных операций распознавания. Распознаватель выполняет каждую операцию по загруженным и включенным грамматикам распознавания речи.  
  
 При вызове этого метода распознаватель может создавать следующие события:  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>.  Возникает, когда распознаватель обнаруживает ввод, который может быть идентифицирован как речь.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>.  Вызывается, когда входные данные создают неоднозначное соответствие с одной из активных грамматик.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> или <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>. Вызывается, когда распознаватель завершает операцию распознавания.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted>. Возникает при <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> завершении операции.  
  
 Чтобы получить результат асинхронной операции распознавания, присоедините обработчик событий к <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> событию распознавателя. Распознаватель вызывает это событие при успешном завершении синхронной или асинхронной операции распознавания. Если распознавание не было успешным, <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.Result%2A> <xref:System.Speech.Recognition.RecognizeCompletedEventArgs> свойство объекта, к которому можно получить доступ <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> в обработчике события, будет иметь `null`значение.  
  
 Асинхронная операция распознавания может завершиться сбоем по следующим причинам:  
  
-   Распознавание речи не определяется до истечения времени ожидания для <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A> свойств <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> или.  
  
-   Механизм распознавания обнаруживает распознавание речи, но не находит совпадений ни в одном из <xref:System.Speech.Recognition.Grammar> загруженных и включенных объектов.  
  
-   Прежде <xref:System.Speech.Recognition.SpeechRecognitionEngine> чем выполнять распознавание, <xref:System.Speech.Recognition.Grammar> необходимо загрузить по крайней мере один объект. Чтобы загрузить грамматику распознавания речи, используйте <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A> метод или. <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A>  
  
-   Чтобы изменить, как распознаватель обрабатывает время речи или тишины в отношении <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>распознавания, используйте свойства, <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A>и <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> .  
  
-   Чтобы выполнить синхронное распознавание, используйте один <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A> из методов.  
  
 ]]></format>
        </remarks>
      </Docs>
    </MemberGroup>
    <Member MemberName="RecognizeAsync">
      <MemberSignature Language="C#" Value="public void RecognizeAsync ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void RecognizeAsync() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
      <MemberSignature Language="VB.NET" Value="Public Sub RecognizeAsync ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void RecognizeAsync();" />
      <MemberSignature Language="F#" Value="member this.RecognizeAsync : unit -&gt; unit" Usage="speechRecognitionEngine.RecognizeAsync " />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Attributes>
        <Attribute FrameworkAlternate="netframework-4.0">
          <AttributeName>System.Runtime.TargetedPatchingOptOut("Performance critical to inline this type of method across NGen image boundaries")</AttributeName>
        </Attribute>
      </Attributes>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Выполняет одиночную, асинхронную операцию распознавания речи.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Этот метод выполняет одну асинхронную операцию распознавания. Распознаватель выполняет операцию с загруженными и включенными грамматиками распознавания речи.  
  
 При вызове этого метода распознаватель может создавать следующие события:  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>.  Возникает, когда распознаватель обнаруживает ввод, который может быть идентифицирован как речь.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>.  Вызывается, когда входные данные создают неоднозначное соответствие с одной из активных грамматик.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> или <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>. Вызывается, когда распознаватель завершает операцию распознавания.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted>. Возникает при <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> завершении операции.  
  
 Чтобы получить результат асинхронной операции распознавания, присоедините обработчик событий к <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> событию распознавателя. Распознаватель вызывает это событие при успешном завершении синхронной или асинхронной операции распознавания. Если распознавание не было успешным, <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.Result%2A> <xref:System.Speech.Recognition.RecognizeCompletedEventArgs> свойство объекта, к которому можно получить доступ <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> в обработчике события, будет иметь `null`значение.  
  
 Чтобы выполнить синхронное распознавание, используйте один <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A> из методов.  
  
   
  
## Examples  
 В следующем примере показана часть консольного приложения, демонстрирующее базовое средство асинхронного распознавания речи. В примере создается <xref:System.Speech.Recognition.DictationGrammar>, загружается в внутрипроцессный распознаватель речи и выполняется одна асинхронная операция распознавания. Обработчики событий включены для демонстрации событий, которые распознаватель вызывает во время операции.  
  
```csharp  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace AsynchronousRecognition  
{  
  class Program  
  {  
    // Indicate whether asynchronous recognition is complete.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
      // Create an in-process speech recognizer.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
        // Create a grammar for choosing cities for a flight.  
        Choices cities = new Choices(new string[]   
        { "Los Angeles", "New York", "Chicago", "San Francisco", "Miami", "Dallas" });  
  
        GrammarBuilder gb = new GrammarBuilder();  
        gb.Append("I want to fly from");  
        gb.Append(cities);  
        gb.Append("to");  
        gb.Append(cities);  
  
        // Construct a Grammar object and load it to the recognizer.  
        Grammar cityChooser = new Grammar(gb);  
        cityChooser.Name = ("City Chooser");  
        recognizer.LoadGrammarAsync(cityChooser);  
  
        // Attach event handlers.  
        recognizer.SpeechDetected +=  
          new EventHandler<SpeechDetectedEventArgs>(  
            SpeechDetectedHandler);  
        recognizer.SpeechHypothesized +=  
          new EventHandler<SpeechHypothesizedEventArgs>(  
            SpeechHypothesizedHandler);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(  
            SpeechRecognitionRejectedHandler);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(  
            RecognizeCompletedHandler);  
  
        // Assign input to the recognizer and start an asynchronous  
        // recognition operation.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        completed = false;  
        Console.WriteLine("Starting asynchronous recognition...");  
        recognizer.RecognizeAsync();  
  
        // Wait for the operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
        Console.WriteLine("Done.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechDetected event.  
    static void SpeechDetectedHandler(object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechDetectedHandler:");  
      Console.WriteLine(" - AudioPosition = {0}", e.AudioPosition);  
    }  
  
    // Handle the SpeechHypothesized event.  
    static void SpeechHypothesizedHandler(  
      object sender, SpeechHypothesizedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechHypothesizedHandler:");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the SpeechRecognitionRejected event.  
    static void SpeechRecognitionRejectedHandler(  
      object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechRecognitionRejectedHandler:");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechRecognizedHandler.");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void RecognizeCompletedHandler(  
      object sender, RecognizeCompletedEventArgs e)  
    {  
      Console.WriteLine(" In RecognizeCompletedHandler.");  
  
      if (e.Error != null)  
      {  
        Console.WriteLine(  
          " - Error occurred during recognition: {0}", e.Error);  
        return;  
      }  
      if (e.InitialSilenceTimeout || e.BabbleTimeout)  
      {  
        Console.WriteLine(  
          " - BabbleTimeout = {0}; InitialSilenceTimeout = {1}",  
          e.BabbleTimeout, e.InitialSilenceTimeout);  
        return;  
      }  
      if (e.InputStreamEnded)  
      {  
        Console.WriteLine(  
          " - AudioPosition = {0}; InputStreamEnded = {1}",  
          e.AudioPosition, e.InputStreamEnded);  
      }  
      if (e.Result != null)  
      {  
        Console.WriteLine(  
          " - Grammar = {0}; Text = {1}; Confidence = {2}",  
          e.Result.Grammar.Name, e.Result.Text, e.Result.Confidence);  
        Console.WriteLine(" - AudioPosition = {0}", e.AudioPosition);  
      }  
      else  
      {  
        Console.WriteLine(" - No result.");  
      }  
  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
      </Docs>
    </Member>
    <Member MemberName="RecognizeAsync">
      <MemberSignature Language="C#" Value="public void RecognizeAsync (System.Speech.Recognition.RecognizeMode mode);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void RecognizeAsync(valuetype System.Speech.Recognition.RecognizeMode mode) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync(System.Speech.Recognition.RecognizeMode)" />
      <MemberSignature Language="VB.NET" Value="Public Sub RecognizeAsync (mode As RecognizeMode)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void RecognizeAsync(System::Speech::Recognition::RecognizeMode mode);" />
      <MemberSignature Language="F#" Value="member this.RecognizeAsync : System.Speech.Recognition.RecognizeMode -&gt; unit" Usage="speechRecognitionEngine.RecognizeAsync mode" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="mode" Type="System.Speech.Recognition.RecognizeMode" />
      </Parameters>
      <Docs>
        <param name="mode">Указывает, нужно ли выполнить одну или несколько операций распознавания.</param>
        <summary>Выполняет одну или несколько асинхронных операций распознавания речи.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Если `mode` имеет <xref:System.Speech.Recognition.RecognizeMode.Multiple>значение, распознаватель продолжит выполнять асинхронные операции распознавания, <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncCancel%2A> пока <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncStop%2A> не будет вызван метод или.  
  
 При вызове этого метода распознаватель может создавать следующие события:  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>.  Возникает, когда распознаватель обнаруживает ввод, который может быть идентифицирован как речь.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>.  Вызывается, когда входные данные создают неоднозначное соответствие с одной из активных грамматик.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> или <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>. Вызывается, когда распознаватель завершает операцию распознавания.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted>. Возникает при <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> завершении операции.  
  
 Чтобы получить результат асинхронной операции распознавания, присоедините обработчик событий к <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> событию распознавателя. Распознаватель вызывает это событие при успешном завершении синхронной или асинхронной операции распознавания. Если распознавание не было успешным, <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.Result%2A> <xref:System.Speech.Recognition.RecognizeCompletedEventArgs> свойство объекта, к которому можно получить доступ <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> в обработчике события, будет иметь `null`значение.  
  
 Асинхронная операция распознавания может завершиться сбоем по следующим причинам:  
  
-   Распознавание речи не определяется до истечения времени ожидания для <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A> свойств <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> или.  
  
-   Механизм распознавания обнаруживает распознавание речи, но не находит совпадений ни в одном из <xref:System.Speech.Recognition.Grammar> загруженных и включенных объектов.  
  
 Чтобы выполнить синхронное распознавание, используйте один <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A> из методов.  
  
   
  
## Examples  
 В следующем примере показана часть консольного приложения, демонстрирующее базовое средство асинхронного распознавания речи. В примере создается <xref:System.Speech.Recognition.DictationGrammar>, загружается в внутрипроцессный распознаватель речи и выполняется несколько асинхронных операций распознавания. Асинхронные операции отменяются через 30 секунд. Обработчики событий включены для демонстрации событий, которые распознаватель вызывает во время операции.  
  
```csharp  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace AsynchronousRecognition  
{  
  class Program  
  {  
    // Indicate whether asynchronous recognition is complete.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
      // Create an in-process speech recognizer.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
        // Create a grammar for choosing cities for a flight.  
        Choices cities = new Choices(new string[] { "Los Angeles", "New York", "Chicago", "San Francisco", "Miami", "Dallas" });  
  
        GrammarBuilder gb = new GrammarBuilder();  
        gb.Append("I want to fly from");  
        gb.Append(cities);  
        gb.Append("to");  
        gb.Append(cities);  
  
        // Construct a Grammar object and load it to the recognizer.  
        Grammar cityChooser = new Grammar(gb);  
        cityChooser.Name = ("City Chooser");  
        recognizer.LoadGrammarAsync(cityChooser);  
  
        // Attach event handlers.  
        recognizer.SpeechDetected +=  
          new EventHandler<SpeechDetectedEventArgs>(  
            SpeechDetectedHandler);  
        recognizer.SpeechHypothesized +=  
          new EventHandler<SpeechHypothesizedEventArgs>(  
            SpeechHypothesizedHandler);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(  
            SpeechRecognitionRejectedHandler);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(  
            RecognizeCompletedHandler);  
  
        // Assign input to the recognizer and start asynchronous  
        // recognition.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        completed = false;  
        Console.WriteLine("Starting asynchronous recognition...");  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Wait 30 seconds, and then cancel asynchronous recognition.  
        Thread.Sleep(TimeSpan.FromSeconds(30));  
        recognizer.RecognizeAsyncCancel();  
  
        // Wait for the operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
        Console.WriteLine("Done.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechDetected event.  
    static void SpeechDetectedHandler(object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechDetectedHandler:");  
      Console.WriteLine(" - AudioPosition = {0}", e.AudioPosition);  
    }  
  
    // Handle the SpeechHypothesized event.  
    static void SpeechHypothesizedHandler(  
      object sender, SpeechHypothesizedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechHypothesizedHandler:");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the SpeechRecognitionRejected event.  
    static void SpeechRecognitionRejectedHandler(  
      object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechRecognitionRejectedHandler:");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechRecognizedHandler.");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void RecognizeCompletedHandler(  
      object sender, RecognizeCompletedEventArgs e)  
    {  
      Console.WriteLine(" In RecognizeCompletedHandler.");  
  
      if (e.Error != null)  
      {  
        Console.WriteLine(  
          " - Error occurred during recognition: {0}", e.Error);  
        return;  
      }  
      if (e.InitialSilenceTimeout || e.BabbleTimeout)  
      {  
        Console.WriteLine(  
          " - BabbleTimeout = {0}; InitialSilenceTimeout = {1}",  
          e.BabbleTimeout, e.InitialSilenceTimeout);  
        return;  
      }  
      if (e.InputStreamEnded)  
      {  
        Console.WriteLine(  
          " - AudioPosition = {0}; InputStreamEnded = {1}",  
          e.AudioPosition, e.InputStreamEnded);  
      }  
      if (e.Result != null)  
      {  
        Console.WriteLine(  
          " - Grammar = {0}; Text = {1}; Confidence = {2}",  
          e.Result.Grammar.Name, e.Result.Text, e.Result.Confidence);  
        Console.WriteLine(" - AudioPosition = {0}", e.AudioPosition);  
      }  
      else  
      {  
        Console.WriteLine(" - No result.");  
      }  
  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
      </Docs>
    </Member>
    <Member MemberName="RecognizeAsyncCancel">
      <MemberSignature Language="C#" Value="public void RecognizeAsyncCancel ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void RecognizeAsyncCancel() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncCancel" />
      <MemberSignature Language="VB.NET" Value="Public Sub RecognizeAsyncCancel ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void RecognizeAsyncCancel();" />
      <MemberSignature Language="F#" Value="member this.RecognizeAsyncCancel : unit -&gt; unit" Usage="speechRecognitionEngine.RecognizeAsyncCancel " />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Останавливает асинхронное распознавание без ожидания завершения текущей операции распознавания.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Этот метод немедленно завершает асинхронное распознавание. Если текущая операция асинхронного распознавания получает входные данные, то входные данные усекаются и операция завершается с использованием существующих входных данных. Распознаватель <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> вызывает событие или <xref:System.ComponentModel.AsyncCompletedEventArgs.Cancelled%2A> <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> при отмене асинхронной операции и задает для `true`свойства объекта значение <xref:System.Speech.Recognition.RecognizeCompletedEventArgs> . Этот метод отменяет асинхронные операции, <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> инициированные методами и <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> .  
  
 Чтобы отключить асинхронное распознавание без усечения входных данных, <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncStop%2A> используйте метод.  
  
   
  
## Examples  
 В следующем примере показана часть консольного приложения, демонстрирующее использование <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncCancel%2A> метода. В примере создается и загружается Грамматика распознавания речи, инициируется продолжение асинхронной операции распознавания, а затем пауза через 2 секунды до отмены операции. Распознаватель получает входные данные из файла, к:\темп\аудиоинпут\сампле.ВАВ. Обработчики событий включены для демонстрации событий, которые распознаватель вызывает во время операции.  
  
```csharp  
  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace AsynchronousRecognition  
{  
  class Program  
  {  
    // Indicate whether asynchronous recognition is complete.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
      // Create an in-process speech recognizer.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
        // Create and load a dictation grammar.  
        Grammar dictation = new DictationGrammar();  
        dictation.Name = "Dictation Grammar";  
  
        recognizer.LoadGrammar(dictation);  
  
        // Attach event handlers.  
        recognizer.SpeechDetected +=  
          new EventHandler<SpeechDetectedEventArgs>(  
            SpeechDetectedHandler);  
        recognizer.SpeechHypothesized +=  
          new EventHandler<SpeechHypothesizedEventArgs>(  
            SpeechHypothesizedHandler);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(  
            SpeechRecognitionRejectedHandler);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(  
            RecognizeCompletedHandler);  
  
        // Begin asynchronous recognition from pre-recorded input.  
        recognizer.SetInputToWaveFile(@"c:\temp\audioinput\sample.wav");  
  
        completed = false;  
        Console.WriteLine("Begin continuing asynchronous recognition...");  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Wait 2 seconds and then cancel the recognition operation.  
        Thread.Sleep(TimeSpan.FromSeconds(2));  
        recognizer.RecognizeAsyncCancel();  
  
        // Wait for the operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
  
        Console.WriteLine("Done.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechDetected event.  
    static void SpeechDetectedHandler(object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechDetectedHandler:");  
      Console.WriteLine(" - AudioPosition = {0}", e.AudioPosition);  
    }  
  
    // Handle the SpeechHypothesized event.  
    static void SpeechHypothesizedHandler(  
      object sender, SpeechHypothesizedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechHypothesizedHandler:");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the SpeechRecognitionRejected event.  
    static void SpeechRecognitionRejectedHandler(  
      object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechRecognitionRejectedHandler:");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechRecognizedHandler.");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void RecognizeCompletedHandler(  
      object sender, RecognizeCompletedEventArgs e)  
    {  
      Console.WriteLine(" In RecognizeCompletedHandler.");  
  
      if (e.Error != null)  
      {  
        Console.WriteLine(  
          " - Error occurred during recognition: {0}", e.Error);  
        return;  
      }  
      if (e.Cancelled)  
      {  
        Console.WriteLine(" - asynchronous operation canceled.");  
      }  
      if (e.InitialSilenceTimeout || e.BabbleTimeout)  
      {  
        Console.WriteLine(  
          " - BabbleTimeout = {0}; InitialSilenceTimeout = {1}",  
          e.BabbleTimeout, e.InitialSilenceTimeout);  
        return;  
      }  
      if (e.InputStreamEnded)  
      {  
        Console.WriteLine(  
          " - AudioPosition = {0}; InputStreamEnded = {1}",  
          e.AudioPosition, e.InputStreamEnded);  
      }  
      if (e.Result != null)  
      {  
        Console.WriteLine(  
          " - Grammar = {0}; Text = {1}; Confidence = {2}",  
          e.Result.Grammar.Name, e.Result.Text, e.Result.Confidence);  
      }  
      else  
      {  
        Console.WriteLine(" - No result.");  
      }  
  
      completed = true;  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncStop" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="RecognizeAsyncStop">
      <MemberSignature Language="C#" Value="public void RecognizeAsyncStop ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void RecognizeAsyncStop() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncStop" />
      <MemberSignature Language="VB.NET" Value="Public Sub RecognizeAsyncStop ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void RecognizeAsyncStop();" />
      <MemberSignature Language="F#" Value="member this.RecognizeAsyncStop : unit -&gt; unit" Usage="speechRecognitionEngine.RecognizeAsyncStop " />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Останавливает асинхронное распознавание после завершения текущей операции распознавания.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Этот метод завершает асинхронное распознавание без усечения входных данных. Если текущая операция асинхронного распознавания получает входные данные, распознаватель продолжит принимать входные данные до завершения текущей операции распознавания. Распознаватель <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> вызывает событие или <xref:System.ComponentModel.AsyncCompletedEventArgs.Cancelled%2A> <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> , когда завершается асинхронная операция, и задает для `true`свойства объекта значение <xref:System.Speech.Recognition.RecognizeCompletedEventArgs> . Этот метод останавливает асинхронные операции, <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> инициированные <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> методами и.  
  
 Чтобы немедленно отменить асинхронное распознавание только с помощью существующих входных <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncCancel%2A> данных, используйте метод.  
  
   
  
## Examples  
 В следующем примере показана часть консольного приложения, демонстрирующее использование <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncStop%2A> метода. В примере создается и загружается Грамматика распознавания речи, инициируется продолжение асинхронной операции распознавания, а затем пауза через 2 секунды до остановки операции. Распознаватель получает входные данные из файла, к:\темп\аудиоинпут\сампле.ВАВ. Обработчики событий включены для демонстрации событий, которые распознаватель вызывает во время операции.  
  
```csharp  
  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace AsynchronousRecognition  
{  
  class Program  
  {  
    // Indicate whether asynchronous recognition is complete.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
      // Create an in-process speech recognizer.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
        // Create and load a dictation grammar.  
        Grammar dictation = new DictationGrammar();  
        dictation.Name = "Dictation Grammar";  
  
        recognizer.LoadGrammar(dictation);  
  
        // Attach event handlers.  
        recognizer.SpeechDetected +=  
          new EventHandler<SpeechDetectedEventArgs>(  
            SpeechDetectedHandler);  
        recognizer.SpeechHypothesized +=  
          new EventHandler<SpeechHypothesizedEventArgs>(  
            SpeechHypothesizedHandler);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(  
            SpeechRecognitionRejectedHandler);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(  
            RecognizeCompletedHandler);  
  
        // Begin asynchronous recognition from pre-recorded input.  
        recognizer.SetInputToWaveFile(@"c:\temp\audioinput\sample.wav");  
  
        completed = false;  
        Console.WriteLine("Begin continuing asynchronous recognition...");  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Wait 2 seconds and then stop the recognition operation.  
        Thread.Sleep(TimeSpan.FromSeconds(2));  
        recognizer.RecognizeAsyncStop();  
  
        // Wait for the operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
  
        Console.WriteLine("Done.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechDetected event.  
    static void SpeechDetectedHandler(object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechDetectedHandler:");  
      Console.WriteLine(" - AudioPosition = {0}", e.AudioPosition);  
    }  
  
    // Handle the SpeechHypothesized event.  
    static void SpeechHypothesizedHandler(  
      object sender, SpeechHypothesizedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechHypothesizedHandler:");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the SpeechRecognitionRejected event.  
    static void SpeechRecognitionRejectedHandler(  
      object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechRecognitionRejectedHandler:");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechRecognizedHandler.");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void RecognizeCompletedHandler(  
      object sender, RecognizeCompletedEventArgs e)  
    {  
      Console.WriteLine(" In RecognizeCompletedHandler.");  
  
      if (e.Error != null)  
      {  
        Console.WriteLine(  
          " - Error occurred during recognition: {0}", e.Error);  
        return;  
      }  
      if (e.Cancelled)  
      {  
        Console.WriteLine(" - asynchronous operation canceled.");  
      }  
      if (e.InitialSilenceTimeout || e.BabbleTimeout)  
      {  
        Console.WriteLine(  
          " - BabbleTimeout = {0}; InitialSilenceTimeout = {1}",  
          e.BabbleTimeout, e.InitialSilenceTimeout);  
        return;  
      }  
      if (e.InputStreamEnded)  
      {  
        Console.WriteLine(  
          " - AudioPosition = {0}; InputStreamEnded = {1}",  
          e.AudioPosition, e.InputStreamEnded);  
      }  
      if (e.Result != null)  
      {  
        Console.WriteLine(  
          " - Grammar = {0}; Text = {1}; Confidence = {2}",  
          e.Result.Grammar.Name, e.Result.Text, e.Result.Confidence);  
      }  
      else  
      {  
        Console.WriteLine(" - No result.");  
      }  
  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncCancel" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="RecognizeCompleted">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.RecognizeCompletedEventArgs&gt; RecognizeCompleted;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.RecognizeCompletedEventArgs&gt; RecognizeCompleted" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
      <MemberSignature Language="VB.NET" Value="Public Event RecognizeCompleted As EventHandler(Of RecognizeCompletedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::RecognizeCompletedEventArgs ^&gt; ^ RecognizeCompleted;" />
      <MemberSignature Language="F#" Value="member this.RecognizeCompleted : EventHandler&lt;System.Speech.Recognition.RecognizeCompletedEventArgs&gt; " Usage="member this.RecognizeCompleted : System.EventHandler&lt;System.Speech.Recognition.RecognizeCompletedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.RecognizeCompletedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Возникает, если <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> оформляет асинхронную операцию распознавания.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Метод<xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A>объектаинициирует асинхронную операцию распознавания. <xref:System.Speech.Recognition.SpeechRecognitionEngine> Когда распознаватель завершает асинхронную операцию, он вызывает это событие.  
  
 С помощью обработчика для <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> события можно <xref:System.Speech.Recognition.RecognitionResult> получить доступ к <xref:System.Speech.Recognition.RecognizeCompletedEventArgs> объекту в объекте. Если распознавание не прошло успешно <xref:System.Speech.Recognition.RecognitionResult> , то `null`будет иметь значение. Чтобы определить, привело ли к сбою распознавания время ожидания или прерывание ввода, можно получить доступ к свойствам для <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.InitialSilenceTimeout%2A>, <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.BabbleTimeout%2A>или <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.InputStreamEnded%2A>.  
  
 Дополнительные сведения см. в описании класса <xref:System.Speech.Recognition.RecognizeCompletedEventArgs>.  
  
 Чтобы получить сведения о наиболее отклоненных кандидатах на распознавание, присоедините <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> обработчик для события.  
  
 При создании делегата <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> необходимо указать метод, обрабатывающий событие. Чтобы связать событие с обработчиком событий, нужно добавить в событие экземпляр делегата. Обработчик событий вызывается всякий раз, когда происходит событие, если делегат не удален. Дополнительные сведения о делегатах обработчиков событий см. в разделе [события и делегаты](https://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 В следующем примере распознаются такие фразы, как "отобразить список исполнителей в категории джаз" или "отобразить альбомы госпел". В примере обработчик для <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> события используется для вывода сведений о результатах распознавания в консоли.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
  
    // Initialize an in-process speech recognition engine.  
    {  
      using (SpeechRecognitionEngine recognizer =  
         new SpeechRecognitionEngine())  
      {  
  
        //  Create lists of alternative choices.  
        Choices listTypes = new Choices(new string[] { "albums", "artists" });  
        Choices genres = new Choices(new string[] {   
          "blues", "classical", "gospel", "jazz", "rock" });  
  
        //  Create a GrammarBuilder object and assemble the grammar components.  
        GrammarBuilder mediaMenu = new GrammarBuilder("Display");  
        mediaMenu.Append("the list of", 0, 1);  
        mediaMenu.Append(listTypes);  
        mediaMenu.Append("in the", 0, 1);  
        mediaMenu.Append(genres);  
        mediaMenu.Append("category.", 0, 1);  
  
        //  Build a Grammar object from the GrammarBuilder.  
        Grammar mediaMenuGrammar = new Grammar(mediaMenu);  
        mediaMenuGrammar.Name = "Media Chooser";  
  
        // Attach event handlers.  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(recognizer_RecognizeCompleted);  
        recognizer.LoadGrammarCompleted +=   
          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
  
        // Load the grammar object to the recognizer.  
        recognizer.LoadGrammarAsync(mediaMenuGrammar);  
  
        // Set the input to the recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start asynchronous, continuous recognition.  
        recognizer.RecognizeAsync();  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void recognizer_RecognizeCompleted(object sender, RecognizeCompletedEventArgs e)  
    {  
      if (e.Error != null)  
      {  
        Console.WriteLine(  
          "RecognizeCompleted, error occurred during recognition: {0}", e.Error);  
        return;  
      }  
  
      if (e.InitialSilenceTimeout || e.BabbleTimeout)  
      {  
        Console.WriteLine(  
          "RecognizeCompleted: BabbleTimeout({0}), InitialSilenceTimeout({1}).",  
          e.BabbleTimeout, e.InitialSilenceTimeout);  
        return;  
      }  
  
      if (e.InputStreamEnded)  
      {  
        Console.WriteLine(  
          "RecognizeCompleted: AudioPosition({0}), InputStreamEnded({1}).",  
          e.AudioPosition, e.InputStreamEnded);  
      }  
  
      if (e.Result != null)  
      {  
        Console.WriteLine("RecognizeCompleted:");  
        Console.WriteLine("  Grammar: " + e.Result.Grammar.Name);  
        Console.WriteLine("  Recognized text: " + e.Result.Text);  
        Console.WriteLine("  Confidence score: " + e.Result.Confidence);  
        Console.WriteLine("  Audio position: " + e.AudioPosition);  
      }  
  
      else  
      {  
        Console.WriteLine("RecognizeCompleted: No result.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the LoadGrammarCompleted event.  
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      Console.WriteLine("Grammar loaded:  " + e.Grammar.Name);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.RecognizeCompletedEventArgs" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
      </Docs>
    </Member>
    <Member MemberName="RecognizerAudioPosition">
      <MemberSignature Language="C#" Value="public TimeSpan RecognizerAudioPosition { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.TimeSpan RecognizerAudioPosition" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property RecognizerAudioPosition As TimeSpan" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property TimeSpan RecognizerAudioPosition { TimeSpan get(); };" />
      <MemberSignature Language="F#" Value="member this.RecognizerAudioPosition : TimeSpan" Usage="System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.TimeSpan</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Получает текущую позицию <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> в обрабатываемых им входных аудиоданных.</summary>
        <value>Позиция распознавателя в обрабатываемых входных звуковых данных.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Расположение звука зависит от распознавателя речи. При включении входного потока устанавливается нулевое значение.  
  
 Свойство ссылается на расположение объекта в звуковом вводе. <xref:System.Speech.Recognition.SpeechRecognitionEngine> <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A> Напротив, <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A> свойство ссылается на расположение устройства ввода в созданном звуковом потоке. Эти позиции могут отличаться. Например, если распознаватель получил входные данные, для которых он еще не создал результат распознавания, значение <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A> свойства меньше значения <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A> свойства.  
  
 ]]></format>
        </remarks>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition" />
      </Docs>
    </Member>
    <Member MemberName="RecognizerInfo">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.RecognizerInfo RecognizerInfo { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance class System.Speech.Recognition.RecognizerInfo RecognizerInfo" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerInfo" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property RecognizerInfo As RecognizerInfo" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property System::Speech::Recognition::RecognizerInfo ^ RecognizerInfo { System::Speech::Recognition::RecognizerInfo ^ get(); };" />
      <MemberSignature Language="F#" Value="member this.RecognizerInfo : System.Speech.Recognition.RecognizerInfo" Usage="System.Speech.Recognition.SpeechRecognitionEngine.RecognizerInfo" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.RecognizerInfo</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Получает сведения о текущем экземпляре <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />.</summary>
        <value>Сведения о текущем распознавателе речи.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Чтобы получить сведения обо всех установленных распознавателях речи для текущей системы, используйте <xref:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers%2A> метод.  
  
   
  
## Examples  
 В следующем примере возвращается частичный список данных для текущего внутрипроцессного модуля распознавания речи. Для получения дополнительной информации см. <xref:System.Speech.Recognition.RecognizerInfo>.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace RecognitionEngine  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
      using (SpeechRecognitionEngine recognizer = new SpeechRecognitionEngine())  
      {  
        Console.WriteLine("Information for the current speech recognition engine:");  
        Console.WriteLine("  Name: {0}", recognizer.RecognizerInfo.Name);  
        Console.WriteLine("  Culture: {0}", recognizer.RecognizerInfo.Culture.ToString());  
        Console.WriteLine("  Description: {0}", recognizer.RecognizerInfo.Description);  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.#ctor(System.Speech.Recognition.RecognizerInfo)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers" />
      </Docs>
    </Member>
    <Member MemberName="RecognizerUpdateReached">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.RecognizerUpdateReachedEventArgs&gt; RecognizerUpdateReached;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.RecognizerUpdateReachedEventArgs&gt; RecognizerUpdateReached" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached" />
      <MemberSignature Language="VB.NET" Value="Public Event RecognizerUpdateReached As EventHandler(Of RecognizerUpdateReachedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::RecognizerUpdateReachedEventArgs ^&gt; ^ RecognizerUpdateReached;" />
      <MemberSignature Language="F#" Value="member this.RecognizerUpdateReached : EventHandler&lt;System.Speech.Recognition.RecognizerUpdateReachedEventArgs&gt; " Usage="member this.RecognizerUpdateReached : System.EventHandler&lt;System.Speech.Recognition.RecognizerUpdateReachedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.RecognizerUpdateReachedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Вызывается, когда исполняемый механизм <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> приостанавливается, чтобы принять изменения.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Приложения должны использовать <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> для приостановки работы запущенного <xref:System.Speech.Recognition.SpeechRecognitionEngine> экземпляра перед <xref:System.Speech.Recognition.Grammar> изменением его параметров или объектов. Вызывает <xref:System.Speech.Recognition.SpeechRecognitionEngine> это событие, когда оно готово к принятию изменений.  
  
 Например <xref:System.Speech.Recognition.SpeechRecognitionEngine> , когда приостанавливается, можно загружать, выгружать, включать и отключать <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A> <xref:System.Speech.Recognition.Grammar> объекты, а также изменять значения свойств, <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>и <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A> . Дополнительные сведения см. в описании метода <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A>.  
  
 При создании делегата <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> необходимо указать метод, обрабатывающий событие. Чтобы связать событие с обработчиком событий, нужно добавить в событие экземпляр делегата. Обработчик событий вызывается всякий раз, когда происходит событие, если делегат не удален. Дополнительные сведения о делегатах обработчиков событий см. в разделе [события и делегаты](https://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 В следующем примере показано консольное приложение, которое загружает и <xref:System.Speech.Recognition.Grammar> выгружает объекты. Приложение использует <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> метод, чтобы запросить приостановку обработчика распознавания речи, чтобы он мог получить обновление. Затем приложение загружает или <xref:System.Speech.Recognition.Grammar> выгружает объект.  
  
 При каждом обновлении обработчик <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> события записывает имя и состояние загруженных <xref:System.Speech.Recognition.Grammar> в данный момент объектов в консоль. По мере загрузки и выгрузки грамматики приложение сначала распознает имена животных фермы, а затем имена животных и имена фруктам, а затем только имена фруктам.  
  
```  
using System;  
using System.Speech.Recognition;  
using System.Collections.Generic;  
using System.Threading;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    private static SpeechRecognitionEngine recognizer;  
    public static void Main(string[] args)  
    {  
  
      // Initialize an in-process speech recognition engine and configure its input.  
      using (recognizer = new SpeechRecognitionEngine(new System.Globalization.CultureInfo("en-US")))  
      {  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Create the first grammar - Farm.  
        Choices animals = new Choices(new string[] { "cow", "pig", "goat" });  
        GrammarBuilder farm = new GrammarBuilder(animals);  
        Grammar farmAnimals = new Grammar(farm);  
        farmAnimals.Name = "Farm";  
  
        // Create the second grammar - Fruit.  
        Choices fruit = new Choices(new string[] { "apples", "peaches", "oranges" });  
        GrammarBuilder favorite = new GrammarBuilder(fruit);  
        Grammar favoriteFruit = new Grammar(favorite);  
        favoriteFruit.Name = "Fruit";  
  
        // Attach event handlers.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
        recognizer.RecognizerUpdateReached +=  
          new EventHandler<RecognizerUpdateReachedEventArgs>(recognizer_RecognizerUpdateReached);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(recognizer_SpeechRecognitionRejected);  
  
        // Load the Farm grammar.  
        recognizer.LoadGrammar(farmAnimals);  
  
        // Start asynchronous, continuous recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
        Console.WriteLine("Starting asynchronous, continuous recognition");  
        Console.WriteLine("  Farm grammar is loaded and enabled.");  
  
        // Pause to recognize farm animals.  
        Thread.Sleep(7000);  
        Console.WriteLine();  
  
        // Request an update and load the Fruit grammar.  
        recognizer.RequestRecognizerUpdate();  
        recognizer.LoadGrammarAsync(favoriteFruit);  
        Thread.Sleep(7000);  
  
        // Request an update and unload the Farm grammar.  
        recognizer.RequestRecognizerUpdate();  
        recognizer.UnloadGrammar(farmAnimals);  
        Thread.Sleep(7000);  
      }  
  
      // Keep the console window open.  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // At the update, get the names and enabled status of the currently loaded grammars.  
    public static void recognizer_RecognizerUpdateReached(  
      object sender, RecognizerUpdateReachedEventArgs e)  
    {  
      Console.WriteLine();  
      Console.WriteLine("Update reached:");  
      Thread.Sleep(1000);  
  
      string qualifier;  
      List<Grammar> grammars = new List<Grammar>(recognizer.Grammars);  
      foreach (Grammar g in grammars)  
      {  
        qualifier = (g.Enabled) ? "enabled" : "disabled";  
        Console.WriteLine("  {0} grammar is loaded and {1}.",  
        g.Name, qualifier);  
      }  
    }  
  
    // Write the text of the recognized phrase to the console.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("    Speech recognized: " + e.Result.Text);  
    }  
  
    // Write a message to the console when recognition fails.  
    static void recognizer_SpeechRecognitionRejected(object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine("    Recognition attempt failed");  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.RecognizerUpdateReachedEventArgs" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate" />
        <altmember cref="T:System.Speech.Recognition.SpeechDetectedEventArgs" />
      </Docs>
    </Member>
    <MemberGroup MemberName="RequestRecognizerUpdate">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>Запрашивает, чтобы распознаватель приостановил обновления состояния.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Используйте этот метод для синхронизации изменений распознавателя. Например, если вы загружаете или выгружаете грамматику распознавания речи во время обработки входных данных распознавателем, используйте <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> этот метод и событие для синхронизации поведения приложения с состоянием распознавателя.  
  
 При вызове этого метода распознаватель приостанавливает или завершает асинхронные операции и создает <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> событие. Затем <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> обработчик событий может изменить состояние распознавателя между операциями распознавания. При обработке <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> событий распознаватель приостанавливается до тех пор, пока обработчик событий не вернет значение.  
  
> [!NOTE]
>  Если входные данные распознавателя изменяются до того, как распознаватель <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> вызовет событие, запрос отклоняется.  
  
 При вызове этого метода:  
  
-   Если распознаватель не обрабатывает входные данные, распознаватель немедленно создает <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> событие.  
  
-   Если распознаватель обрабатывает входные данные, состоящие из тишины или фонового шума, распознаватель приостанавливает операцию распознавания и создает <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> событие.  
  
-   Если распознаватель обрабатывает входные данные, не состоящие из тишины или фонового шума, распознаватель завершает операцию распознавания, а затем создает <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> событие.  
  
 В то время как распознаватель <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> обрабатывает событие:  
  
-   Распознаватель не обрабатывает входные данные, и значение <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A> свойства остается неизменным.  
  
-   Распознаватель продолжит получать входные данные, и значение <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A> свойства может измениться.  
  
 ]]></format>
        </remarks>
      </Docs>
    </MemberGroup>
    <Member MemberName="RequestRecognizerUpdate">
      <MemberSignature Language="C#" Value="public void RequestRecognizerUpdate ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void RequestRecognizerUpdate() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate" />
      <MemberSignature Language="VB.NET" Value="Public Sub RequestRecognizerUpdate ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void RequestRecognizerUpdate();" />
      <MemberSignature Language="F#" Value="member this.RequestRecognizerUpdate : unit -&gt; unit" Usage="speechRecognitionEngine.RequestRecognizerUpdate " />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Запрашивает, чтобы распознаватель приостановил обновления состояния.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Когда распознаватель создает <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> событие <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs.UserToken%2A> , свойство объекта <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs> имеет `null`значение.  
  
 Чтобы предоставить маркер пользователя, используйте <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> метод или. <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> Чтобы задать смещение позиции звука, используйте <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> метод.  
  
   
  
## Examples  
 В следующем примере показано консольное приложение, которое загружает и <xref:System.Speech.Recognition.Grammar> выгружает объекты. Приложение использует <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> метод, чтобы запросить приостановку обработчика распознавания речи, чтобы он мог получить обновление. Затем приложение загружает или <xref:System.Speech.Recognition.Grammar> выгружает объект.  
  
 При каждом обновлении обработчик <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> события записывает имя и состояние загруженных <xref:System.Speech.Recognition.Grammar> в данный момент объектов в консоль. По мере загрузки и выгрузки грамматики приложение сначала распознает имена животных фермы, а затем имена животных и имена фруктам, а затем только имена фруктам.  
  
```  
using System;  
using System.Speech.Recognition;  
using System.Collections.Generic;  
using System.Threading;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    private static SpeechRecognitionEngine recognizer;  
    public static void Main(string[] args)  
    {  
  
      // Initialize an in-process speech recognition engine and configure its input.  
      using (recognizer = new SpeechRecognitionEngine(new System.Globalization.CultureInfo("en-US")))  
      {  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Create the first grammar - Farm.  
        Choices animals = new Choices(new string[] { "cow", "pig", "goat" });  
        GrammarBuilder farm = new GrammarBuilder(animals);  
        Grammar farmAnimals = new Grammar(farm);  
        farmAnimals.Name = "Farm";  
  
        // Create the second grammar - Fruit.  
        Choices fruit = new Choices(new string[] { "apples", "peaches", "oranges" });  
        GrammarBuilder favorite = new GrammarBuilder(fruit);  
        Grammar favoriteFruit = new Grammar(favorite);  
        favoriteFruit.Name = "Fruit";  
  
        // Attach event handlers.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
        recognizer.RecognizerUpdateReached +=  
          new EventHandler<RecognizerUpdateReachedEventArgs>(recognizer_RecognizerUpdateReached);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(recognizer_SpeechRecognitionRejected);  
  
        // Load the Farm grammar.  
        recognizer.LoadGrammar(farmAnimals);  
  
        // Start asynchronous, continuous recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
        Console.WriteLine("Starting asynchronous, continuous recognition");  
        Console.WriteLine("  Farm grammar is loaded and enabled.");  
  
        // Pause to recognize farm animals.  
        Thread.Sleep(7000);  
        Console.WriteLine();  
  
        // Request an update and load the Fruit grammar.  
        recognizer.RequestRecognizerUpdate();  
        recognizer.LoadGrammarAsync(favoriteFruit);  
        Thread.Sleep(7000);  
  
        // Request an update and unload the Farm grammar.  
        recognizer.RequestRecognizerUpdate();  
        recognizer.UnloadGrammar(farmAnimals);  
        Thread.Sleep(7000);  
      }  
  
      // Keep the console window open.  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // At the update, get the names and enabled status of the currently loaded grammars.  
    public static void recognizer_RecognizerUpdateReached(  
      object sender, RecognizerUpdateReachedEventArgs e)  
    {  
      Console.WriteLine();  
      Console.WriteLine("Update reached:");  
      Thread.Sleep(1000);  
  
      string qualifier;  
      List<Grammar> grammars = new List<Grammar>(recognizer.Grammars);  
      foreach (Grammar g in grammars)  
      {  
        qualifier = (g.Enabled) ? "enabled" : "disabled";  
        Console.WriteLine("  {0} grammar is loaded and {1}.",  
        g.Name, qualifier);  
      }  
    }  
  
    // Write the text of the recognized phrase to the console.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("    Speech recognized: " + e.Result.Text);  
    }  
  
    // Write a message to the console when recognition fails.  
    static void recognizer_SpeechRecognitionRejected(object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine("    Recognition attempt failed");  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition" />
      </Docs>
    </Member>
    <Member MemberName="RequestRecognizerUpdate">
      <MemberSignature Language="C#" Value="public void RequestRecognizerUpdate (object userToken);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void RequestRecognizerUpdate(object userToken) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate(System.Object)" />
      <MemberSignature Language="VB.NET" Value="Public Sub RequestRecognizerUpdate (userToken As Object)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void RequestRecognizerUpdate(System::Object ^ userToken);" />
      <MemberSignature Language="F#" Value="member this.RequestRecognizerUpdate : obj -&gt; unit" Usage="speechRecognitionEngine.RequestRecognizerUpdate userToken" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="userToken" Type="System.Object" />
      </Parameters>
      <Docs>
        <param name="userToken">Пользовательский объект, содержащий сведения для данной операции.</param>
        <summary>Запрашивает, чтобы распознаватель приостановил обновление состояния и предоставил токен пользователя для связанного события.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Когда распознаватель создает <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> событие <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs.UserToken%2A> , свойство объекта <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs> содержит значение `userToken` параметра.  
  
 Чтобы задать смещение позиции звука, используйте <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> метод.  
  
 ]]></format>
        </remarks>
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition" />
      </Docs>
    </Member>
    <Member MemberName="RequestRecognizerUpdate">
      <MemberSignature Language="C#" Value="public void RequestRecognizerUpdate (object userToken, TimeSpan audioPositionAheadToRaiseUpdate);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void RequestRecognizerUpdate(object userToken, valuetype System.TimeSpan audioPositionAheadToRaiseUpdate) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate(System.Object,System.TimeSpan)" />
      <MemberSignature Language="VB.NET" Value="Public Sub RequestRecognizerUpdate (userToken As Object, audioPositionAheadToRaiseUpdate As TimeSpan)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void RequestRecognizerUpdate(System::Object ^ userToken, TimeSpan audioPositionAheadToRaiseUpdate);" />
      <MemberSignature Language="F#" Value="member this.RequestRecognizerUpdate : obj * TimeSpan -&gt; unit" Usage="speechRecognitionEngine.RequestRecognizerUpdate (userToken, audioPositionAheadToRaiseUpdate)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="userToken" Type="System.Object" />
        <Parameter Name="audioPositionAheadToRaiseUpdate" Type="System.TimeSpan" />
      </Parameters>
      <Docs>
        <param name="userToken">Пользовательский объект, содержащий сведения для данной операции.</param>
        <param name="audioPositionAheadToRaiseUpdate">Смещение от текущего <see cref="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition" />, чтобы отложить запрос.</param>
        <summary>Запрашивает, чтобы распознаватель приостановил обновление состояния и предоставил смещение и токен пользователя для связанного события.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Распознаватель не инициирует запрос на обновление распознавателя, пока распознаватель не станет <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A> равным текущему <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A> плюс. `audioPositionAheadToRaiseUpdate`  
  
 Когда распознаватель создает <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> событие <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs.UserToken%2A> , свойство объекта <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs> содержит значение `userToken` параметра.  
  
 ]]></format>
        </remarks>
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition" />
      </Docs>
    </Member>
    <Member MemberName="SetInputToAudioStream">
      <MemberSignature Language="C#" Value="public void SetInputToAudioStream (System.IO.Stream audioSource, System.Speech.AudioFormat.SpeechAudioFormatInfo audioFormat);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void SetInputToAudioStream(class System.IO.Stream audioSource, class System.Speech.AudioFormat.SpeechAudioFormatInfo audioFormat) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream(System.IO.Stream,System.Speech.AudioFormat.SpeechAudioFormatInfo)" />
      <MemberSignature Language="VB.NET" Value="Public Sub SetInputToAudioStream (audioSource As Stream, audioFormat As SpeechAudioFormatInfo)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void SetInputToAudioStream(System::IO::Stream ^ audioSource, System::Speech::AudioFormat::SpeechAudioFormatInfo ^ audioFormat);" />
      <MemberSignature Language="F#" Value="member this.SetInputToAudioStream : System.IO.Stream * System.Speech.AudioFormat.SpeechAudioFormatInfo -&gt; unit" Usage="speechRecognitionEngine.SetInputToAudioStream (audioSource, audioFormat)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="audioSource" Type="System.IO.Stream" />
        <Parameter Name="audioFormat" Type="System.Speech.AudioFormat.SpeechAudioFormatInfo" />
      </Parameters>
      <Docs>
        <param name="audioSource">Входной поток звука.</param>
        <param name="audioFormat">Входной звуковой формат.</param>
        <summary>Настраивает объект <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> для получения входных данных из аудиопотока.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Если распознаватель достигает конца входного потока во время операции распознавания, Операция распознавания завершается с помощью доступных входных данных. Любые последующие операции распознавания могут создать исключение, если только входные данные не будут обновлены распознавателем.  
  
   
  
## Examples  
 В следующем примере показана часть консольного приложения, демонстрирующее базовое распознавание речи. В примере используются входные данные из звукового файла example. wav, которые содержат фразы "Test testing 1 2 3" и "Мистер Купер", разделенные признаком паузы. В примере создаются следующие выходные данные.  
  
```  
  
Starting asynchronous recognition...  
  Recognized text =  Testing testing 123  
  Recognized text =  Mr. Cooper  
  End of stream encountered.  
Done.  
  
Press any key to exit...  
```  
  
```csharp  
  
using System;  
using System.Globalization;  
using System.IO;  
using System.Speech.AudioFormat;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace InputExamples  
{  
  class Program  
  {  
    // Indicate whether asynchronous recognition is complete.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
  
        // Create and load a grammar.  
        Grammar dictation = new DictationGrammar();  
        dictation.Name = "Dictation Grammar";  
  
        recognizer.LoadGrammar(dictation);  
  
        // Configure the input to the recognizer.  
        recognizer.SetInputToAudioStream(  
          File.OpenRead(@"c:\temp\audioinput\example.wav"),  
          new SpeechAudioFormatInfo(  
            44100, AudioBitsPerSample.Sixteen, AudioChannel.Mono));  
  
        // Attach event handlers.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(  
            RecognizeCompletedHandler);  
  
        // Perform recognition of the whole file.  
        Console.WriteLine("Starting asynchronous recognition...");  
        completed = false;  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
        Console.WriteLine("Done.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechRecognized event.  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      if (e.Result != null && e.Result.Text != null)  
      {  
        Console.WriteLine("  Recognized text =  {0}", e.Result.Text);  
      }  
      else  
      {  
        Console.WriteLine("  Recognized text not available.");  
      }  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void RecognizeCompletedHandler(  
      object sender, RecognizeCompletedEventArgs e)  
    {  
      if (e.Error != null)  
      {  
        Console.WriteLine("  Error encountered, {0}: {1}",  
          e.Error.GetType().Name, e.Error.Message);  
      }  
      if (e.Cancelled)  
      {  
        Console.WriteLine("  Operation cancelled.");  
      }  
      if (e.InputStreamEnded)  
      {  
        Console.WriteLine("  End of stream encountered.");  
      }  
  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream(System.IO.Stream)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="SetInputToDefaultAudioDevice">
      <MemberSignature Language="C#" Value="public void SetInputToDefaultAudioDevice ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void SetInputToDefaultAudioDevice() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice" />
      <MemberSignature Language="VB.NET" Value="Public Sub SetInputToDefaultAudioDevice ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void SetInputToDefaultAudioDevice();" />
      <MemberSignature Language="F#" Value="member this.SetInputToDefaultAudioDevice : unit -&gt; unit" Usage="speechRecognitionEngine.SetInputToDefaultAudioDevice " />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Настраивает объект <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> для получения входных данных от аудиоустройства по умолчанию.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Examples  
 В следующем примере показана часть консольного приложения, демонстрирующее базовое распознавание речи. В примере используется выход из аудиоустройства по умолчанию, выполняется несколько асинхронных операций распознавания и завершается, когда пользователь уттерс фразу "Exit".  
  
```csharp  
  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace DefaultInput  
{  
  class Program  
  {  
    // Indicate whether asynchronous recognition has finished.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
  
        // Create and load the exit grammar.  
        Grammar exitGrammar = new Grammar(new GrammarBuilder("exit"));  
        exitGrammar.Name = "Exit Grammar";  
        recognizer.LoadGrammar(exitGrammar);  
  
        // Create and load the dictation grammar.  
        Grammar dictation = new DictationGrammar();  
        dictation.Name = "Dictation Grammar";  
        recognizer.LoadGrammar(dictation);  
  
        // Attach event handlers to the recognizer.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(  
            RecognizeCompletedHandler);  
  
        // Assign input to the recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Begin asynchronous recognition.  
        Console.WriteLine("Starting recognition...");  
        completed = false;  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Wait for recognition to finish.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
        Console.WriteLine("Done.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("  Speech recognized:");  
      string grammarName = "<not available>";  
      if (e.Result.Grammar.Name != null &&  
        !e.Result.Grammar.Name.Equals(string.Empty))  
      {  
        grammarName = e.Result.Grammar.Name;  
      }  
      Console.WriteLine("    {0,-17} - {1}",  
        grammarName, e.Result.Text);  
  
      if (grammarName.Equals("Exit Grammar"))  
      {  
        ((SpeechRecognitionEngine)sender).RecognizeAsyncCancel();  
      }  
    }  
  
    static void RecognizeCompletedHandler(  
      object sender, RecognizeCompletedEventArgs e)  
    {  
      Console.WriteLine("  Recognition completed.");  
      completed = true;  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream(System.IO.Stream,System.Speech.AudioFormat.SpeechAudioFormatInfo)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream(System.IO.Stream)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="SetInputToNull">
      <MemberSignature Language="C#" Value="public void SetInputToNull ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void SetInputToNull() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull" />
      <MemberSignature Language="VB.NET" Value="Public Sub SetInputToNull ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void SetInputToNull();" />
      <MemberSignature Language="F#" Value="member this.SetInputToNull : unit -&gt; unit" Usage="speechRecognitionEngine.SetInputToNull " />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Отключает ввод в распознаватель речи.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Настройте объект для отсутствия входных данных при <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize%2A> использовании <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> методов и или при временном переводе механизма распознавания. <xref:System.Speech.Recognition.SpeechRecognitionEngine>  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream(System.IO.Stream,System.Speech.AudioFormat.SpeechAudioFormatInfo)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream(System.IO.Stream)" />
      </Docs>
    </Member>
    <Member MemberName="SetInputToWaveFile">
      <MemberSignature Language="C#" Value="public void SetInputToWaveFile (string path);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void SetInputToWaveFile(string path) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile(System.String)" />
      <MemberSignature Language="VB.NET" Value="Public Sub SetInputToWaveFile (path As String)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void SetInputToWaveFile(System::String ^ path);" />
      <MemberSignature Language="F#" Value="member this.SetInputToWaveFile : string -&gt; unit" Usage="speechRecognitionEngine.SetInputToWaveFile path" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="path" Type="System.String" />
      </Parameters>
      <Docs>
        <param name="path">Путь к файлу, который требуется использовать как входные данные.</param>
        <summary>Настраивает объект <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> для получения входных данных из файла аудиоформата WAV.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Если распознаватель достигает конца входного файла во время операции распознавания, Операция распознавания завершается с помощью доступных входных данных. Любые последующие операции распознавания могут создать исключение, если только входные данные не будут обновлены распознавателем.  
  
   
  
## Examples  
 В следующем примере выполняется распознавание звука в WAV-файле и записывается распознанный текст на консоль.  
  
```  
using System;  
using System.IO;  
using System.Speech.Recognition;  
using System.Speech.AudioFormat;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static bool completed;  
  
    static void Main(string[] args)  
  
    // Initialize an in-process speech recognition engine.  
    {  
      using (SpeechRecognitionEngine recognizer =  
         new SpeechRecognitionEngine())  
      {  
  
        // Create and load a grammar.  
        Grammar dictation = new DictationGrammar();  
        dictation.Name = "Dictation Grammar";  
  
        recognizer.LoadGrammar(dictation);  
  
        // Configure the input to the recognizer.  
recognizer.SetInputToWaveFile(@"c:\temp\SampleWAVInput.wav");  
  
        // Attach event handlers for the results of recognition.  
        recognizer.SpeechRecognized +=   
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
        recognizer.RecognizeCompleted +=   
          new EventHandler<RecognizeCompletedEventArgs>(recognizer_RecognizeCompleted);  
  
        // Perform recognition on the entire file.  
        Console.WriteLine("Starting asynchronous recognition...");  
        completed = false;  
        recognizer.RecognizeAsync();  
  
        // Keep the console window open.  
        while (!completed)  
        {  
          Console.ReadLine();  
        }  
        Console.WriteLine("Done.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      if (e.Result != null && e.Result.Text != null)  
      {  
        Console.WriteLine("  Recognized text =  {0}", e.Result.Text);  
      }  
      else  
      {  
        Console.WriteLine("  Recognized text not available.");  
      }  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void recognizer_RecognizeCompleted(object sender, RecognizeCompletedEventArgs e)  
    {  
      if (e.Error != null)  
      {  
        Console.WriteLine("  Error encountered, {0}: {1}",  
        e.Error.GetType().Name, e.Error.Message);  
      }  
      if (e.Cancelled)  
      {  
        Console.WriteLine("  Operation cancelled.");  
      }  
      if (e.InputStreamEnded)  
      {  
        Console.WriteLine("  End of stream encountered.");  
      }  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream(System.IO.Stream,System.Speech.AudioFormat.SpeechAudioFormatInfo)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream(System.IO.Stream)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="SetInputToWaveStream">
      <MemberSignature Language="C#" Value="public void SetInputToWaveStream (System.IO.Stream audioSource);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void SetInputToWaveStream(class System.IO.Stream audioSource) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream(System.IO.Stream)" />
      <MemberSignature Language="VB.NET" Value="Public Sub SetInputToWaveStream (audioSource As Stream)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void SetInputToWaveStream(System::IO::Stream ^ audioSource);" />
      <MemberSignature Language="F#" Value="member this.SetInputToWaveStream : System.IO.Stream -&gt; unit" Usage="speechRecognitionEngine.SetInputToWaveStream audioSource" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="audioSource" Type="System.IO.Stream" />
      </Parameters>
      <Docs>
        <param name="audioSource">Поток, содержащий звуковые данные.</param>
        <summary>Настраивает объект <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> для получения входных данных от потока, содержащего аудиоформата WAV.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Если распознаватель достигает конца входного потока во время операции распознавания, Операция распознавания завершается с помощью доступных входных данных. Любые последующие операции распознавания могут создать исключение, если только входные данные не будут обновлены распознавателем.  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream(System.IO.Stream,System.Speech.AudioFormat.SpeechAudioFormatInfo)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile(System.String)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="SpeechDetected">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.SpeechDetectedEventArgs&gt; SpeechDetected;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.SpeechDetectedEventArgs&gt; SpeechDetected" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
      <MemberSignature Language="VB.NET" Value="Public Event SpeechDetected As EventHandler(Of SpeechDetectedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::SpeechDetectedEventArgs ^&gt; ^ SpeechDetected;" />
      <MemberSignature Language="F#" Value="member this.SpeechDetected : EventHandler&lt;System.Speech.Recognition.SpeechDetectedEventArgs&gt; " Usage="member this.SpeechDetected : System.EventHandler&lt;System.Speech.Recognition.SpeechDetectedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.SpeechDetectedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Возникает, когда <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> обнаруживает введенные данные, которые могут быть идентифицированы как речь.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Каждый распознаватель речи имеет алгоритм для различения тишины и речи. Когда выполняет операцию распознавания речи, она <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected> вызывает событие, когда его алгоритм определяет входные данные как речь. <xref:System.Speech.Recognition.SpeechRecognitionEngine> <xref:System.Speech.Recognition.SpeechDetectedEventArgs.AudioPosition%2A> Свойство связанного<xref:System.Speech.Recognition.SpeechDetectedEventArgs> объекта указывает расположение во входном потоке, в котором распознаватель обнаружил распознавание речи. Объект <xref:System.Speech.Recognition.SpeechRecognitionEngine> <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized> <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> создает событие до того, как оно вызовет события, или. <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>  
  
 Дополнительные сведения см. в <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A>разделе <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A>методы <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize%2A>,, <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> и.  
  
 При создании делегата <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected> необходимо указать метод, обрабатывающий событие. Чтобы связать событие с обработчиком событий, нужно добавить в событие экземпляр делегата. Обработчик событий вызывается всякий раз, когда происходит событие, если делегат не удален. Дополнительные сведения о делегатах обработчиков событий см. в разделе [события и делегаты](https://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 Следующий пример является частью консольного приложения для выбора исходных и целевых городов для рейса. Приложение распознает такие фразы, как «я хочу полета из Майами в Чикаго».  В примере <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected> событие используется для <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A> сообщения о каждом обнаружении речи.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
  
    // Initialize an in-process speech recognition engine.  
    {  
      using (SpeechRecognitionEngine recognizer =  
         new SpeechRecognitionEngine())  
      {  
  
        // Create a grammar.  
        Choices cities = new Choices(new string[] {   
          "Los Angeles", "New York", "Chicago", "San Francisco", "Miami", "Dallas" });  
  
        GrammarBuilder gb = new GrammarBuilder();  
        gb.Append("I would like to fly from");  
        gb.Append(cities);  
        gb.Append("to");  
        gb.Append(cities);  
  
        // Create a Grammar object and load it to the recognizer.  
        Grammar g = new Grammar(gb);  
        g.Name = ("City Chooser");  
        recognizer.LoadGrammarAsync(g);  
  
        // Attach event handlers.  
        recognizer.LoadGrammarCompleted +=  
          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
        recognizer.SpeechDetected +=  
          new EventHandler<SpeechDetectedEventArgs>(recognizer_SpeechDetected);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
        // Set the input to the recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start recognition.  
        recognizer.RecognizeAsync();  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Handle the SpeechDetected event.  
    static void recognizer_SpeechDetected(object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine("  Speech detected at AudioPosition = {0}", e.AudioPosition);  
    }  
  
    // Handle the LoadGrammarCompleted event.  
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      Console.WriteLine("Grammar loaded: " + e.Grammar.Name);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("  Speech recognized: " + e.Result.Text);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.SpeechDetectedEventArgs" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
      </Docs>
    </Member>
    <Member MemberName="SpeechHypothesized">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.SpeechHypothesizedEventArgs&gt; SpeechHypothesized;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.SpeechHypothesizedEventArgs&gt; SpeechHypothesized" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
      <MemberSignature Language="VB.NET" Value="Public Custom Event SpeechHypothesized As EventHandler(Of SpeechHypothesizedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::SpeechHypothesizedEventArgs ^&gt; ^ SpeechHypothesized;" />
      <MemberSignature Language="F#" Value="member this.SpeechHypothesized : EventHandler&lt;System.Speech.Recognition.SpeechHypothesizedEventArgs&gt; " Usage="member this.SpeechHypothesized : System.EventHandler&lt;System.Speech.Recognition.SpeechHypothesizedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.SpeechHypothesizedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Возникает, когда <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> распознал слово или слова, которые могут являться нескольких составных фраз в грамматике.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 <xref:System.Speech.Recognition.SpeechRecognitionEngine> Создает множество<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized> событий при попытке указать входную фразу. Можно получить доступ к тексту частично распознанных фраз в <xref:System.Speech.Recognition.RecognitionEventArgs.Result%2A> свойстве <xref:System.Speech.Recognition.SpeechHypothesizedEventArgs> объекта <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized> в обработчике события. Как правило, обработка этих событий полезна только для отладки.  
  
 Интерфейс <xref:System.Speech.Recognition.SpeechHypothesizedEventArgs> является производным от интерфейса <xref:System.Speech.Recognition.RecognitionEventArgs>.  
  
 Дополнительные сведения см. в <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> разделе Свойства <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A>и методы <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize%2A>, и <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> .  
  
 При создании делегата <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized> необходимо указать метод, обрабатывающий событие. Чтобы связать событие с обработчиком событий, нужно добавить в событие экземпляр делегата. Обработчик событий вызывается всякий раз, когда происходит событие, если делегат не удален. Дополнительные сведения о делегатах обработчиков событий см. в разделе [события и делегаты](https://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 В следующем примере распознаются такие фразы, как "отобразить список исполнителей в категории джаз". В примере используется <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized> событие для вывода незавершенных фрагментов фраз в консоли по мере их распознавания.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
  
    // Initialize an in-process speech recognition engine.  
    {  
      using (SpeechRecognitionEngine recognizer =  
         new SpeechRecognitionEngine())  
      {  
  
        // Create a grammar.  
        //  Create lists of alternative choices.  
        Choices listTypes = new Choices(new string[] { "albums", "artists" });  
        Choices genres = new Choices(new string[] {   
          "blues", "classical", "gospel", "jazz", "rock" });  
  
        //  Create a GrammarBuilder object and assemble the grammar components.  
        GrammarBuilder mediaMenu = new GrammarBuilder("Display the list of");  
        mediaMenu.Append(listTypes);  
        mediaMenu.Append("in the");  
        mediaMenu.Append(genres);  
        mediaMenu.Append("category.");  
  
        //  Build a Grammar object from the GrammarBuilder.  
        Grammar mediaMenuGrammar = new Grammar(mediaMenu);  
        mediaMenuGrammar.Name = "Media Chooser";  
  
        // Attach event handlers.  
        recognizer.LoadGrammarCompleted +=  
          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
        recognizer.SpeechHypothesized +=  
          new EventHandler<SpeechHypothesizedEventArgs>(recognizer_SpeechHypothesized);  
  
        // Load the grammar object to the recognizer.  
        recognizer.LoadGrammarAsync(mediaMenuGrammar);  
  
        // Set the input to the recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start asynchronous recognition.  
        recognizer.RecognizeAsync();  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Handle the SpeechHypothesized event.  
    static void recognizer_SpeechHypothesized(object sender, SpeechHypothesizedEventArgs e)  
    {  
      Console.WriteLine("Speech hypothesized: " + e.Result.Text);  
    }  
  
    // Handle the LoadGrammarCompleted event.  
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      Console.WriteLine("Grammar loaded: " + e.Grammar.Name);  
      Console.WriteLine();  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine();   
      Console.WriteLine("Speech recognized: " + e.Result.Text);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.SpeechDetectedEventArgs" />
        <altmember cref="T:System.Speech.Recognition.Grammar" />
        <altmember cref="T:System.Speech.Recognition.RecognitionResult" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="T:System.Speech.Recognition.RecognitionEventArgs" />
      </Docs>
    </Member>
    <Member MemberName="SpeechRecognitionRejected">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.SpeechRecognitionRejectedEventArgs&gt; SpeechRecognitionRejected;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.SpeechRecognitionRejectedEventArgs&gt; SpeechRecognitionRejected" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
      <MemberSignature Language="VB.NET" Value="Public Event SpeechRecognitionRejected As EventHandler(Of SpeechRecognitionRejectedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::SpeechRecognitionRejectedEventArgs ^&gt; ^ SpeechRecognitionRejected;" />
      <MemberSignature Language="F#" Value="member this.SpeechRecognitionRejected : EventHandler&lt;System.Speech.Recognition.SpeechRecognitionRejectedEventArgs&gt; " Usage="member this.SpeechRecognitionRejected : System.EventHandler&lt;System.Speech.Recognition.SpeechRecognitionRejectedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.SpeechRecognitionRejectedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Возникает, когда <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> получает ввод, не соответствующий ни одному из загруженных и включенных объектов <see cref="T:System.Speech.Recognition.Grammar" />.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Распознаватель вызывает это событие, если определяет, что входные данные не совпадают с достаточным уровнем достоверности всех загруженных <xref:System.Speech.Recognition.Grammar> и включенных объектов. Свойство объекта содержит отклоненный <xref:System.Speech.Recognition.RecognitionResult>объект. <xref:System.Speech.Recognition.RecognitionEventArgs.Result%2A> <xref:System.Speech.Recognition.SpeechRecognitionRejectedEventArgs> Обработчик <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> события можно использовать для получения отклоненных распознавать <xref:System.Speech.Recognition.RecognitionResult.Alternates%2A> и их <xref:System.Speech.Recognition.RecognizedPhrase.Confidence%2A> оценок.  
  
 Если приложение использует <xref:System.Speech.Recognition.SpeechRecognitionEngine> экземпляр служб, можно изменить уровень достоверности, при котором речевое вход принимается или отклоняется одним <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> из методов. Можно изменить способ реагирования распознавания речи на неречевые входы с <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>помощью свойств <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>, и <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> .  
  
 При создании делегата <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> необходимо указать метод, обрабатывающий событие. Чтобы связать событие с обработчиком событий, нужно добавить в событие экземпляр делегата. Обработчик событий вызывается всякий раз, когда происходит событие, если делегат не удален. Дополнительные сведения о делегатах обработчиков событий см. в разделе [события и делегаты](https://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 В следующем примере распознаются такие фразы, как "отобразить список исполнителей в категории джаз" или "отобразить альбомы госпел". В примере используется обработчик <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> события для вывода уведомления в консоли, когда речевой ввод не может быть сопоставлен с содержимым грамматики с достаточным <xref:System.Speech.Recognition.RecognizedPhrase.Confidence%2A> для успешного распознавания. Обработчик также отображает результаты <xref:System.Speech.Recognition.RecognitionResult.Alternates%2A> распознавания, которые были отклонены из-за оценки низкой достоверности.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
  
    // Initialize an in-process speech recognition engine.  
    {  
      using (SpeechRecognitionEngine recognizer =  
         new SpeechRecognitionEngine(new System.Globalization.CultureInfo("en-US")))  
      {  
  
        // Create a grammar.  
        //  Create lists of alternative choices.  
        Choices listTypes = new Choices(new string[] { "albums", "artists" });  
        Choices genres = new Choices(new string[] {   
          "blues", "classical", "gospel", "jazz", "rock" });  
  
        //  Create a GrammarBuilder object and assemble the grammar components.  
        GrammarBuilder mediaMenu = new GrammarBuilder("Display");  
        mediaMenu.Append("the list of", 0, 1);  
        mediaMenu.Append(listTypes);  
        mediaMenu.Append("in the", 0, 1);  
        mediaMenu.Append(genres);  
        mediaMenu.Append("category", 0, 1);  
  
        //  Build a Grammar object from the GrammarBuilder.  
        Grammar mediaMenuGrammar = new Grammar(mediaMenu);  
        mediaMenuGrammar.Name = "Media Chooser";  
  
        // Attach event handlers.  
        recognizer.LoadGrammarCompleted +=  
          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(recognizer_SpeechRecognitionRejected);  
  
        // Load the grammar object to the recognizer.  
        recognizer.LoadGrammarAsync(mediaMenuGrammar);  
  
        // Set the input to the recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Handle the SpeechRecognitionRejected event.  
    static void recognizer_SpeechRecognitionRejected(object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine("Speech input was rejected.");  
      foreach (RecognizedPhrase phrase in e.Result.Alternates)  
      {  
      Console.WriteLine("  Rejected phrase: " + phrase.Text);  
      Console.WriteLine("  Confidence score: " + phrase.Confidence);  
      }  
    }  
  
    // Handle the LoadGrammarCompleted event.  
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      Console.WriteLine("Grammar loaded: " + e.Grammar.Name);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Speech recognized: " + e.Result.Text);  
      Console.WriteLine("  Confidence score: " + e.Result.Confidence);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.SpeechDetectedEventArgs" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
      </Docs>
    </Member>
    <Member MemberName="SpeechRecognized">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.SpeechRecognizedEventArgs&gt; SpeechRecognized;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.SpeechRecognizedEventArgs&gt; SpeechRecognized" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
      <MemberSignature Language="VB.NET" Value="Public Event SpeechRecognized As EventHandler(Of SpeechRecognizedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::SpeechRecognizedEventArgs ^&gt; ^ SpeechRecognized;" />
      <MemberSignature Language="F#" Value="member this.SpeechRecognized : EventHandler&lt;System.Speech.Recognition.SpeechRecognizedEventArgs&gt; " Usage="member this.SpeechRecognized : System.EventHandler&lt;System.Speech.Recognition.SpeechRecognizedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.SpeechRecognizedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Возникает, когда <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> получает ввод, соответствующий любому из загруженных и включенных объектов <see cref="T:System.Speech.Recognition.Grammar" />.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Операцию распознавания можно инициировать с помощью одного из <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A> методов или. <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> Распознаватель вызывает <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> событие, если оно определяет, что входные данные соответствуют одному из <xref:System.Speech.Recognition.Grammar> загруженных объектов с достаточным уровнем достоверности для распознавания. Свойство объекта содержит допустимый <xref:System.Speech.Recognition.RecognitionResult>объект. <xref:System.Speech.Recognition.RecognitionEventArgs.Result%2A> <xref:System.Speech.Recognition.SpeechRecognitionRejectedEventArgs> Обработчики <xref:System.Speech.Recognition.RecognitionResult.Alternates%2A> событий могут получить распознанную фразу, а также список распознавания с более низкой оценкой достоверности. <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>  
  
 Если приложение использует <xref:System.Speech.Recognition.SpeechRecognitionEngine> экземпляр служб, можно изменить уровень достоверности, при котором речевое вход принимается или отклоняется одним <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> из методов.  Можно изменить способ реагирования распознавания речи на неречевые входы с <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>помощью свойств <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>, и <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> .  
  
 Когда распознаватель получает входные данные, соответствующие грамматике, <xref:System.Speech.Recognition.Grammar> объект может вызвать его <xref:System.Speech.Recognition.Grammar.SpeechRecognized> событие. Событие объекта возникает до <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> события распознавателя речи. <xref:System.Speech.Recognition.Grammar> <xref:System.Speech.Recognition.Grammar.SpeechRecognized> Все задачи, относящиеся к конкретной грамматике, всегда должны выполняться обработчиком <xref:System.Speech.Recognition.Grammar.SpeechRecognized> события.  
  
 При создании делегата <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> необходимо указать метод, обрабатывающий событие. Чтобы связать событие с обработчиком событий, нужно добавить в событие экземпляр делегата. Обработчик событий вызывается всякий раз, когда происходит событие, если делегат не удален. Дополнительные сведения о делегатах обработчиков событий см. в разделе [события и делегаты](https://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 Следующий пример является частью консольного приложения, которое создает грамматику распознавания речи, создает <xref:System.Speech.Recognition.Grammar> объект и загружает его <xref:System.Speech.Recognition.SpeechRecognitionEngine> в для выполнения распознавания. В примере демонстрируется ввод речи в <xref:System.Speech.Recognition.SpeechRecognitionEngine>, связанные результаты распознавания и связанные события, вызванные распознавателем речи.  
  
 Входные данные, такие как «я хочу вылет из Чикаго в Майами», активируют <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> событие. Говорите, что фраза «Вылет от Хьюстоне к Чикаго» не будет <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> запускать событие.  
  
 В примере используется обработчик для <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> события, чтобы отобразить успешно распознанные фразы и семантику, которую они содержат в консоли.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
  
    // Initialize an in-process speech recognition engine.  
    {  
      using (SpeechRecognitionEngine recognizer = new SpeechRecognitionEngine())  
      {  
  
        // Create SemanticResultValue objects that contain cities and airport codes.  
        SemanticResultValue chicago = new SemanticResultValue("Chicago", "ORD");  
        SemanticResultValue boston = new SemanticResultValue("Boston", "BOS");  
        SemanticResultValue miami = new SemanticResultValue("Miami", "MIA");  
        SemanticResultValue dallas = new SemanticResultValue("Dallas", "DFW");  
  
        // Create a Choices object and add the SemanticResultValue objects, using  
        // implicit conversion from SemanticResultValue to GrammarBuilder  
        Choices cities = new Choices();  
        cities.Add(new Choices(new GrammarBuilder[] { chicago, boston, miami, dallas }));  
  
        // Build the phrase and add SemanticResultKeys.  
        GrammarBuilder chooseCities = new GrammarBuilder();  
        chooseCities.Append("I want to fly from");  
        chooseCities.Append(new SemanticResultKey("origin", cities));  
        chooseCities.Append("to");  
        chooseCities.Append(new SemanticResultKey("destination", cities));  
  
        // Build a Grammar object from the GrammarBuilder.  
        Grammar bookFlight = new Grammar(chooseCities);  
        bookFlight.Name = "Book Flight";  
  
        // Add a handler for the LoadGrammarCompleted event.  
        recognizer.LoadGrammarCompleted +=  
          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
  
        // Add a handler for the SpeechRecognized event.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
        // Load the grammar object to the recognizer.  
        recognizer.LoadGrammarAsync(bookFlight);  
  
        // Set the input to the recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start recognition.  
        recognizer.RecognizeAsync();  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Handle the LoadGrammarCompleted event.  
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      Console.WriteLine("Grammar loaded: " + e.Grammar.Name);  
      Console.WriteLine();  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Speech recognized:  " + e.Result.Text);  
      Console.WriteLine();  
      Console.WriteLine("Semantic results:");  
      Console.WriteLine("  The flight origin is " + e.Result.Semantics["origin"].Value);  
      Console.WriteLine("  The flight destination is " + e.Result.Semantics["destination"].Value);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.SpeechDetectedEventArgs" />
        <altmember cref="T:System.Speech.Recognition.Grammar" />
        <altmember cref="T:System.Speech.Recognition.RecognitionResult" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="T:System.Speech.Recognition.RecognitionEventArgs" />
      </Docs>
    </Member>
    <Member MemberName="UnloadAllGrammars">
      <MemberSignature Language="C#" Value="public void UnloadAllGrammars ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void UnloadAllGrammars() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.UnloadAllGrammars" />
      <MemberSignature Language="VB.NET" Value="Public Sub UnloadAllGrammars ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void UnloadAllGrammars();" />
      <MemberSignature Language="F#" Value="member this.UnloadAllGrammars : unit -&gt; unit" Usage="speechRecognitionEngine.UnloadAllGrammars " />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Выгружает все объекты <see cref="T:System.Speech.Recognition.Grammar" /> из распознавателя.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Если распознаватель в данный момент загружает <xref:System.Speech.Recognition.Grammar> асинхронно, этот метод ждет, <xref:System.Speech.Recognition.Grammar> пока не загрузится, перед <xref:System.Speech.Recognition.Grammar> выгрузкой всех объектов из <xref:System.Speech.Recognition.SpeechRecognitionEngine> экземпляра.  
  
 Чтобы выгрузить определенную грамматику <xref:System.Speech.Recognition.SpeechRecognitionEngine.UnloadGrammar%2A> , используйте метод.  
  
   
  
## Examples  
 В следующем примере показана часть консольного приложения, демонстрирующее синхронную загрузку и выгрузку грамматики распознавания речи.  
  
```  
Loading grammars...  
Loaded grammars:  
 - Grammar1  
 - Grammar2  
 - Grammar3  
  
Unloading Grammar1...  
Loaded grammars:  
 - Grammar2  
 - Grammar3  
  
Unloading all grammars...  
No grammars loaded.  
  
Press any key to exit...  
```  
  
```csharp  
  
using System;  
using System.Collections.Generic;  
using System.Globalization;  
using System.Speech.Recognition;  
  
namespace UnloadGrammars  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
        Console.WriteLine("Loading grammars...");  
  
        // Create and load a number of grammars.  
        Grammar grammar1 = new Grammar(new GrammarBuilder("first grammar"));  
        grammar1.Name = "Grammar1";  
        recognizer.LoadGrammar(grammar1);  
  
        Grammar grammar2 = new Grammar(new GrammarBuilder("second grammar"));  
        grammar2.Name = "Grammar2";  
        recognizer.LoadGrammar(grammar2);  
  
        Grammar grammar3 = new Grammar(new GrammarBuilder("third grammar"));  
        grammar3.Name = "Grammar3";  
        recognizer.LoadGrammar(grammar3);  
  
        // List the recognizer's loaded grammars.  
        ListGrammars(recognizer);  
  
        // Unload one grammar and list the loaded grammars.  
        Console.WriteLine("Unloading Grammar1...");  
        recognizer.UnloadGrammar(grammar1);  
        ListGrammars(recognizer);  
  
        // Unload all grammars and list the loaded grammars.  
        Console.WriteLine("Unloading all grammars...");  
        recognizer.UnloadAllGrammars();  
        ListGrammars(recognizer);  
      }  
  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    private static void ListGrammars(SpeechRecognitionEngine recognizer)  
    {  
      // Make a copy of the recognizer's grammar collection.  
      List<Grammar> loadedGrammars = new List<Grammar>(recognizer.Grammars);  
  
      if (loadedGrammars.Count > 0)  
      {  
        Console.WriteLine("Loaded grammars:");  
        foreach (Grammar g in recognizer.Grammars)  
        {  
          Console.WriteLine(" - {0}", g.Name);  
        }  
      }  
      else  
      {  
        Console.WriteLine("No grammars loaded.");  
      }  
      Console.WriteLine();  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.UnloadGrammar(System.Speech.Recognition.Grammar)" />
      </Docs>
    </Member>
    <Member MemberName="UnloadGrammar">
      <MemberSignature Language="C#" Value="public void UnloadGrammar (System.Speech.Recognition.Grammar grammar);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void UnloadGrammar(class System.Speech.Recognition.Grammar grammar) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.UnloadGrammar(System.Speech.Recognition.Grammar)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void UnloadGrammar(System::Speech::Recognition::Grammar ^ grammar);" />
      <MemberSignature Language="F#" Value="member this.UnloadGrammar : System.Speech.Recognition.Grammar -&gt; unit" Usage="speechRecognitionEngine.UnloadGrammar grammar" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="grammar" Type="System.Speech.Recognition.Grammar" />
      </Parameters>
      <Docs>
        <param name="grammar">Объект грамматики для выгрузки.</param>
        <summary>Выгружает заданный объект <see cref="T:System.Speech.Recognition.Grammar" /> из экземпляра <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Если распознаватель запущен, приложения должны <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> <xref:System.Speech.Recognition.SpeechRecognitionEngine> приостанавливать экземпляр перед загрузкой, выгрузкой, <xref:System.Speech.Recognition.Grammar> включением или отключением объекта. Чтобы выгрузить все <xref:System.Speech.Recognition.Grammar> объекты, <xref:System.Speech.Recognition.SpeechRecognitionEngine.UnloadAllGrammars%2A> используйте метод.  
  
   
  
## Examples  
 В следующем примере показана часть консольного приложения, демонстрирующее синхронную загрузку и выгрузку грамматики распознавания речи.  
  
```  
Loading grammars...  
Loaded grammars:  
 - Grammar1  
 - Grammar2  
 - Grammar3  
  
Unloading Grammar1...  
Loaded grammars:  
 - Grammar2  
 - Grammar3  
  
Unloading all grammars...  
No grammars loaded.  
  
Press any key to exit...  
```  
  
```csharp  
  
using System;  
using System.Collections.Generic;  
using System.Globalization;  
using System.Speech.Recognition;  
  
namespace UnloadGrammars  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
        Console.WriteLine("Loading grammars...");  
  
        // Create and load a number of grammars.  
        Grammar grammar1 = new Grammar(new GrammarBuilder("first grammar"));  
        grammar1.Name = "Grammar1";  
        recognizer.LoadGrammar(grammar1);  
  
        Grammar grammar2 = new Grammar(new GrammarBuilder("second grammar"));  
        grammar2.Name = "Grammar2";  
        recognizer.LoadGrammar(grammar2);  
  
        Grammar grammar3 = new Grammar(new GrammarBuilder("third grammar"));  
        grammar3.Name = "Grammar3";  
        recognizer.LoadGrammar(grammar3);  
  
        // List the recognizer's loaded grammars.  
        ListGrammars(recognizer);  
  
        // Unload one grammar and list the loaded grammars.  
        Console.WriteLine("Unloading Grammar1...");  
        recognizer.UnloadGrammar(grammar1);  
        ListGrammars(recognizer);  
  
        // Unload all grammars and list the loaded grammars.  
        Console.WriteLine("Unloading all grammars...");  
        recognizer.UnloadAllGrammars();  
        ListGrammars(recognizer);  
      }  
  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    private static void ListGrammars(SpeechRecognitionEngine recognizer)  
    {  
      // Make a copy of the recognizer's grammar collection.  
      List<Grammar> loadedGrammars = new List<Grammar>(recognizer.Grammars);  
  
      if (loadedGrammars.Count > 0)  
      {  
        Console.WriteLine("Loaded grammars:");  
        foreach (Grammar g in recognizer.Grammars)  
        {  
          Console.WriteLine(" - {0}", g.Name);  
        }  
      }  
      else  
      {  
        Console.WriteLine("No grammars loaded.");  
      }  
      Console.WriteLine();  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentNullException">Свойство <paramref name="Grammar" /> имеет значение <see langword="null" />.</exception>
        <exception cref="T:System.InvalidOperationException">Грамматика не загружена в этот распознаватель или этот распознаватель в данный момент загружает грамматику асинхронно.</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.UnloadAllGrammars" />
      </Docs>
    </Member>
    <MemberGroup MemberName="UpdateRecognizerSetting">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>Обновляет значение параметра для распознавателя.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Параметры распознавателя могут содержать строку, 64-разрядное целое число или данные адреса памяти. В следующей таблице описаны параметры, определенные для распознавателя, совместимого с Microsoft Speech API (SAPI). Следующие параметры должны иметь один и тот же диапазон для каждого распознавателя, который поддерживает этот параметр. Совместимый с SAPI распознаватель не требуется для поддержки этих параметров и может поддерживать другие параметры.  
  
|name|Описание|  
|----------|-----------------|  
|`ResourceUsage`|Указывает использование ЦП распознавателем. Диапазон — от 0 до 100. Значение по умолчанию — 50.|  
|`ResponseSpeed`|Указывает длину тишины в конце однозначных входных данных до того, как распознаватель речи Завершает операцию распознавания. Значение в диапазоне от 0 до 10 000 миллисекунд (МС). Этот параметр соответствует <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A> свойству распознавателя. Значение по умолчанию — 150ms.|  
|`ComplexResponseSpeed`|Указывает длину бездействия в миллисекундах (МС) в конце неоднозначных входных данных до того, как распознаватель речи Завершает операцию распознавания. Диапазон — от 0 до 10, 000 мс.. Этот параметр соответствует <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> свойству распознавателя. Значение по умолчанию — 500 мс.|  
|`AdaptationOn`|Указывает, включена ли адаптация акустической модели (value = `1`) или OFF (значение = `0`). Значение по умолчанию `1` — (on).|  
|`PersistedBackgroundAdaptation`|Указывает, включена ли фоновая адаптация (value `1`=) или OFF (значение `0`=), и сохраняет параметр в реестре. Значение по умолчанию `1` — (on).|  
  
 Чтобы вернуть один из параметров распознавателя, используйте <xref:System.Speech.Recognition.SpeechRecognitionEngine.QueryRecognizerSetting%2A> метод.  
  
 За исключением `PersistedBackgroundAdaptation`, значения свойств, заданные <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> с помощью методов, остаются действительными только <xref:System.Speech.Recognition.SpeechRecognitionEngine>для текущего экземпляра, после чего они возвращаются к значениям по умолчанию.  
  
 Можно изменить способ реагирования распознавания речи на неречевые входы с <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>помощью свойств <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>, и <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> .  
  
 ]]></format>
        </remarks>
      </Docs>
    </MemberGroup>
    <Member MemberName="UpdateRecognizerSetting">
      <MemberSignature Language="C#" Value="public void UpdateRecognizerSetting (string settingName, int updatedValue);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void UpdateRecognizerSetting(string settingName, int32 updatedValue) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting(System.String,System.Int32)" />
      <MemberSignature Language="VB.NET" Value="Public Sub UpdateRecognizerSetting (settingName As String, updatedValue As Integer)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void UpdateRecognizerSetting(System::String ^ settingName, int updatedValue);" />
      <MemberSignature Language="F#" Value="member this.UpdateRecognizerSetting : string * int -&gt; unit" Usage="speechRecognitionEngine.UpdateRecognizerSetting (settingName, updatedValue)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="settingName" Type="System.String" />
        <Parameter Name="updatedValue" Type="System.Int32" />
      </Parameters>
      <Docs>
        <param name="settingName">Имя обновляемого параметра.</param>
        <param name="updatedValue">Новое значение для параметра.</param>
        <summary>Обновляет заданный параметр для <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> с указанным целым числом.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 За исключением `PersistedBackgroundAdaptation`, значения свойств, заданные <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> с помощью метода, остаются действительными только <xref:System.Speech.Recognition.SpeechRecognitionEngine>для текущего экземпляра, после чего они возвращаются к значениям по умолчанию. Описание <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> поддерживаемых параметров см. в разделе.  
  
   
  
## Examples  
 Следующий пример является частью консольного приложения, которое выводит значения для ряда параметров, определенных для распознавателя, поддерживающего языковой стандарт EN-US. В этом примере обновляются параметры уровня достоверности, а затем запрашивается распознаватель для проверки обновленных значений. В примере создаются следующие выходные данные.  
  
```  
Settings for recognizer MS-1033-80-DESK:  
  
  ResourceUsage                  is not supported by this recognizer.  
  ResponseSpeed                  = 150  
  ComplexResponseSpeed           = 500  
  AdaptationOn                   = 1  
  PersistedBackgroundAdaptation  = 1  
  
Updated settings:  
  
  ResourceUsage                  is not supported by this recognizer.  
  ResponseSpeed                  = 200  
  ComplexResponseSpeed           = 300  
  AdaptationOn                   = 0  
  PersistedBackgroundAdaptation  = 0  
  
Press any key to exit...  
```  
  
```csharp  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
  
namespace RecognizerSettings  
{  
  class Program  
  {  
    static readonly string[] settings = new string[] {  
      "ResourceUsage",  
      "ResponseSpeed",  
      "ComplexResponseSpeed",  
      "AdaptationOn",  
      "PersistedBackgroundAdaptation",  
    };  
  
    static void Main(string[] args)  
    {  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new System.Globalization.CultureInfo("en-US")))  
      {  
        Console.WriteLine("Settings for recognizer {0}:",  
          recognizer.RecognizerInfo.Name);  
        Console.WriteLine();  
  
        // List the current settings.  
        ListSettings(recognizer);  
  
        // Change some of the settings.  
        recognizer.UpdateRecognizerSetting("ResponseSpeed", 200);  
        recognizer.UpdateRecognizerSetting("ComplexResponseSpeed", 300);  
        recognizer.UpdateRecognizerSetting("AdaptationOn", 1);  
        recognizer.UpdateRecognizerSetting("PersistedBackgroundAdaptation", 0);  
  
        Console.WriteLine("Updated settings:");  
        Console.WriteLine();  
  
        // List the updated settings.  
        ListSettings(recognizer);  
      }  
  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    private static void ListSettings(SpeechRecognitionEngine recognizer)  
    {  
      foreach (string setting in settings)  
      {  
        try  
        {  
          object value = recognizer.QueryRecognizerSetting(setting);  
          Console.WriteLine("  {0,-30} = {1}", setting, value);  
        }  
        catch  
        {  
          Console.WriteLine("  {0,-30} is not supported by this recognizer.",  
            setting);  
        }  
      }  
      Console.WriteLine();  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentNullException">Свойство <paramref name="settingName" /> имеет значение <see langword="null" />.</exception>
        <exception cref="T:System.ArgumentException"><paramref name="settingName" /> является пустой строкой ("").</exception>
        <exception cref="T:System.Collections.Generic.KeyNotFoundException">Распознаватель не имеет параметра с указанным именем.</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.QueryRecognizerSetting(System.String)" />
      </Docs>
    </Member>
    <Member MemberName="UpdateRecognizerSetting">
      <MemberSignature Language="C#" Value="public void UpdateRecognizerSetting (string settingName, string updatedValue);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void UpdateRecognizerSetting(string settingName, string updatedValue) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting(System.String,System.String)" />
      <MemberSignature Language="VB.NET" Value="Public Sub UpdateRecognizerSetting (settingName As String, updatedValue As String)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void UpdateRecognizerSetting(System::String ^ settingName, System::String ^ updatedValue);" />
      <MemberSignature Language="F#" Value="member this.UpdateRecognizerSetting : string * string -&gt; unit" Usage="speechRecognitionEngine.UpdateRecognizerSetting (settingName, updatedValue)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="settingName" Type="System.String" />
        <Parameter Name="updatedValue" Type="System.String" />
      </Parameters>
      <Docs>
        <param name="settingName">Имя обновляемого параметра.</param>
        <param name="updatedValue">Новое значение для параметра.</param>
        <summary>Обновляет указанный параметр механизма распознавания речи с заданным стоковым  значением.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 За исключением `PersistedBackgroundAdaptation`, значения свойств, заданные <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> с помощью метода, остаются действительными только <xref:System.Speech.Recognition.SpeechRecognitionEngine>для текущего экземпляра, после чего они возвращаются к значениям по умолчанию. Описание <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> поддерживаемых параметров см. в разделе.  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentNullException">Свойство <paramref name="settingName" /> имеет значение <see langword="null" />.</exception>
        <exception cref="T:System.ArgumentException"><paramref name="settingName" /> является пустой строкой ("").</exception>
        <exception cref="T:System.Collections.Generic.KeyNotFoundException">Распознаватель не имеет параметра с указанным именем.</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.QueryRecognizerSetting(System.String)" />
      </Docs>
    </Member>
  </Members>
</Type>
